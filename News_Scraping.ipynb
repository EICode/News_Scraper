{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated News Update, or, \"Dwyer's attempt at automating himself out of a job\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Do\n",
    "* Fix Detroit News\n",
    "* Functionality to add news article to SQL database after the fact\n",
    "* Add Phys.org\n",
    "* Add infinite scrolling functionality to css_scraypuh\n",
    "\n",
    "### Can't because of paywalls:\n",
    "* WSJ\n",
    "* Nikkei\n",
    "* Automotive World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T11:29:08.311107Z",
     "start_time": "2018-08-29T11:29:08.300133Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages, define important stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T13:30:37.376863Z",
     "start_time": "2019-01-07T13:30:27.352145Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import requests\n",
    "\n",
    "import docx\n",
    "from docx.enum.text import WD_COLOR_INDEX\n",
    "from docx.shared import Pt\n",
    "from docx.shared import Inches\n",
    "\n",
    "import random\n",
    "\n",
    "#!pip install nltk\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables and dictionary of scrapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T13:30:37.453156Z",
     "start_time": "2019-01-07T13:30:37.390327Z"
    }
   },
   "outputs": [],
   "source": [
    "# Search date and search date string for file naming and tracking\n",
    "search_date = dt.date.today()\n",
    "search_date_str = str(search_date)\n",
    "\n",
    "# Essential keys to include in scraper_info subdictionaries:\n",
    "# 'day': Integer that represents the day of the week that corresponds with a particular scraper. e.g., 0==Monday.\n",
    "# 'keywords': Keyword lists for each of the different news updates\n",
    "# 'bad_words': If any of these words are in the article title, drop article. Words are not case sensitive.\n",
    "# 'ideal_no_articles': Ideal number of articles to be scraped for each scraper.  If excess articles are scraped, the articles will be sampled (articles from best sources are sampled first).\n",
    "#                      Journal articles do not count towards this total.\n",
    "# 'max age': Age filter, in days (If max_age==7, it means we only want to pull articles that are <= 1 week old)\n",
    "# 'news_download_filename': subpath (starting in directory that contains News_Scraping.ipynb) to which news_downloads should be downloaded plus filename for news_download\n",
    "# 'make_deletions': boolean that specifies whether deletions (i.e. of duplicate articles, irrelevant articles, etc.) should be made.  True if deletions desired, False if not.\n",
    "# 'gen_docx': boolean that specifies whether docx should be created from news_download file.  True if docx desired, False if not.\n",
    "# 'restrict_sites: feature coming soon.\n",
    "\n",
    "# Optional keys to include in scraper_info subdictionaries:\n",
    "# The following keys are dependent on 'make_deletions'.  They MUST be included if 'make_deletions' is True.\n",
    "#    'deletions_filename': subpath (starting in directory that contains News_Scraping.ipynb) to which news_deletions should be downloaded plus filename for news_deletions\n",
    "# The following keys are dependent on 'gendocx'.  They MUST be included if 'gendocx' is True.\n",
    "#    'docx_title': Title of docx file (string)\n",
    "#    'docx_intro': Intro paragraph for docx file (string or None)\n",
    "#    'docx_headers': Dictionary of codes that indicate news_download categories and their corresponding docx headers\n",
    "#    'auto_id_research': boolean that specifies whether or not research category should be auto-id'd in the news_download file.  If True, articles from journals will be \n",
    "#                        automatically categorized as '4.' If False, articles from journals will not be categorized.  Mark as False if you don't want a separate research\n",
    "#                        section in the docx.\n",
    "# 'research_intro': String. May be included in the scraper_info dict or not.  Include if a paragraph of introduction to the research section of the docx file is desired.  Otherwise leave out.\n",
    "\n",
    "scraper_info = { '21CTP':{'day': 4,\n",
    "                          'essential_keywords':['truck'],\n",
    "                          'general_keywords': ['energy storage', 'hydrogen', 'hybrid', \n",
    "                                       'Phase 2', 'Phase II', 'efficiency', 'sustainability', 'lithium', 'biofuel', 'fast charging', 'downspeed', 'skirt', \n",
    "                                       'high productivity', 'restructuring', 'acquisition', 'idle', 'electric', 'electrified', 'battery',\n",
    "                                       'facilities', 'proving ground', 'joint venture',  'FOA', 'funding opportunity', \n",
    "                                       'greenhouse gas', 'GHG', 'strategic plan','partnership', 'connected', 'announce', 'offer', 'expansion', 'unveil', 'grant'],\n",
    "                          'specific_keywords': ['alternative fuel', 'natural gas', 'compressed natural gas', 'liquefied natural gas', 'CNG', 'LNG', 'propane', 'LPG', 'dimethyl ether', \n",
    "                                       'DME', 'electric drive', 'fuel cell', 'hybrid electric', 'hybrid hydraulic', 'fuel efficiency', 'fuel economy', 'aftertreatment', 'emission control', \n",
    "                                       'diesel particulate filter', 'DPF', 'selective catalytic reduction', 'SCR', 'aerodynamics','waste heat recovery', 'Rankine', 'organic Rankine', 'SuperTruck',\n",
    "                                       'automated manual', 'AMT', 'platooning', 'downsize', 'clean diesel', 'turbocompound', 'rolling resistance', 'boat tail', 'axle', 'low viscosity',\n",
    "                                       'catenary', 'autonomy', 'autonomous','telematics', 'driver assist', 'CACC', 'active cruise control', \n",
    "                                       'crash avoidance', 'crashworthiness', 'weigh-in-motion', 'weigh in motion', 'truck size and weight', 'V2I', 'V2V', \n",
    "                                       'vehicle to infrastructure', ' vehicle to vehicle','driver cost', 'operational efficiency','regional haul', 'emission regulation',\n",
    "                                       'emissions regulation', 'idling', 'zero emissions','SmartWay', 'VIUS', 'well to wheels', 'pump to wheels', \n",
    "                                       'well to pump', 'CARB', 'CEC', 'air resources board', 'energy commission', 'EPA','Environmental Protection Agency', 'smart mobility', \n",
    "                                       'smart cities'],\n",
    "                          'bad_words':['brochure'],\n",
    "                          'ideal_no_articles': 40,\n",
    "                          'max_age':7,\n",
    "                          'news_download_filename': f'21CTP_news_updates/{search_date_str}_21CTP_news_download.xls',\n",
    "                          'deletions_filename': f'21CTP_news_updates/{search_date_str}_21CTP_news_deletions.xls',\n",
    "                          'make_deletions': True,\n",
    "                          'gen_docx': True,\n",
    "                          'docx_title': \"21CTP Weekly News Update\",\n",
    "                          'docx_intro':None,\n",
    "                          'docx_headers':{1: 'Business and Market Analysis',\n",
    "                                          2: 'Technology, Testing, and Analysis', \n",
    "                                          3: 'Policy and Government', \n",
    "                                          4: 'Relevant Transportation Research'},\n",
    "                          'auto_id_research': True,\n",
    "                          'research_intro':'This section includes publications, papers, articles, and conferences that investigate and/or '\n",
    "                                           'are related to trucking. Portions of the abstract '\n",
    "                                           \"or description (not Energetics' words) are included under each title for more information.\",\n",
    "                          'news_df': None\n",
    "                       },\n",
    "                 'CAV':{'day': 0,\n",
    "                        'essential_keywords':[],\n",
    "                        'general_keywords':['automated'],\n",
    "                        'specific_keywords': ['self-driving', 'self driving', 'autonomous', 'MaaS', 'ride-sharing', 'ridesharing',\n",
    "                                     'ridehailing', 'lidar', 'LiDAR', 'rideshare', 'ridehail', 'ride-hail', 'ridesource', 'ride-source', 'ridesourcing', 'ride-sourcing',\n",
    "                                     'carsharing', 'car-sharing', 'carshare', 'car-share', 'Uber', 'Lyft', 'Chariot', 'connected car', 'Waymo', ' TRI',\n",
    "                                     'Cruise', 'Zoox', 'Mobileye', 'Softbank', 'peer-to-peer', 'Turo'],\n",
    "                        'bad_words':['brochure'],\n",
    "                        'ideal_no_articles': 45,\n",
    "                        'max_age':7,\n",
    "                        'deletions_filename' : f'cav_news_updates/{search_date_str}_CAV_news_deletions.xls',\n",
    "                        'news_download_filename': f'cav_news_updates/{search_date_str}_cav_news_download.xls',\n",
    "                        'make_deletions': True,\n",
    "                        'gen_docx': True,\n",
    "                        'docx_title':\"Smart Mobility Weekly News Update\",\n",
    "                        'docx_intro':'Includes coverage of ride-sharing and other smart mobility technologies. '\n",
    "                                     'The majority of this is direct quotations from the respective articles. Energetics '\n",
    "                                     'claims none of this text content as its own, having only sifted through the '\n",
    "                                     'web to find already-existing pieces relevant to these topics.',\n",
    "                        'docx_headers':{1: 'Business and Market Analysis',\n",
    "                                        2: 'Technology, Testing, and Analysis', \n",
    "                                        3: 'Policy and Government', \n",
    "                                        4: 'Relevant Transportation Research'},\n",
    "                        'auto_id_research': True,\n",
    "                        'research_intro':'This section includes publications, papers, articles, and conferences that investigate and/or '\n",
    "                                         'discuss transportation and travel demand impacts of MaaS or other “future travel” considerations. '\n",
    "                                         \"Portions of the abstract or description (not Energetics' words) are included under each title for more\"\n",
    "                                         'information.',\n",
    "                        'news_df': None\n",
    "                       },\n",
    "                 'AFV':{'day':2,\n",
    "                        'essential_keywords':[],\n",
    "                        'general_keywords':[],\n",
    "                        'specific_keywords': ['rare-earth', 'rare earth', 'natural gas', 'electric vehicle', 'electric car', 'EV', 'electrification', 'alternative fuel', 'CNG', 'LNG',\n",
    "                                       'alt-fuel', 'propane', 'charging station', 'EVSE', 'electric vehicle charging', 'HEV', 'hybrid', 'hybrid-electric', 'plug-in', 'PHEV', \n",
    "                                       'electric motor', 'bio-fuel', 'biofuel', 'idle reduction', 'fuel cell', 'electric bus', 'electric truck', 'electric drive',\n",
    "                                       'battery-electric', 'battery electric', 'battery-electric-powered', 'regenerative braking'],\n",
    "                        'bad_words': [\"Today's Car News\", 'image', 'picture', 'podcast', 'video', 'photo', 'tweet', 'scooter', \n",
    "                                      'motorcycle', 'shipping', 'This Week in Reverse', 'brochure'],\n",
    "                        'ideal_no_articles':50,\n",
    "                        'max_age':7,\n",
    "                        'news_download_filename' : f'afv_news_updates/{search_date_str}_afv_news_download.xls',\n",
    "                        'deletions_filename' : f'afv_news_updates/{search_date_str}_afv_news_deletions.xls',\n",
    "                        'make_deletions': True,\n",
    "                        'gen_docx': True,\n",
    "                        'docx_title':\"Alternative Fuel Vehicle Weekly News Update\",\n",
    "                        'docx_intro':None,\n",
    "                        'docx_headers':{1: 'Business and Market Analysis',\n",
    "                                        2: 'Technology, Testing, and Analysis', \n",
    "                                        3: 'Policy and Government', \n",
    "                                        4: 'Relevant Transportation Research'},\n",
    "                        'auto_id_research': True,\n",
    "                        'research_intro':'This section includes publications, papers, articles, and conferences that investigate and/or '\n",
    "                                         'discuss alternative fuel vehicle impacts on transportation systems. Portions of the abstract '\n",
    "                                         \"or description (not Energetics' words) are included under each title for more information.\",\n",
    "                        'news_df': None\n",
    "                         },\n",
    "                 'Hyperloop':{'day':0,\n",
    "                              'essential_keywords':[],\n",
    "                              'general_keywords': [],\n",
    "                              'specific_keywords': ['hyperloop', 'high-speed train', 'high speed train', 'bullet train', 'ET3', 'Non-Traditional and Emerging Transportation Technology Council'\n",
    "                                                    'NETT', 'evacuated tube', 'evacuated-tube'],\n",
    "                              'bad_words':['brochure'],\n",
    "                              'ideal_no_articles': 35,\n",
    "                              'max_age':7,\n",
    "                              'news_download_filename': f'hyperloop_news_updates/{search_date_str}_hyperloop_news_download.xls',\n",
    "                              'deletions_filename':f'hyperloop_news_updates/{search_date_str}_hyperloop_news_deletions.xls',\n",
    "                              'make_deletions': False,\n",
    "                              'gen_docx': False,\n",
    "                              'auto_id_research': True,\n",
    "                              'news_df': None\n",
    "                             },\n",
    "                 'eVTOL':{'day':2,\n",
    "                          'essential_keywords':[],\n",
    "                          'general_keywords':[],\n",
    "                          'specific_keywords': ['UAV', 'VTOL', 'Pipistrel', 'Airbus', 'Uber Air', 'UberAir', 'Uber Elevate', 'Bell Helicopter', 'Kitty Hawk', 'Kittyhawk', 'Lilium', 'Karem', \n",
    "                                       'Volocopter', 'Aurora Flight Sciences', 'Embraer', 'vertical take-off', 'flying taxi','air taxi', 'air car', 'Sikorsky', 'PAV', 'flying car', \n",
    "                                       'unmanned aerial vehicle', 'passenger air vehicle', 'UAM', 'urban air mobility', 'DreamMaker', 'Butterfly', 'Vahana'],\n",
    "                          'bad_words':['brochure'],\n",
    "                          'ideal_no_articles': 35,\n",
    "                          'max_age':7,\n",
    "                          'news_download_filename':f'eVTOL_news_updates/{search_date_str}_eVTOL_news_download.xls',\n",
    "                          'deletions_filename':f'eVTOL_news_updates/{search_date_str}_eVTOL_news_deletions.xls',\n",
    "                          'make_deletions': False,\n",
    "                          'gen_docx': False,\n",
    "                          'auto_id_research': True,\n",
    "                          'news_df': None\n",
    "                         },\n",
    "                'INL':{'day':3,\n",
    "                       'essential_keywords':[],\n",
    "                       'general_keywords':['material', 'polymer', 'composite', 'next gen', 'next-gen', 'suspension', 'powertrain'],\n",
    "                       'specific_keywords':['carbon fiber', 'carbon-fiber', 'polymer', \n",
    "                                   'lightweight', 'lightweight material', 'multi-material', 'multi material', \n",
    "                                   'multimaterial', 'nanotech', 'nanomaterial', 'glazing', 'suspension', 'spider silk', \n",
    "                                   'carbon reinforced polymer', 'corrosion', 'corrosion resistance', 'corrosion-resistant',\n",
    "                                   'corrosion resistant', 'carbon-reinforced polymer', 'carbon-reinforced', 'carbon reinforced'],\n",
    "                       'bad_words':['battery material', 'cathode design', 'anode design', 'separator design', 'battery cell', 'battery module','brochure', \"Today's Car News\", \"This Week in Reverse\"],\n",
    "                       'ideal_no_articles': 1000,\n",
    "                       'max_age':31,\n",
    "                       'news_download_filename':f'INL_news_updates/{search_date_str}_INL_news_download.xls',\n",
    "                       'deletions_filename':f'INL_news_updates/{search_date_str}_INL_news_deletions.xls',\n",
    "                       'make_deletions': True,\n",
    "                       'gen_docx': True,\n",
    "                       'docx_title':'INL Monthly News Update',\n",
    "                       'docx_intro':None,\n",
    "                       'docx_headers':{ 1: 'Carbon Fiber and Polymer Composites',\n",
    "                                        2: 'Lightweight Materials', \n",
    "                                        3: 'Multi-Material Joining',\n",
    "                                        4: 'Nanomaterials', \n",
    "                                        5: 'Next Generation Materials and Vehicles',\n",
    "                                        6: 'Propulsion Materials',                                       \n",
    "                                      },\n",
    "                       'auto_id_research': False, # Determines whether research articles are automatically identified as such or not.  \n",
    "                                                 # If true, classifies all journal articles as \"Relevant Transportation Research\".\n",
    "                                                 # docx_headers must have a header called \"Relevant Transportation Research\" if this is set to True.\n",
    "                       'news_df': None\n",
    "                      } \n",
    "}\n",
    "# Generates an error message if AFV scraper is run on a day other than Wednesday.\n",
    "if scraper_info['AFV']['day'] != 2:\n",
    "    sys.exit('Error: AFV scraper must ony be run on Wednesdays.')\n",
    "\n",
    "# Used for diagnostics/tracking later\n",
    "scrape_specs = {}\n",
    "\n",
    "# Needed for web scraping \"browser\"\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'}\n",
    "\n",
    "# For database update; ensures duplicates aren't loaded\n",
    "db_update = False\n",
    "\n",
    "# Maximum number of sentences to be scraped for each article pulled.\n",
    "max_sentences = 5\n",
    "\n",
    "# Get a list of scrapers we wish to run today.\n",
    "todays_scrapers = [scraper for scraper in scraper_info if scraper_info[scraper]['day']==search_date.weekday()]\n",
    "\n",
    "# List of all scrapers in scraper_info\n",
    "all_scrapers = list(scraper_info.keys())\n",
    "\n",
    "try:\n",
    "    max_age = max([scraper_info[scraper]['max_age'] for scraper in scraper_info if scraper in todays_scrapers])\n",
    "except:\n",
    "    print('No scrapers are run on this weekday.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test a single scraper (i.e. CAV, 21CTP, etc.)\n",
    "To run a single scraper, set single_scraper = True and enter a scraper name in the input box. AFV scraper can only be run on Wednesdays due to its dependency on \"EVSE Market Analysis.ipynb.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Which scraper would you like to test?\n",
      "Options:\n",
      "21CTP\n",
      "CAV\n",
      "AFV\n",
      "Hyperloop\n",
      "eVTOL\n",
      "INL\n",
      " 21CTP\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21CTP selected.\n"
     ]
    }
   ],
   "source": [
    "single_scraper = False\n",
    "if single_scraper:\n",
    "    choice = input('Which scraper would you like to test?\\nOptions:\\n'+ '\\n'.join(list(scraper_info.keys()))+'\\n')\n",
    "    choice = [scraper for scraper in scraper_info.keys() if choice.upper()==scraper.upper()][0]\n",
    "    while choice == '':\n",
    "        print('Please enter a valid input.')\n",
    "        choice = input('Which scraper would you like to test?\\nOptions:\\n', list(scraper_info.keys()))\n",
    "        choice = [scraper for scraper in scraper_info.keys() if choice.upper()==scraper.upper()][0]\n",
    "    print(choice, 'selected.')\n",
    "    if (choice == 'AFV') and (search_date.weekday() != 2):\n",
    "        sys.exit('Error: AFV scraper must ony be run on Wednesdays.')\n",
    "    else:\n",
    "        for scraper in scraper_info:\n",
    "            if scraper == choice:\n",
    "                scraper_info[scraper]['day'] = search_date.weekday()\n",
    "            else:\n",
    "                scraper_info[scraper]['day'] = None\n",
    "        # Get a list of scrapers we wish to run today.\n",
    "        todays_scrapers = [scraper for scraper in scraper_info if scraper_info[scraper]['day']==search_date.weekday()]\n",
    "\n",
    "        # List of all scrapers in scraper_info\n",
    "        all_scrapers = list(scraper_info.keys())\n",
    "        \n",
    "        max_age = max([scraper_info[scraper]['max_age'] for scraper in scraper_info if scraper in todays_scrapers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21CTP scraper(s) will be run. Max age of articles is 7 days.\n"
     ]
    }
   ],
   "source": [
    "print(', '.join(todays_scrapers), 'scraper(s) will be run. Max age of articles is', max_age, 'days.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limit sites to scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful if you want to generate an entire news report restricted to just these sources.  Also useful if you just want to test the report-generating functions (e.g. gen_docx) and don't want to run the entire scraper.  Also a good way to test multiple sites at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_sites = False\n",
    "sites_to_scrape = ['TechCrunch'] #['Lightweighting World', 'Autoblog']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for pulling keywords, formatting scraper output and generating word doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T13:30:37.503023Z",
     "start_time": "2019-01-07T13:30:37.468125Z"
    }
   },
   "outputs": [],
   "source": [
    "# scraped_count = 0\n",
    "# skip_count = 0\n",
    "# too_old = 0\n",
    "# iteration = 0\n",
    "# skip_ind = []\n",
    "# old_ind = []\n",
    "#def click_button(driver, method, css, s = ''):\n",
    " #   button = eval(f'driver.find_element{s}_by_{method}')\n",
    "  #  button.click()\n",
    "    \n",
    "    \n",
    "def replace_em(text):\n",
    "    '''Replaces odd characters in text. Used for page titles and summaries'''\n",
    "    bad_chars = ['â€œ', 'â€™', 'â€�', '\\n', 'Â',\n",
    "                 'â€”', '(earlier post)', 'â€?', '\\t', 'â€œ', '(TNS) — ', '(Reuters) - ',\n",
    "                 'DUBAI (Reuters) - ', '(adsbygoogle = window.adsbygoogle || []).push({});']\n",
    "    for bad_char in bad_chars:\n",
    "        text = text.replace(bad_char, '')\n",
    "    return text\n",
    "\n",
    "def try_locs(loc_type, loc, article):\n",
    "    if type(loc)==list:\n",
    "        for option in loc:\n",
    "            try:\n",
    "                scraping = eval(option)\n",
    "                break # Break out of the for loop once the summary/date/etc. is found.\n",
    "            except:\n",
    "                scraping = ''\n",
    "                continue # Continue trying different options if summary/date/etc. has not been found.\n",
    "    elif type(loc)==str:\n",
    "        scraping = eval(loc)\n",
    "    else:\n",
    "        sys.exit(f'Error: self.{loc_type}_loc must be a string or a list of strings.')\n",
    "    return scraping\n",
    "\n",
    "# Pars must either be a list of beautifulsoup paragraph objects OR a string.\n",
    "def clean_summary(pars):\n",
    "    # Clean article paragraphs and join them into one block of text\n",
    "    try:\n",
    "        all_text = replace_em(' '.join(par.text for par in pars)) \n",
    "    except:\n",
    "        all_text = replace_em(pars)\n",
    "    # Split the article text up into sentences\n",
    "    sentence_list = tokenizer.tokenize(all_text)\n",
    "    # If the number of sentences is greater than max_sentences, return the first max_sentences sentences.\n",
    "    if len(sentence_list) > max_sentences:\n",
    "        return ' '.join([sentence.strip() for sentence in sentence_list[0:max_sentences]])\n",
    "    # Otherwise, return all sentences.\n",
    "    else:\n",
    "        return ' '.join(' '.join([sentence.strip() for sentence in sentence_list]).split())\n",
    "\n",
    "def grab_homepage(url):\n",
    "    '''Creates BeautifulSoup object using input url'''\n",
    "#     headers = {'user-agent': 'Mozilla/5.0'}\n",
    "    page_1 = requests.get(url, headers=headers)\n",
    "    return BeautifulSoup(page_1.content, 'lxml')\n",
    "\n",
    "def print_results(site, scraped_count, skip_count, too_old, df, duration, scrape_specs):\n",
    "    '''Prints out a quick summary of one website's full scraping and adds summary specs to scrape_specs dictionary'''\n",
    "    print(f'{scraped_count} {site} article(s) scraped')\n",
    "    print(f'{skip_count} {site} article(s) skipped due to error')\n",
    "    print(f'{too_old} {site} article(s) skipped due to age')\n",
    "    print(f'{df.shape[0]} relevant article(s) collected')\n",
    "    scrape_specs[f\"{site}\"] = {'Pages Scraped': scraped_count, 'Relevant Articles': df.shape[0], 'Errors': skip_count,\n",
    "                               'Too old': too_old, 'Time spent': duration}\n",
    "    return scrape_specs\n",
    "\n",
    "\n",
    "def page_scan(title, summary, url, date, source):\n",
    "    '''\n",
    "    Searches a web page title and summary for keywords; returns the dictionary object that is used to create \n",
    "    the final dataframe. Searches the title first; if the keyword is there, it doesn't search the summary.\n",
    "\n",
    "    Only searches for keywords specific to that day of the week's news update.\n",
    "    '''\n",
    "    bool_dict = {scraper:0 for scraper in all_scrapers}\n",
    "    title_scrape = title+' '+title.lower()\n",
    "    summary_scrape = summary+' '+summary.lower()\n",
    "    \n",
    "    for scraper in todays_scrapers:\n",
    "        # Gets a list of all keywords found in in title or summary.\n",
    "        essential_bool = 0\n",
    "        essential_keywords_found = []\n",
    "        if scraper_info[scraper]['essential_keywords']!=[]:\n",
    "            essential_bool = 1\n",
    "            essential_keywords_found = [keyword for keyword in list(set(scraper_info[scraper]['essential_keywords'])) if keyword in (title_scrape) or (keyword in summary_scrape)]\n",
    "        general_keywords_found = [keyword for keyword in list(set(scraper_info[scraper]['general_keywords'])) if keyword in (title_scrape) or (keyword in summary_scrape)]\n",
    "        specific_keywords_found = [keyword for keyword in list(set(scraper_info[scraper]['specific_keywords'])) if keyword in (title_scrape) or (keyword in summary_scrape)]\n",
    "        if (scraper_dict[source]['vehicle_specific']) and (len(general_keywords_found)>0 or len(specific_keywords_found)>0):\n",
    "            if not essential_bool:\n",
    "                bool_dict[scraper] = 1\n",
    "            else:\n",
    "                if len(essential_keywords_found)>=1:\n",
    "                    bool_dict[scraper]=1\n",
    "        elif (not scraper_dict[source]['vehicle_specific']) and (len(general_keywords_found)>1 or len(specific_keywords_found)>0) and (not essential_bool or len(essential_keywords_found)>1):\n",
    "            if not essential_bool:\n",
    "                bool_dict[scraper] = 1\n",
    "            else:\n",
    "                if len(essential_keywords_found)>=1:\n",
    "                    bool_dict[scraper]=1\n",
    "    if sum(bool_dict.values()) > 0:\n",
    "        scan_dict = {'title': title.strip(), 'summary': summary.strip(), 'link': url, 'source': source,\n",
    "                'date': date}\n",
    "        scan_dict.update({scraper:bool_dict[scraper] for scraper in all_scrapers})\n",
    "        return scan_dict\n",
    "    \n",
    "    else:\n",
    "        return 'Most definitely nope'\n",
    "\n",
    "\n",
    "# The following two functions are for the Word document output!\n",
    "\n",
    "def add_hyperlink(paragraph, url, text):\n",
    "    '''\n",
    "    :param paragraph: The paragraph we are adding the hyperlink to.\n",
    "    :param url: A string containing the required url\n",
    "    :param text: The text displayed for the url\n",
    "    :return: The hyperlink object\n",
    "    '''\n",
    "    # This gets access to the document.xml.rels file and gets a new relation id value\n",
    "    part = paragraph.part\n",
    "    r_id = part.relate_to(\n",
    "        url, docx.opc.constants.RELATIONSHIP_TYPE.HYPERLINK, is_external=True)\n",
    "\n",
    "    # Create the w:hyperlink tag and add needed values\n",
    "    hyperlink = docx.oxml.shared.OxmlElement('w:hyperlink')\n",
    "    hyperlink.set(docx.oxml.shared.qn('r:id'), r_id, )\n",
    "\n",
    "    # Create a w:r element\n",
    "    new_run = docx.oxml.shared.OxmlElement('w:r')\n",
    "\n",
    "    # Create a new w:rPr element\n",
    "    rPr = docx.oxml.shared.OxmlElement('w:rPr')\n",
    "\n",
    "    # bold the text\n",
    "    u = docx.oxml.shared.OxmlElement('w:b')\n",
    "    rPr.append(u)\n",
    "\n",
    "    # Join all the xml elements together add add the required text to the w:r element\n",
    "    new_run.append(rPr)\n",
    "    new_run.text = text\n",
    "    hyperlink.append(new_run)\n",
    "\n",
    "    paragraph._p.append(hyperlink)\n",
    "\n",
    "    return hyperlink\n",
    "\n",
    "\n",
    "def gen_docx(scraper, graph_bool = False, dwyer=True):\n",
    "    '''\n",
    "    Generates news Word doc using data file from web scrape\n",
    "    :param newstype: Either \"21CTP\", \"CAV\", or \"AFV\"\n",
    "    :param dwyer: If not running on Dwyer's computer, set this to False and put all needed files in the same directory\n",
    "    '''\n",
    "    \n",
    "    headers = scraper_info[scraper]['docx_headers']\n",
    "    \n",
    "    # select data file (xls) based on the newstype and date. Note that search_date_str is a global variable defined outside\n",
    "    # of this function. Each news update only happens once a week --> only one xls file per newstype per week --> can't just\n",
    "    # pick any old search_date_str and make a file.\n",
    "    if dwyer:\n",
    "        #Name of the excel file (standardized)\n",
    "        data_file = f\"{scraper.lower()}_news_updates/{search_date_str}_{scraper}_news_download.xls\"\n",
    "    else:\n",
    "        data_file = f\"{search_date_str}_{scraper}_news_download.xls\"\n",
    "    # Read the data in from the selected file\n",
    "    df = pd.read_excel(data_file)\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    df.category = df.category.astype('int64') # Cast category column to int\n",
    "    section_dict = {headers[header_no]: df[df.category==header_no].T.to_dict() for header_no in headers}\n",
    "\n",
    "    # Start creating the word doc\n",
    "    newsdoc = docx.Document(docx='python_docx.docx')\n",
    "\n",
    "    newsdoc.add_heading(scraper_info[scraper]['docx_title']+ ' - ' + f\"{search_date.strftime('%m/%d/%Y')}\", 0)\n",
    "    newsdoc.add_paragraph(' ')\n",
    "    if scraper_info[scraper]['docx_intro']!=None:\n",
    "        newsdoc.add_paragraph(scraper_info[scraper]['docx_intro'])\n",
    "    \n",
    "    # If it's an AFV day (i.e., Wednesday), add graphics generated by \"EVSE Market Analysis.ipynb\"\n",
    "    if scraper == 'AFV':\n",
    "        newsdoc.add_heading('EVSE Market Analysis', 1)\n",
    "        # Add EVSE bar chart if graph_bool is true\n",
    "        if graph_bool:\n",
    "            evse_bar_chart = EVSE_file_dict['EVSE_bar_chart']\n",
    "            newsdoc.add_paragraph().add_run().add_picture(evse_bar_chart, width=Inches(6))\n",
    "            CA_nums = EVSE_file_dict['CA_shares'] # Adjust this to actual string with numbers.  Read string in from CSV\n",
    "        # Otherwise create a placeholder where the graph can be added by hand\n",
    "        else:\n",
    "            evse_bar_chart = newsdoc.add_paragraph().add_run('INSERT EVSE BAR CHART HERE')\n",
    "            evse_bar_chart.font.bold = True\n",
    "            evse_bar_chart.font.size = Pt(16)\n",
    "            evse_bar_chart.font.highlight_color = WD_COLOR_INDEX.YELLOW\n",
    "            CA_nums='NEED TO INSERT' # Default string for the CA EVSE numbers (automatically populates the caption for the EVSE bar chart figure)\n",
    "        newsdoc.add_paragraph('Figure: Number of EVSE plugs (note: not stations) by state and charging level. '\n",
    "                              'CA is not included, since it would make the rest of the state numbers illegible. '\n",
    "                              f\"CA holds a disproportionately large share of the total EVSE plugs: {CA_nums} \"\n",
    "                              'of Level 1, Level 2, and DCFC plugs respectively. Data Source: U.S. DOE AFDC Station Locator.',\n",
    "                              style='Caption')\n",
    "        newsdoc.add_page_break()\n",
    "        newsdoc.add_paragraph('The table below summarizes overall changes in number of EV charging stations by state between '\n",
    "                              f\"{(search_date - dt.timedelta(7)).strftime('%m/%d/%Y')} and {search_date.strftime('%m/%d/%Y')}:\",\n",
    "                              style='Normal')\n",
    "        newsdoc.add_paragraph('Table 1: Change in number of EV charging stations by state, between '\n",
    "                              f\"{(search_date - dt.timedelta(7)).strftime('%m/%d/%Y')} and {search_date.strftime('%m/%d/%Y')}\",\n",
    "                              style='Caption')\n",
    "        evse_delta_table = newsdoc.add_paragraph().add_run('INSERT EVSE DELTA TABLE HERE')\n",
    "        evse_delta_table.font.bold = True\n",
    "        evse_delta_table.font.size = Pt(16)\n",
    "        evse_delta_table.font.highlight_color = WD_COLOR_INDEX.YELLOW\n",
    "        newsdoc.add_page_break()\n",
    "\n",
    "    for header in section_dict:\n",
    "        newsdoc.add_heading(header, 1)\n",
    "        if (header == 'Relevant Transportation Research') and ('research_intro' in scraper_info[scraper]):\n",
    "            newsdoc.add_paragraph(scraper_info[scraper]['research_intro'])\n",
    "\n",
    "        for row in section_dict[header]:\n",
    "            row = section_dict[header][row]\n",
    "            newsdoc.add_heading(row['title'], level=2)\n",
    "            p = newsdoc.add_paragraph(row['summary'] + ' ')\n",
    "            p.add_run('(')\n",
    "            # This is where the add_hyperlink function is used\n",
    "            add_hyperlink(p, '{}'.format(row['link']), '{}'.format(row['source']))\n",
    "            p.add_run(')')\n",
    "            \n",
    "    if dwyer:\n",
    "        filename = f\"{scraper.lower()}_news_updates/Energetics {scraper} News Update - {search_date_str}.docx\"\n",
    "        newsdoc.save(filename)\n",
    "    else:\n",
    "        filename = f\"Energetics {scraper} News Update - {search_date_str}.docx\"\n",
    "        newsdoc.save(filename)\n",
    "    return filename\n",
    "\n",
    "\n",
    "def which_keyword_found(row):\n",
    "    ''' Identifies and stores which keywords triggered the news item pull '''\n",
    "    words_found = []\n",
    "    todays_keywords = []\n",
    "    for scraper in todays_scrapers:\n",
    "        todays_keywords += scraper_info[scraper]['general_keywords']\n",
    "        todays_keywords += scraper_info[scraper]['specific_keywords']\n",
    "    for keyword in list(set(todays_keywords)):\n",
    "        try:\n",
    "            if (row['summary'].find(keyword) > 0) | (row['title'].find(keyword) > 0):\n",
    "                words_found.append(keyword)\n",
    "        except:\n",
    "            continue\n",
    "    return ', '.join(words_found)\n",
    "\n",
    "\n",
    "def keyword_pull(string):\n",
    "    ''' Pulls all relevant capitalized words out of the title, as a quick \"keyword\" list '''\n",
    "    not_keywords = ['A', 'New', 'First', 'Group', 'The', 'This', 'I', 'To', 'Who', 'Silicon', 'Valley', 'System', 'Build', 'Payment', 'Business', 'API', 'JV', 'JVs',\n",
    "                    'European', 'American', 'America', 'Europe', 'China', 'But', 'Are', 'They', 'Legal', 'Says', 'AV', 'Revolution', 'Is',\n",
    "                    'TechCrunch', 'For', 'EVs', 'Really', 'Get', 'Money', 'Adds', 'We', 'All', 'Starts', 'Return', 'Apart',\n",
    "                    'Them', 'Cities', 'After', 'Insurance', 'Back', 'Against', 'Would', 'Displace', 'Improves', 'While',\n",
    "                    'That', 'You', 'Find', 'Along', 'From', 'Their', 'Not', 'So', 'Say', 'Experts', 'Drivers', 'Its', 'Into', 'Fully',\n",
    "                    'Ranks', 'Stretch', 'SUV', 'Data', 'Sharing', 'Live', 'When', 'Agencies', 'Still', 'Trying', 'Program', 'Offer', 'Four',\n",
    "                    'Will', 'Backs', 'Just', 'Around', 'Years', 'Its', 'Future', 'Deploying', 'Objects', 'Distance', 'Highlights']\n",
    "    string = [w for w in string.replace(';', '').replace(',', '').lstrip().split(' ') if w!='']\n",
    "    keywords = [word for word in string if (word[0].isupper()) & (word not in not_keywords)]\n",
    "    return ', '.join(keywords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a scraper class that will be used for each website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T13:30:37.531950Z",
     "start_time": "2019-01-07T13:30:37.511002Z"
    }
   },
   "outputs": [],
   "source": [
    "class scraypah:\n",
    "    '''\n",
    "    Scraypah is a web scraper that searches through all of the recent articles on a website and extracts key information\n",
    "    from those that include relevant keywords. It requires a dictionary of parameters specific to each website that needs\n",
    "    to be scraped. See the __init__ docstring for information on the input parameter requirements.\n",
    "    '''\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"This is an object of class scraypah!\"\n",
    "\n",
    "    def __init__(self, params):\n",
    "        '''\n",
    "        Attributes:\n",
    "            params[url] (str): Homepage of the website, where each of the article page links are extracted from\n",
    "            params[source] (str): Name of the website\n",
    "            params[strain_bool] (bool): Is there a soup strainer for this website or not?\n",
    "            params[strain_tag] (str): Tag used for soup strainer\n",
    "            params[strain_attr_name] (str): Attribute name used for soup strainer\n",
    "            params[strain_attr_value] (str): Attribute value used for soup strainer\n",
    "            params[date_loc] (str): Location of the date in the HTML\n",
    "            params[date_format] (str): Allows user to set the date format, if the format on the website does not parse automaticall\n",
    "            params[sum_loc] (str): Location of the summary in the HTML (this is typically the first 3 paragraphs of the article)\n",
    "            params[title_loc] (str): Location of the title in the HTML\n",
    "            params[url_list_query] (str): BeautifulSoup code to extract the list of articles from the website homepage(s) (url)\n",
    "        '''\n",
    "        self.base_url = params['url']\n",
    "        self.source = params['source']\n",
    "        self.strainer = params['strain_bool']\n",
    "        if self.strainer:\n",
    "            self.strain_tag = params['strain_tag']\n",
    "            self.strain_attr_name = params['strain_attr_name']\n",
    "            self.strain_attr_value = params['strain_attr_value']\n",
    "        self.date_loc = params['date_loc']\n",
    "        self.date_format = params['date_format']\n",
    "        self.sum_loc = params['sum_loc']\n",
    "        self.title_loc = params['title_loc']\n",
    "        self.url_list_query = params['url_list_query']\n",
    "        self.css = params['css_bool']\n",
    "        if self.css:\n",
    "            self.load_more_css = params['load_more_css']\n",
    "            if self.load_more_css!=None:\n",
    "                self.max_loads = params['max_loads']\n",
    "            self.date_css = params['date_css']\n",
    "            self.title_css = params['title_css']\n",
    "            self.sum_css = params['sum_css']\n",
    "        self.time_sleep = params['time_sleep']\n",
    "            \n",
    "\n",
    "    def get_urls(self):\n",
    "        '''Populates self.urls_to_scrape with a list of urls extracted from the website homepage(s)'''\n",
    "        self.urls_to_scrape = []\n",
    "        with requests.Session() as s:\n",
    "\n",
    "            # Checks if the base_url is a single url or a list of urls - some websites publish enough articles\n",
    "            # that we have to pull multiple pages\n",
    "            if isinstance(self.base_url, str):\n",
    "\n",
    "                # Checks if there is a \"soup strainer\" for the website being scraped. See here:\n",
    "                # https://www.crummy.com/software/BeautifulSoup/bs4/doc/#parsing-only-part-of-a-document\n",
    "                if not self.strainer:\n",
    "                    page = requests.get(self.base_url, headers=headers)\n",
    "                    time.sleep(self.time_sleep['get_urls']['parse_page'])\n",
    "                    self.base_soup = BeautifulSoup(page.content, \"lxml\")\n",
    "                else:\n",
    "                    only_parse = SoupStrainer(self.strain_tag, attrs={\n",
    "                                              self.strain_attr_name: self.strain_attr_value})\n",
    "                    self.base_soup = BeautifulSoup(requests.get(\n",
    "                        self.base_url, headers=headers).content, \"lxml\", parse_only=only_parse)\n",
    "\n",
    "                time.sleep(self.time_sleep['get_urls']['get_page'])\n",
    "                self.urls_to_scrape = eval(self.url_list_query)\n",
    "\n",
    "            else:\n",
    "                for url in list(self.base_url):\n",
    "                    if not self.strainer:\n",
    "                        page = requests.get(url, headers=headers)\n",
    "                        time.sleep(self.time_sleep['get_urls']['parse_page'])\n",
    "                        self.base_soup = BeautifulSoup(page.content, \"lxml\")\n",
    "                    else:\n",
    "                        only_parse = SoupStrainer(self.strain_tag, attrs={\n",
    "                                                  self.strain_attr_name: self.strain_attr_value})\n",
    "                        self.base_soup = BeautifulSoup(requests.get(\n",
    "                                url, headers=headers).content, \"lxml\", parse_only=only_parse)\n",
    "                    time.sleep(self.time_sleep['get_urls']['get_page'])\n",
    "                    self.urls_to_scrape += eval(self.url_list_query)\n",
    "        self.urls_to_scrape = list(set(self.urls_to_scrape))\n",
    "    \n",
    "    def scrape_em(self):\n",
    "        self.relevant_articles = {}\n",
    "        self.scraped_count = 0\n",
    "        self.skip_count = 0\n",
    "        self.too_old = 0\n",
    "        self.iteration = 0\n",
    "        self.skip_ind = []\n",
    "        self.old_ind = []\n",
    "        for url in self.urls_to_scrape:\n",
    "            time.sleep(self.time_sleep['scrape_em'])\n",
    "            self.iteration += 1\n",
    "            summary = None\n",
    "            title = None\n",
    "            date = None\n",
    "            try:\n",
    "                with requests.Session() as s:\n",
    "                    page = s.get(url, headers=headers)\n",
    "                    article = BeautifulSoup(page.content, \"lxml\")\n",
    "                    date = pd.to_datetime(try_locs('date', self.date_loc, article).strip().replace(\n",
    "                        '\\\\xa0', '').replace(' -\\nBy:', ''), format=self.date_format).date()\n",
    "                    if (date - search_date).days >= -max_age:\n",
    "                        summary = clean_summary(try_locs('sum', self.sum_loc, article)) \n",
    "                        title = try_locs('title', self.title_loc, article).replace('â€™', \"'\").replace('\\\\xa0', ' ').replace('\\\\n', '').lstrip().replace('  ', '')\n",
    "                        temp = page_scan(title, summary, url, date, self.source)\n",
    "                        if temp != 'Most definitely nope':\n",
    "                            self.relevant_articles[self.scraped_count] = temp\n",
    "                        self.scraped_count += 1\n",
    "                    else:\n",
    "                        self.too_old += 1\n",
    "                        self.old_ind.append(self.iteration-1)\n",
    "                        #break # Remove break if you want to scrape older articles or see how many articles will be scraped.\n",
    "                        continue\n",
    "            except Exception as exc:\n",
    "                print(\n",
    "                    f\"{str(exc)}: {url} \\ndate:{date}\\ntitle:{title}\\nsummary:{summary}\")\n",
    "                self.skip_count += 1\n",
    "                self.skip_ind.append(self.iteration-1)\n",
    "                continue\n",
    "        self.relevant_df = pd.DataFrame.from_dict(self.relevant_articles).T\n",
    "        if not self.relevant_df.empty:\n",
    "            self.relevant_df.drop_duplicates('link', inplace=True)\n",
    "    \n",
    "    def css_scrape_em(self):          \n",
    "\n",
    "        self.relevant_articles = {}\n",
    "        self.scraped_count = 0\n",
    "        self.skip_count = 0\n",
    "        self.too_old = 0\n",
    "        self.iteration = 0\n",
    "        self.skip_ind = []\n",
    "        self.old_ind = []\n",
    "        self.scraped_count = 0\n",
    "        papers = {}\n",
    "    \n",
    "        # Important to define these-- otherwise variables could be referenced before assignment.\n",
    "        title = ''\n",
    "        date = ''\n",
    "        summary = ''\n",
    "    \n",
    "        if type(self.base_url) != list:\n",
    "            self.base_url = [self.base_url]\n",
    "        for base_url in self.base_url:\n",
    "            load_button = ''\n",
    "            load_count = 0\n",
    "            still_more = True\n",
    "            try:\n",
    "                driver.get(base_url)\n",
    "            except TimeoutException as ex:\n",
    "                print(ex)\n",
    "                driver.refresh()\n",
    "            time.sleep(self.time_sleep['css_scrape_em']['base_url']) # Wait for page to load - otherwise certain elements won't appear\n",
    "            self.base_soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "            # Click on CSS element to load more articles if available.\n",
    "            if self.load_more_css != None:\n",
    "                for loadNo in range(self.max_loads):\n",
    "                    try:\n",
    "                        time.sleep(self.time_sleep['css_scrape_em']['sub_url'])\n",
    "                        eval(self.load_more_css).click()\n",
    "                    except Exception as exc:\n",
    "                        pass\n",
    "                self.base_soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "            self.urls_to_scrape = eval(self.url_list_query)\n",
    "        \n",
    "            for url in self.urls_to_scrape:\n",
    "                title = ''\n",
    "                date = ''\n",
    "                summary = ''\n",
    "                time.sleep(self.time_sleep['css_scrape_em']['sub_url'])\n",
    "                self.iteration += 1\n",
    "                bad_egg = False\n",
    "                if not still_more:\n",
    "                    break\n",
    "                # Open article URL using selenium.\n",
    "                try:\n",
    "                    driver.get(url)\n",
    "                except TimeoutException as ex:\n",
    "                    print(ex)\n",
    "                    driver.refresh()\n",
    "                article = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "                # Get article publication date, title, and summary.\n",
    "                try:\n",
    "                    # Click on CSS element to reveal the location of the publication date if needed.\n",
    "                    if self.date_css != None:\n",
    "                        eval(self.date_css).click()\n",
    "                        article = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "                    # Scrape the publication date.\n",
    "                    date = pd.to_datetime(try_locs('date', self.date_loc, article).strip().replace(\n",
    "                        '\\\\xa0', '').replace(' -\\nBy:', ''), format=self.date_format).date()\n",
    "                    # If the publication date shows the article is not too old (i.e., it's more recent than than max_age (in days)), \n",
    "                    # scrape it and see if it contains relevant keywords\n",
    "                    if ((date - search_date).days >= -max_age):\n",
    "                        # Click on CSS element to reveal the location of the title if needed.\n",
    "                        if self.title_css != None:\n",
    "                            eval(self.title_css).click()\n",
    "                            article = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "                        # Scrape and clean the title.\n",
    "                        title = try_locs('title', self.title_loc, article)\n",
    "                        title = replace_em(title)\n",
    "                        # Click on CSS element to reveal the location of the summary if needed.\n",
    "                        if self.sum_css != None:\n",
    "                            eval(self.sum_css).click()\n",
    "                            article = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "                        # Scrape and clean the summary.\n",
    "                        summary = clean_summary(try_locs('sum', self.sum_loc, article)) \n",
    "                        temp = page_scan(title, summary, url,\n",
    "                                         date, self.source)\n",
    "                        if temp != 'Most definitely nope':\n",
    "                            self.relevant_articles[self.scraped_count] = temp\n",
    "                        self.scraped_count += 1\n",
    "                    else:\n",
    "                        self.too_old += 1\n",
    "                        self.old_ind.append(self.iteration-1)\n",
    "                        still_more = False\n",
    "               \n",
    "            \n",
    "                except Exception as exc:\n",
    "                    print(f\"{str(exc)}: {url} \\ndate:{date}\\ntitle:{title}\\nsummary:{summary}\")\n",
    "                    self.skip_count += 1\n",
    "                    self.skip_ind.append(self.iteration-1)\n",
    "                    continue \n",
    "            \n",
    "            \n",
    "        self.relevant_df = pd.DataFrame.from_dict(self.relevant_articles).T\n",
    "        if not self.relevant_df.empty:\n",
    "            self.relevant_df.drop_duplicates('link', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"scraper_dict\"></a>\n",
    "## Set parameters for each website you want to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T13:30:37.588794Z",
     "start_time": "2019-01-07T13:30:37.551893Z"
    }
   },
   "outputs": [],
   "source": [
    "# 'article' is the variable that stores the BeautifulSoup soup for a particular article page. e.g. \"energy.gov/some-article.\"\n",
    "# The values of date_loc, sum_loc, and title_loc should be the BeautifulSoup commands for accessing the date location, \n",
    "# summary location and title location, respectively.\n",
    "# News sources are rated from 1-3 (1-Best, 3-Worst).  Sources with a rating of 0 (e.g., academic journals) will be kept no matter what.\n",
    "# journal_bool is True for academic journals.  This way academic journal articles can be autocategorized later.\n",
    "\n",
    "scraper_dict = {'MIT': {'url': 'http://news.mit.edu/mit-news',\n",
    "                        'source': 'MIT',\n",
    "                        'css_bool': False,\n",
    "                        'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.1}, 'scrape_em':.1},\n",
    "                        'strain_tag': 'ul',\n",
    "                        'strain_attr_name': 'class',\n",
    "                        'strain_attr_value': 'view-mit-news clearfix',\n",
    "                        'url_list_query': \"['http://news.mit.edu'+item.a['href'] for item in self.base_soup.find('ul', class_='view-mit-news clearfix').find_all('li')]\",\n",
    "                        'date_loc': \"article.find('span', attrs={'itemprop':'datePublished'}).text\",\n",
    "                        'date_format': None,\n",
    "                        'sum_loc': \"article.find('div', attrs={'class': 'field-item even'}).find_all('p')\",\n",
    "                        'title_loc': \"article.find('h1', attrs={'class':'article-heading'}).text\",\n",
    "                        'strain_bool': True,\n",
    "                        'journal_bool': False,\n",
    "                        'vehicle_specific': False,\n",
    "                        'rating': 1},\n",
    "                'Semiconductor Engineering': {'url': 'http://semiengineering.com/category-main-page-iot-security/',\n",
    "                                              'source': 'Semiconductor Engineering',\n",
    "                                              'css_bool': False,\n",
    "                                              'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.1}, 'scrape_em':.1},\n",
    "                                              'strain_tag': 'div',\n",
    "                                              'strain_attr_name': 'class',\n",
    "                                              'strain_attr_value': 'l_col',\n",
    "                                              'url_list_query': \"[item['href'] for item in self.base_soup.find('div', class_='l_col').find_all('a', href=True,title=True)]\",\n",
    "                                              'date_loc': \"article.find('div',class_='loop_post_meta').contents[0]\",\n",
    "                                              'date_format': None,\n",
    "                                              'sum_loc': \"article.find('div', class_='post_cnt post_cnt_first_letter').find_all('p')\",\n",
    "                                              'title_loc': \"article.find('h1', class_='post_title').text\",\n",
    "                                              'strain_bool': True,\n",
    "                                              'journal_bool': False,    \n",
    "                                              'vehicle_specific': False,                   \n",
    "                                              'rating': 1},\n",
    "                \n",
    "                'Quartz': {'url': 'https://qz.com/search/self-driving',\n",
    "                           'source': 'Quartz',\n",
    "                           'css_bool': False,\n",
    "                           'time_sleep': {'get_urls':{'get_page':.2, 'parse_page':.2}, 'scrape_em':.2},\n",
    "                           'strain_tag': 'a',\n",
    "                           'strain_attr_name': 'class',\n",
    "                           'strain_attr_value': '_5ff1a',\n",
    "                           'url_list_query': \"['https://qz.com' + a['href'] for a in self.base_soup.find_all('a', class_='_5ff1a')]\",\n",
    "                           'date_loc': \"article.time.text\",\n",
    "                           'date_format': None,\n",
    "                           'sum_loc': \"article.find_all('p')\",\n",
    "                           'title_loc': \"article.h1.text\",\n",
    "                           'strain_bool': True,\n",
    "                           'journal_bool': False,\n",
    "                           'vehicle_specific': True,\n",
    "                           'rating': 2},\n",
    "                            # Note: member exclusive articles for Quartz will be skipped.\n",
    "                'Recode': {'url': 'https://www.recode.net/',\n",
    "                           'source': 'Recode',\n",
    "                           'css_bool': False,\n",
    "                           'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.1}, 'scrape_em': .1},\n",
    "                           'strain_tag': 'a',\n",
    "                           'strain_attr_name': 'data-analytics-link',\n",
    "                           'strain_attr_value': 'article',\n",
    "                           'url_list_query': \"[item['href'] for item in self.base_soup.find_all('a', attrs={'data-analytics-link':'article'})]\",\n",
    "                           'date_loc': \"article.time.text.replace('\\\\n', '')\",\n",
    "                           'date_format': None,\n",
    "                           'sum_loc': \"article.find_all('p')\",\n",
    "                           'title_loc': \"article.h1.text\",\n",
    "                           'strain_bool': True,\n",
    "                           'journal_bool': False,\n",
    "                           'vehicle_specific': False,\n",
    "                           'rating': 2},\n",
    "                # Gov tech has errors due to pop up window, I think - can try solving this using selenium to click out of the window.\n",
    "                'GovTech': {'url': 'http://www.govtech.com/fs/transportation/',\n",
    "                            'source': 'GovTech',\n",
    "                            'css_bool': False,\n",
    "                            'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.1}, 'scrape_em': .1},\n",
    "                            'url_list_query': \"[header.a['href'] for header in self.base_soup.find_all('h2')]\",\n",
    "                            'date_loc': \"article.find('span', class_='date').text.strip()\",\n",
    "                            'date_format': None,\n",
    "                            'sum_loc':\"article.find('div', 'Section1').find_all('p')\",\n",
    "                            'title_loc': \"article.find('h1').text.strip()\",\n",
    "                            'strain_bool': False,\n",
    "                            'journal_bool': False,\n",
    "                            'vehicle_specific': True,\n",
    "                            'rating': 1},\n",
    "                'Reuters': {'url': [f'https://www.reuters.com/news/archive/technologynews?view=page&page={page_no}' for page_no in range(50)],\n",
    "                            'source': 'Reuters',\n",
    "                            'css_bool': False,\n",
    "                            'time_sleep': {'get_urls':{'get_page':.01, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                            'strain_tag': 'div',\n",
    "                            'strain_attr_name': 'class',\n",
    "                            'strain_attr_value': 'story-content',\n",
    "                            'url_list_query': \"['https://www.reuters.com'+item.a['href'] for item in self.base_soup.find_all('div', class_='story-content')]\",\n",
    "                            'date_loc': \"article.find('div', attrs={'class':'ArticleHeader_date'}).text.split('/')[0]\",\n",
    "                            'date_format': '%B %d, %Y',\n",
    "                            'sum_loc': \"article.find('div', attrs={'class':'StandardArticleBody_body'}).find_all('p')\",\n",
    "                            'title_loc': \"article.h1.text\",\n",
    "                            'strain_bool': True,\n",
    "                            'journal_bool': False,\n",
    "                            'vehicle_specific': False,\n",
    "                            'rating': 2},\n",
    "                'CityLab': {'url': [f'https://www.citylab.com/transportation/?page={page_no}' for page_no in range(1,3)],\n",
    "                            'source': 'CityLab',\n",
    "                            'css_bool': False,\n",
    "                            'time_sleep': {'get_urls':{'get_page':.2, 'parse_page':.2}, 'scrape_em':.2},\n",
    "                            'strain_tag': ['h2', 'h1'],\n",
    "                            'strain_attr_name': 'class', \n",
    "                            'strain_attr_value': ['c-promo__hed', 'c-river-item__hed c-river-item__hed--'],\n",
    "                            'url_list_query': \"[item.a['href'] for item in self.base_soup.find_all(['h1','h2'], class_=['c-promo__hed','c-river-item__hed c-river-item__hed--'])]\",\n",
    "                            'date_loc': \"article.time.text\",\n",
    "                            'date_format': None,\n",
    "                            'sum_loc': \"article.find('section', 's-article__section o-small-container').find_all('p')\",\n",
    "                            'title_loc': \"article.h1.text\",\n",
    "                            'strain_bool': True,\n",
    "                            'journal_bool': False,\n",
    "                            'vehicle_specific': True,\n",
    "                            'rating': 1},\n",
    "                'Autoblog': {'url': [f'https://www.autoblog.com/archive/pg-{page_no}' for page_no in range(1,25)],\n",
    "                             'source': 'Autoblog',\n",
    "                             'css_bool': False,\n",
    "                             'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                             'strain_tag': 'h6',\n",
    "                             'strain_attr_name': 'class',\n",
    "                             'strain_attr_value': 'record-heading',\n",
    "                             'url_list_query': \"['https://www.autoblog.com' + header.a['href'] for header in self.base_soup.find_all('h6', class_ = 'record-heading') if 'http' not in header.a['href']]\",\n",
    "                             'date_format': None,\n",
    "                             'date_loc': \"article.find('div', class_='post-date').text.strip().split(' at')[0]\",\n",
    "                             'sum_loc': [\"article.find('div', attrs={'class':'post-body'}).find_all('p')\",\"article.find('div', class_='post-body').text\"],\n",
    "                             'title_loc': \"article.h1.text\",\n",
    "                             'strain_bool': True,\n",
    "                             'journal_bool': False,\n",
    "                             'vehicle_specific': True,\n",
    "                             'rating': 3},\n",
    "                'Electrek': {'url': ['https://electrek.co/'] + \n",
    "                                     ['https://electrek.co/page/' + str(i) for i in range(2,30)],\n",
    "                             'source': 'Electrek',\n",
    "                             'css_bool': False,\n",
    "                             'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                             'strain_tag': 'h1',\n",
    "                             'strain_attr_name': 'class', \n",
    "                             'strain_attr_value': 'post-title',\n",
    "                             'url_list_query': \"[item.a['href'] for item in self.base_soup.find_all('h1', class_='post-title')]\",\n",
    "                             'date_loc': \"article.find('p', class_='time-twitter').text\",\n",
    "                             'date_format': None,\n",
    "                             'sum_loc': \"article.find('div', class_='post-body').find_all('p')[1:]\",\n",
    "                             'title_loc': \"article.find('h1', class_='post-title').text\",\n",
    "                             'strain_bool': True,\n",
    "                             'journal_bool': False,\n",
    "                             'vehicle_specific': True,\n",
    "                             'rating': 3},\n",
    "                'The Verge': {'url': [f'https://www.theverge.com/transportation/archives/{page_no}' for page_no in range(1,3)],\n",
    "                              'source': 'The Verge',\n",
    "                              'css_bool': False,\n",
    "                              'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                              'strain_tag': 'h2',\n",
    "                              'strain_attr_name': 'class', \n",
    "                              'strain_attr_value': 'c-entry-box--compact__title',\n",
    "                              'url_list_query': \"[item.a['href'] for item in self.base_soup.find_all('h2', class_='c-entry-box--compact__title')]\",\n",
    "                              'date_loc': \"article.time.text\",\n",
    "                              'date_format': None,\n",
    "                              'sum_loc': \"article.find_all('p')\",\n",
    "                              'title_loc': \"article.h1.text\",\n",
    "                              'strain_bool': True,\n",
    "                              'journal_bool': False,\n",
    "                              'vehicle_specific': True,\n",
    "                              'rating': 2},\n",
    "                'Crunchbase': {'url': [f'https://news.crunchbase.com/page/{page_no}' for page_no in range(1,14)],\n",
    "                               'source': 'Crunchbase',\n",
    "                               'css_bool': False,\n",
    "                               'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                               'strain_tag': 'h2',\n",
    "                               'strain_attr_name': 'class',\n",
    "                               'strain_attr_value': 'entry-title h3',\n",
    "                               'url_list_query': \"[header.a['href'] for header in self.base_soup.find_all('h2', 'entry-title h3') if header.a!=None]\",\n",
    "                               'date_loc': \"article.find('div', class_='meta-item herald-date').text\",\n",
    "                               'date_format': None,\n",
    "                               'sum_loc': \"article.find('div', class_='entry-content herald-entry-content').find_all('p')\",\n",
    "                               'title_loc': \"article.h1.text\",\n",
    "                               'strain_bool': True,\n",
    "                               'journal_bool': False,\n",
    "                               'vehicle_specific': False,\n",
    "                               'rating': 3},\n",
    "                'Truck News': {'url': [f'https://www.trucknews.com/news/page/{page_no}/' for page_no in range(1,11)],\n",
    "                               'source': 'Truck News',\n",
    "                               'css_bool': False,\n",
    "                               'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                               'strain_tag': 'ul',\n",
    "                               'strain_attr_name': 'class',\n",
    "                               'strain_attr_value': 'media-list',\n",
    "                               'url_list_query': \"[item.a['href'] for item in self.base_soup.find('ul', class_='media-list').find_all('h4')]\",\n",
    "                               'date_loc': \"article.find('div', class_ = 'well').find('p').text.split('by')[0].strip()\",\n",
    "                               'date_format': None,\n",
    "                               'sum_loc': \"article.find('div', class_ = 'the-content').find_all('p')\",\n",
    "                               'title_loc': \"article.find('h2').text.strip()\",\n",
    "                               'strain_bool': False,\n",
    "                               'journal_bool': False,\n",
    "                               'vehicle_specific': True,\n",
    "                               'rating': 3},\n",
    "                'Trucks.com': {'url': ['https://www.trucks.com/category/news/'],\n",
    "                               'source': 'Trucks.com',\n",
    "                               'css_bool': False,\n",
    "                               'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                               'strain_tag': 'h2',\n",
    "                               'strain_attr_name': 'class',\n",
    "                               'strain_attr_value': 'cb-post-title',\n",
    "                               'url_list_query': \"[header.a['href'] for header in self.base_soup.find_all('h2', 'cb-post-title')]\",\n",
    "                               'date_loc': \"article.find('div', 'date-author').text.split(' by')[0].strip()\",\n",
    "                               'date_format': '%B %d, %Y',\n",
    "                               'sum_loc': \"article.find('section', itemprop = 'articleBody').find_all('p')\",\n",
    "                               'title_loc': \"article.h1.text\",\n",
    "                               'strain_bool': False,\n",
    "                               'journal_bool': False,\n",
    "                               'vehicle_specific': True,\n",
    "                               'rating': 2},\n",
    "                'TechCrunch': {'url': 'https://techcrunch.com/', \n",
    "                               'source': 'TechCrunch',\n",
    "                               'css_bool': True,\n",
    "                               'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.1}, 'css_scrape_em':{'base_url': .1, 'sub_url':.1}},\n",
    "                               'date_css': None,\n",
    "                               'sum_css':None,\n",
    "                               'title_css':None,\n",
    "                               'load_more_css':'driver.find_element_by_css_selector(\"button.load-more\")',\n",
    "                               'max_loads':40,\n",
    "                               'url_list_query': \"['https://techcrunch.com' + item['href'] for item in self.base_soup.find_all('a', class_='post-block__title__link')]\",\n",
    "                               'date_loc': \"driver.find_element_by_css_selector(\\'time.time-since\\').get_attribute(\\'datetime\\').split('T')[0]\",\n",
    "                               'date_format': '%Y-%m-%d',\n",
    "                               'sum_loc': \"article.find('div', attrs={'class':'article-content'}).find_all('p')\",\n",
    "                               'title_loc': \"article.find('h1', attrs={'class':'article__title'}).text\",\n",
    "                               'strain_bool': False,\n",
    "                               'journal_bool': False,\n",
    "                               'vehicle_specific': False,\n",
    "                               'rating': 3},\n",
    "                'Charged EVs': {'url': ['https://chargedevs.com/category/newswire/', 'https://chargedevs.com/category/newswire/page/2/'],\n",
    "                                'source': 'Charged EVs',\n",
    "                                'css_bool': False,\n",
    "                                'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                                'strain_tag': 'h3',\n",
    "                                'strain_attr_name': 'class',\n",
    "                                'strain_attr_value': 'h2',\n",
    "                                'url_list_query': '[item.a[\"href\"] for item in self.base_soup.find_all(\"h3\", class_=\"h2\")]',\n",
    "                                'date_loc': \"article.find('time').text\",\n",
    "                                'date_format': None,\n",
    "                                'sum_loc': \"article.find('section',class_='entry-content clearfix').find_all('p')\",\n",
    "                                'title_loc': \"article.find('h2', class_='page-title').text\",\n",
    "                                'strain_bool': True,\n",
    "                                'journal_bool': False,\n",
    "                                'vehicle_specific': True,\n",
    "                                'rating': 3},\n",
    "                'ARS Technica': {'url': 'https://arstechnica.com/cars/',\n",
    "                                 'source': 'ARS Technica',\n",
    "                                 'css_bool': False,\n",
    "                                 'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                                 'strain_tag': 'a',\n",
    "                                 'strain_attr_name': 'class',\n",
    "                                 'strain_attr_value': 'overlay',\n",
    "                                 'url_list_query': \"[item['href'] for item in self.base_soup.find_all('a', attrs={'class': 'overlay'})]\",\n",
    "                                 'date_loc': \"article.find('time', attrs={'class':'date'}).text\",\n",
    "                                 'date_format': None,\n",
    "                                 'sum_loc': \"article.find('div', attrs={'itemprop':'articleBody'}).find_all('p', attrs={'class':None})\",\n",
    "                                 'title_loc': \"article.h1.text\",\n",
    "                                 'strain_bool': True,\n",
    "                                 'journal_bool': False,\n",
    "                                 'vehicle_specific': True,\n",
    "                                 'rating': 3},\n",
    "                'Venture Beat': {'url': 'https://venturebeat.com/category/transportation/',\n",
    "                                 'source': 'Venture Beat',\n",
    "                                 'css_bool': False,\n",
    "                                 'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                                 'url_list_query': \"[item.a['href'] for item in self.base_soup.select('h2.article-title')]+[item.a['href'] for item in self.base_soup.select('article')]\",\n",
    "                                 'date_loc': \"article.find('meta', attrs={'property':'article:published_time'})['content']\",\n",
    "                                 'date_format': None,\n",
    "                                 'sum_loc': \"article.find('div', class_ = 'article-content').find_all('p')\",\n",
    "                                 'title_loc': \"article.find('h1').text\",\n",
    "                                 'strain_bool': False,\n",
    "                                 'journal_bool': False,\n",
    "                                 'vehicle_specific': True,\n",
    "                                 'rating': 3},\n",
    "                'IEEE Spectrum': {'url': 'https://spectrum.ieee.org/transportation', \n",
    "                                  'source': 'IEEE Spectrum',\n",
    "                                  'css_bool': False,\n",
    "                                  'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                                  'url_list_query': \"['https://spectrum.ieee.org'+item.a['href'] for item in self.base_soup.find_all('article')]\",\n",
    "                                  'strain_tag': 'article',\n",
    "                                  'strain_attr_name': 'class',\n",
    "                                  'strain_attr_value': 'item sml_article transportation',\n",
    "                                  'date_loc': \"article.label.text\",\n",
    "                                  'date_format': '%d %b %Y | %H:%M GMT',\n",
    "                                  'sum_loc': \"article.find_all('p')\",\n",
    "                                  'title_loc': \"article.h1.text\",\n",
    "                                  'strain_bool': True,\n",
    "                                  'journal_bool': False,\n",
    "                                  'vehicle_specific': True,\n",
    "                                  'rating': 1},\n",
    "                'Transport Topics': {'url': ['https://www.ttnews.com/government',\n",
    "                                             'https://www.ttnews.com/government?page=1',\n",
    "                                             'https://www.ttnews.com/government?page=2',\n",
    "                                             'https://www.ttnews.com/government?page=3',\n",
    "                                             'https://www.ttnews.com/business',\n",
    "                                             'https://www.ttnews.com/business?page=1',\n",
    "                                             'https://www.ttnews.com/business?page=2',\n",
    "                                             'https://www.ttnews.com/technology',\n",
    "                                             'https://www.ttnews.com/technology?page=1',\n",
    "                                             'https://www.ttnews.com/technology?page=2',\n",
    "                                             'https://www.ttnews.com/equipment',\n",
    "                                             'https://www.ttnews.com/equipment?page=1',\n",
    "                                             'https://www.ttnews.com/equipment?page=2'],\n",
    "                                     'source': 'Transport Topics',\n",
    "                                     'css_bool': False,\n",
    "                                     'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                                     'strain_tag': 'h2',\n",
    "                                     'strain_attr_name': 'class',\n",
    "                                     'strain_attr_value': 'content-access-1067',\n",
    "                                     'url_list_query': \"['https://www.ttnews.com'+item.a['href'] for item in self.base_soup.find_all('h2', class_='content-access-1067')]\",\n",
    "                                     'date_loc': \"article.find('span',class_='date-display-single')['content']\",\n",
    "                                     'date_format': None,\n",
    "                                     'sum_loc': \"[p for p in article.find_all('p') if p.text and len(p.text)>10]\",\n",
    "                                     'title_loc': \"article.find('h1').text\",\n",
    "                                     'strain_bool': True,\n",
    "                                     'journal_bool': False,\n",
    "                                     'vehicle_specific': True,\n",
    "                                     'rating': 1},\n",
    "                'GreenCarCongress': {'url': [f'http://www.greencarcongress.com/page/{page_no}/' for page_no in range(1,5)],\n",
    "                                     'source': 'GreenCarCongress',\n",
    "                                     'css_bool': False,\n",
    "                                     'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                                     'strain_tag': 'div',\n",
    "                                     'strain_attr_name': 'class',\n",
    "                                     'strain_attr_value': 'highlight-image-wrapper',\n",
    "                                     'url_list_query': \"[div.a['href'] for div in self.base_soup.find_all('div', 'highlight-image-wrapper')]\",\n",
    "                                     'date_loc': \"article.find('span', 'entry-date').a.text\",\n",
    "                                     'date_format': '%d %B %Y',\n",
    "                                     'sum_loc': \"article.find('div', 'entry-body font-entrybody').find_all('p')\",\n",
    "                                     'title_loc': \"article.h2.a.text\",\n",
    "                                     'strain_bool': True,\n",
    "                                     'journal_bool': False,\n",
    "                                     'vehicle_specific': True,\n",
    "                                     'rating': 1},\n",
    "                'Green Car Reports': {'url': [f'https://www2.greencarreports.com/news/page-{page_no}' for page_no in range(1,7)],\n",
    "                                     'source': 'Green Car Reports',\n",
    "                                     'css_bool': False,\n",
    "                                     'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                                     'strain_tag': 'a',\n",
    "                                     'strain_attr_name': 'class',\n",
    "                                     'strain_attr_value': 'title',\n",
    "                                     'url_list_query': \"['https://www2.greencarreports.com' + a['href'] for a in self.base_soup.find_all('a', 'title')]\",\n",
    "                                     'date_loc': \"article.find('time').text.strip()\",\n",
    "                                     'date_format': '%B %d, %Y',\n",
    "                                     'sum_loc': \"article.find('section', 'article-body').find_all('p')\",\n",
    "                                     'title_loc': \"article.h1.text\",\n",
    "                                     'strain_bool': True,\n",
    "                                     'journal_bool': False,\n",
    "                                     'vehicle_specific': True,\n",
    "                                     'rating': 3},\n",
    "                'The Fuse': {'url': 'http://energyfuse.org/category/autonomous-vehicles/',\n",
    "                             'source': 'The Fuse',\n",
    "                             'css_bool': False,\n",
    "                             'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                             'strain_tag': 'a',\n",
    "                             'strain_attr_name': 'class',\n",
    "                             'strain_attr_value': 'full-block-link',\n",
    "                             'url_list_query': \"[a['href'] for a in self.base_soup.find_all('a', class_='full-block-link')]\",\n",
    "                             'date_loc': \"article.h2.text.split('| ')[-1]\",\n",
    "                             'date_format': None,\n",
    "                             'sum_loc': \"article.find('div', class_='content-wrapper').find('p')\",\n",
    "                             'title_loc': \"article.h1.text\",\n",
    "                             'strain_bool': True,\n",
    "                             'journal_bool': False,\n",
    "                             'vehicle_specific': True,\n",
    "                             'rating': 3},\n",
    "                'Business Wire': {'url': 'https://www.businesswire.com/portal/site/home/news/',\n",
    "                                  'source': 'Business Wire',\n",
    "                                  'css_bool': False,\n",
    "                                  'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                                  'strain_tag': 'a',\n",
    "                                  'strain_attr_name': 'class',\n",
    "                                  'strain_attr_value': 'bwTitleLink',\n",
    "                                  'url_list_query': \"['https://www.businesswire.com' + a['href'] for a in self.base_soup.find_all('a', class_='bwTitleLink') if '/en/' in a['href']]\", #just get articles that are in English\n",
    "                                  'date_loc': \"' '.join(article.find('time').text.split()[:3])\",\n",
    "                                  'date_format': None,\n",
    "                                  'sum_loc': \"article.find('div', class_='bw-release-story').find_all('p')\",\n",
    "                                  'title_loc': \"' '.join(article.h1.text.strip().split())\",\n",
    "                                  'strain_bool': True,\n",
    "                                  'journal_bool': False,\n",
    "                                  'vehicle_specific': False,\n",
    "                                  'rating': 3},\n",
    "                'U.S. Department of Energy': {'url': 'https://www.energy.gov/listings/energy-news',\n",
    "                                              'source': 'U.S. Department of Energy',\n",
    "                                              'css_bool': False,\n",
    "                                              'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                                              'strain_tag': 'a',\n",
    "                                              'strain_attr_name': 'class',\n",
    "                                              'strain_attr_value': 'title-link',\n",
    "                                              'url_list_query': \"['https://www.energy.gov' + a['href'] for a in self.base_soup.find_all('a', class_='title-link')]\",\n",
    "                                              'date_loc': \"article.find('div', class_='node-hero-date').text\",\n",
    "                                              'date_format': None,\n",
    "                                              'sum_loc': \"article.find('div', class_='field-items').find_all('p')\",\n",
    "                                              'title_loc': \"article.h1.text\",\n",
    "                                              'strain_bool': True,\n",
    "                                              'journal_bool': False,\n",
    "                                              'vehicle_specific': False,\n",
    "                                              'rating': 2},\n",
    "                'Journal of Modern Transportation': {'url': 'https://link.springer.com/journal/40534/onlineFirst/page/1',\n",
    "                                                     'source': 'Journal of Modern Transportation',\n",
    "                                                     'css_bool': False,\n",
    "                                                     'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                                                     'strain_tag': 'div',\n",
    "                                                     'strain_attr_name': 'class',\n",
    "                                                     'strain_attr_value': 'toc-item',\n",
    "                                                     'url_list_query': \"['https://link.springer.com' + title.a['href'] for title in self.base_soup.find_all('div', class_='toc-item') if title.p.text == 'OriginalPaper']\",\n",
    "                                                     'date_loc': \"article.time.text\",\n",
    "                                                     'date_format': None,\n",
    "                                                     'sum_loc': \"article.find('section', class_='Abstract').p\",\n",
    "                                                     'title_loc': \"article.h1.text\",\n",
    "                                                     'strain_bool': True,\n",
    "                                                     'journal_bool': True,\n",
    "                                                     'vehicle_specific': True,\n",
    "                                                     'rating': 0},\n",
    "                'Jalopnik': {'url':  'https://jalopnik.com/c/news',\n",
    "                             'source': 'Jalopnik',\n",
    "                             'css_bool': False,\n",
    "                             'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                             'strain_tag': 'article',\n",
    "                             'strain_attr_name': 'class',\n",
    "                             'strain_attr_value': 'postlist__item--compact',\n",
    "                             'url_list_query': \"[article.a['href'] for article in self.base_soup.find_all('article', class_='postlist__item--compact')]\",\n",
    "                             'date_loc': \"article.time['datetime'].split('T')[0]\",\n",
    "                             'date_format': None,\n",
    "                             # The below method of searching for the summary filters out some odd divs in the middle of articles.\n",
    "                             'sum_loc': \"[par for par in list(article.find('div', class_='post-content entry-content js_entry-content ').children) if str(par)[:3] =='<p>']\",\n",
    "                             'title_loc': \"article.h1.text\",\n",
    "                             'strain_bool': True,\n",
    "                             'journal_bool': False,\n",
    "                             'vehicle_specific': False,\n",
    "                             'rating': 3},\n",
    "                'Bloomberg': {'url':  ['https://www.bloomberg.com/search?query=self-driving&sort=time:desc',\n",
    "                                       'https://www.bloomberg.com/search?query=self-driving&sort=time:desc&page=2'],\n",
    "                              'source': 'Bloomberg',\n",
    "                              'css_bool': False,\n",
    "                              'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                              'strain_tag': 'h1',\n",
    "                              'strain_attr_name': 'class',\n",
    "                              'strain_attr_value': 'search-result-story__headline',\n",
    "                              'url_list_query': \"[article.a['href'] for article in self.base_soup.find_all('h1', class_ = 'search-result-story__headline')]\",\n",
    "                              'date_loc': \"article.time['datetime'].split('T')[0]\",\n",
    "                              'date_format': None,\n",
    "                              'sum_loc': \"article.find('div', class_='middle-column').find_all('p')\",\n",
    "                              'title_loc': \"article.h1.text\",\n",
    "                              'strain_bool': True,\n",
    "                              'journal_bool': False,\n",
    "                              'vehicle_specific': True,\n",
    "                              'rating': 2},\n",
    "                'Business Insider': {'url':  [f'https://www.businessinsider.com/sai?page={page_no}' for page_no in range(1,9)],\n",
    "                                     'source': 'Business Insider',\n",
    "                                     'css_bool': False,\n",
    "                                     'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                                     'strain_tag': 'a',\n",
    "                                     'strain_attr_name': 'class',\n",
    "                                     'strain_attr_value': 'title',\n",
    "                                     'url_list_query': \"[a['href'] for a in self.base_soup.find_all('a', class_='title')[1:]]\",\n",
    "                                     'date_loc': \"article.find('div', class_ = 'byline-timestamp')['data-timestamp'].split('T')[0]\",\n",
    "                                     'date_format': None,\n",
    "                                     'sum_loc': \"article.find('div', id='piano-inline').findNext('div').find_all('p')\",\n",
    "                                     'title_loc': \"article.h1.text\",\n",
    "                                     'strain_bool': True,\n",
    "                                     'journal_bool': False,\n",
    "                                     'vehicle_specific': False,\n",
    "                                     'rating': 3},\n",
    "                'CNET': {'url':  'https://www.cnet.com/roadshow/search/?query=self-driving',\n",
    "                         'source': 'CNET',\n",
    "                         'css_bool': False,\n",
    "                         'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                         'strain_tag': 'section',\n",
    "                         'strain_attr_name': 'class',\n",
    "                         'strain_attr_value': 'searchItem product',\n",
    "                         'url_list_query': \"['https://www.cnet.com' + article.a['href'] for article in self.base_soup.find_all('section', class_='searchItem product')]\",\n",
    "                         'date_loc': \"article.find('span', class_='formattedDate').text\",\n",
    "                         'date_format': None,\n",
    "                         'sum_loc': \"article.find('div', class_='col-7 article-main-body row').find_all('p')\",\n",
    "                         'title_loc': \"article.h1.text\",\n",
    "                         'strain_bool': True,\n",
    "                         'journal_bool': False,\n",
    "                         'vehicle_specific': True,\n",
    "                         'rating': 3},\n",
    "                'Electric VTOL News': {'url':  'http://evtol.news/news/',\n",
    "                                     'source': 'Electric VTOL News',\n",
    "                                     'css_bool': False,\n",
    "                                     'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                                     'strain_tag': 'h2',\n",
    "                                     'strain_attr_name': 'class',\n",
    "                                     'strain_attr_value': 'entry-title',\n",
    "                                     'url_list_query': \"[title.a['href'] for title in self.base_soup.find_all('h2', class_='entry-title')]\",\n",
    "                                     'date_loc': \"article.time.text\",\n",
    "                                     'date_format': None,\n",
    "                                     'sum_loc': \"article.find('div', class_='entry-content').find_all('p')\",\n",
    "                                     'title_loc': \"article.h1.text\",\n",
    "                                     'strain_bool': True,\n",
    "                                     'journal_bool': False,\n",
    "                                     'vehicle_specific': True,\n",
    "                                     'rating': 1},\n",
    "                 'Detroit News': {'url':  'https://www.detroitnews.com/autos/',\n",
    "                                  'source': 'Detroit News',\n",
    "                                  'css_bool': False,\n",
    "                                  'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                                  'url_list_query': \"['https://www.detroitnews.com' + a['href'] for div in self.base_soup.find_all('div', 'flm-bundle') for a in div.find_all('a') if '/story/' in a['href'] and 'detroitnews' not in a['href']]\",\n",
    "                                  'date_loc': \"article.find('span', 'asset-metabar-time-updated').text.split('ET ')[1]\",\n",
    "                                  'date_format': '%B %d, %Y',\n",
    "                                  'sum_loc': \"article.find('div', itemprop = 'mainEntity articleBody').find_all('p')\",\n",
    "                                  'title_loc': \"article.h1.text\",\n",
    "                                  'strain_bool': True,\n",
    "                                  'strain_tag': 'div',\n",
    "                                  'strain_attr_name': 'class',\n",
    "                                  'strain_attr_value': 'flm-bundle',\n",
    "                                  'journal_bool': False,\n",
    "                                  'vehicle_specific': True,\n",
    "                                  'rating': 3},\n",
    "                    'Transportation Research': {'url': ['https://www.journals.elsevier.com/transportation-research-part-a-policy-and-practice/recent-articles',\n",
    "                                                            'https://www.journals.elsevier.com/transportation-research-part-b-methodological/recent-articles',\n",
    "                                                            'https://www.journals.elsevier.com/transportation-research-part-c-emerging-technologies/recent-articles',\n",
    "                                                            'https://www.journals.elsevier.com/transportation-research-part-d-transport-and-environment/recent-articles',\n",
    "                                                            'https://www.journals.elsevier.com/transportation-research-part-e-logistics-and-transportation-review/recent-articles',\n",
    "                                                            'https://www.journals.elsevier.com/transportation-research-part-f-traffic-psychology-and-behaviour/recent-articles' \n",
    "                                                           ],\n",
    "                                                'source': 'Transportation Research',\n",
    "                                                'url_list_query': \"[div.a['href'] for div in self.base_soup.find_all('div', class_='pod-listing-header')]\",\n",
    "                                                'css_bool': True,\n",
    "                                                'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.1}, 'css_scrape_em':{'base_url': .1, 'sub_url':.1}},\n",
    "                                                'load_more_css': None,\n",
    "                                                'max_scrapes': 100,\n",
    "                                                'max_loads': None,\n",
    "                                                'date_css': \"driver.find_element_by_css_selector(\\'button.show-hide-details\\')\",\n",
    "                                                'date_loc': \"article.find('div', 'wrapper').p.text.split('online ')[1][:-1]\",\n",
    "                                                'date_format': '%d %B %Y',\n",
    "                                                'sum_css': None,\n",
    "                                                'sum_loc': \"article.find('div', class_='abstract author').find_all('p')\",\n",
    "                                                'title_loc': \"article.h1.text\",\n",
    "                                                'title_css': None,\n",
    "                                                'strain_bool': False,\n",
    "                                                'journal_bool': True,\n",
    "                                                'vehicle_specific': True,\n",
    "                                                'rating': 0},\n",
    "                    'Journal of Urban Economics': {'url': 'https://www.journals.elsevier.com/journal-of-urban-economics/recent-articles',\n",
    "                                                   'source': 'Journal of Urban Economics',\n",
    "                                                   'url_list_query': \"[div.a['href'] for div in self.base_soup.find_all('div', class_='pod-listing-header')]\",\n",
    "                                                   'css_bool': True,\n",
    "                                                   'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.1}, 'css_scrape_em':{'base_url': .1, 'sub_url':.1}},\n",
    "                                                   'load_more_css': None,\n",
    "                                                   'max_scrapes': 100,\n",
    "                                                   'max_loads': None,\n",
    "                                                   'date_css': \"driver.find_element_by_css_selector(\\'button.show-hide-details\\')\",\n",
    "                                                   'date_loc': \"article.find('div', 'wrapper').p.text.split('online ')[1][:-1]\",\n",
    "                                                   'date_format': '%d %B %Y',\n",
    "                                                   'sum_css': None,\n",
    "                                                   'sum_loc': \"article.find('div', class_='Abstracts').find_all('p')\",\n",
    "                                                   'title_loc': \"article.h1.text\",\n",
    "                                                   'title_css': None,\n",
    "                                                   'strain_bool': False,\n",
    "                                                   'journal_bool': True,\n",
    "                                                   'vehicle_specific': False,\n",
    "                                                   'rating': 0},\n",
    "                    'Transport Policy': {'url': 'https://www.journals.elsevier.com/transport-policy/recent-articles',\n",
    "                                         'source': 'Transport Policy',\n",
    "                                         'url_list_query': \"[div.a['href'] for div in self.base_soup.find_all('div', class_='pod-listing-header')]\",\n",
    "                                         'css_bool': True,\n",
    "                                         'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.1}, 'css_scrape_em':{'base_url': .1, 'sub_url':.1}},\n",
    "                                         'load_more_css': None,\n",
    "                                         'max_scrapes': 100,\n",
    "                                         'max_loads': None,\n",
    "                                         'date_css': \"driver.find_element_by_css_selector(\\'button.show-hide-details\\')\",\n",
    "                                         'date_loc': \"article.find('div', 'wrapper').p.text.split('online ')[1][:-1]\",\n",
    "                                         'date_format': '%d %B %Y',\n",
    "                                         'sum_css': None,\n",
    "                                         'sum_loc': \"article.find('div', class_='Abstracts').find_all('p')\",\n",
    "                                         'title_loc': \"article.h1.text\",\n",
    "                                         'title_css': None,\n",
    "                                         'strain_bool': False,\n",
    "                                         'journal_bool': True,\n",
    "                                         'vehicle_specific': True,\n",
    "                                         'rating': 0},\n",
    "                     'Science': {'url':  [f'https://www.sciencemag.org/news/latest-news?r3f_986=https%3A//www.google.ru/&page={page_no}' for page_no in range(7)],\n",
    "                                 'source': 'Science',\n",
    "                                 'css_bool': False,\n",
    "                                 'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                                 'strain_tag': 'div',\n",
    "                                 'strain_attr_name': 'class',\n",
    "                                 'strain_attr_value': 'primary page primary--listpage',\n",
    "                                 'url_list_query': \"['https://www.sciencemag.org' + header.a['href'] for header in self.base_soup.find('div', 'primary page primary--listpage').find_all('h2')]\",\n",
    "                                 'date_loc': \"article.time.text.split(' ,')[0]\",\n",
    "                                 'date_format': None,\n",
    "                                 'sum_loc': \"article.find('div', 'article__body').find_all('p')[1:]\",\n",
    "                                 'title_loc': \"article.h1.text\",\n",
    "                                 'strain_bool': True,\n",
    "                                 'journal_bool': False,\n",
    "                                 'vehicle_specific': False,\n",
    "                                 'rating': 3},\n",
    "                'Biomass Magazine': {'url':  [f'http://biomassmagazine.com/browse/30/{no*30}' for no in range(4)],\n",
    "                                     'source': 'Biomass Magazine',\n",
    "                                     'css_bool': False,\n",
    "                                     'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                                     'url_list_query': \"['http://biomassmagazine.com' + header.a['href'] for header in self.base_soup.find_all('h2')]\",\n",
    "                                     'date_loc': \"article.find('div', 'author').text.split('| ')[1].split('\\n')[0]\",\n",
    "                                     'date_format': '%B %d, %Y',\n",
    "                                     'sum_loc': \"article.find('div', 'body').find_all('p')\",\n",
    "                                     'title_loc': \"article.h1.text\",\n",
    "                                     'strain_bool': False,\n",
    "                                     'journal_bool': False,\n",
    "                                     'vehicle_specific': False,\n",
    "                                     'rating': 3},\n",
    "                 'Alternative Energy News': {'url': [f\"http://www.alternative-energy-news.info/technology/{topic}/page/{page_no}\" for topic in ['transportation', 'inventions'] for page_no in range(1,3)],\n",
    "                                             'source': 'Alternative Energy News',\n",
    "                                             'strain_tag': 'div',\n",
    "                                             'strain_attr_name': 'class',\n",
    "                                             'strain_attr_value': 'index_post',\n",
    "                                             'url_list_query': \"[div.a['href'] for div in self.base_soup.find_all('div', class_ = 'index_post')]\",\n",
    "                                             'css_bool': False,\n",
    "                                             'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                                             'date_loc': \"article.find('div', 'post_content').div.text\",\n",
    "                                             'date_format':'%b %d',\n",
    "                                             'sum_loc': \"article.find('div', 'post_content_font').find_all('p')\",\n",
    "                                             'title_loc': \"article.h2.text\",\n",
    "                                             'strain_bool': True,\n",
    "                                             'journal_bool': False,\n",
    "                                             'vehicle_specific': False,\n",
    "                                             'rating': 3},\n",
    "                 'New Atlas': {'url': [f\"https://newatlas.com/transport/{page_no}/\" for page_no in range(1, 14)], # New Atlas is the new GizMag\n",
    "                               'source': 'New Atlas',\n",
    "                               'url_list_query': \"[header.a['href'] for header in self.base_soup.find_all('h2')]\",\n",
    "                               'css_bool': False,\n",
    "                               'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                               'date_loc': \"article.find('div','article-detail__byline').find_all('span')[-1].text\",\n",
    "                               'date_format':None,\n",
    "                               'sum_loc': \"article.find('div', 'ArticleBody article-detail__body').find_all('p')\",\n",
    "                               'title_loc': \"article.h1.text\",\n",
    "                               'strain_bool': False,\n",
    "                               'journal_bool': False,\n",
    "                               'vehicle_specific': True,\n",
    "                               'rating': 3},\n",
    "                 'Automotive News': {'url': 'https://www.autonews.com/news',\n",
    "                                     'source': 'Automotive News',\n",
    "                                     'url_list_query': \"['https://www.autonews.com' + div.a['href'] for div in self.base_soup.find_all('div', 'feature-article-headline')]\",\n",
    "                                     'css_bool': True,\n",
    "                                     'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.1}, 'css_scrape_em':{'base_url': .1, 'sub_url':.1}},\n",
    "                                     'load_more_css': \"a.button.omnitrack\",\n",
    "                                     'max_scrapes': 100,\n",
    "                                     'max_loads': 70, # Make sure that loads is not too many -- otherwise load button may become inactive.  Test this.\n",
    "                                     'date_css': None,\n",
    "                                     'date_loc': \"' '.join(article.find('span', itemprop = 'datePublished').text.split()[:3])\",\n",
    "                                     'date_format': '%B %d, %Y',\n",
    "                                     'sum_css': None,\n",
    "                                     'sum_loc': \"article.find('div', itemprop = 'articleBody').find_all('p')\",\n",
    "                                     'title_loc': \"article.h1.text\",\n",
    "                                     'title_css': None,\n",
    "                                     'strain_bool': False,\n",
    "                                     'journal_bool': False,\n",
    "                                     'vehicle_specific': True,\n",
    "                                     'rating': 1},\n",
    "                 'AZoM': {'url': [f\"https://www.azom.com/materials-news-index.aspx?page={page_no}\" for page_no in range(25)], \n",
    "                          'source': 'AZoM',\n",
    "                          'strain_tag': 'div',\n",
    "                          'strain_attr_name': 'class',\n",
    "                          'strain_attr_value': 'col-xs-9',\n",
    "                          'url_list_query': \"['https://azom.com' + div.a['href'] for div in self.base_soup.find_all('div', 'col-xs-9')]\",\n",
    "                          'css_bool': False,\n",
    "                          'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                          'date_loc': \"article.find('span', 'article-meta-date').text\",\n",
    "                          'date_format':None,\n",
    "                          'sum_loc': \"[p for p in article.find('div', 'item-body content-item-body clearfix').find_all('p')[1:] if p.text!='']\",\n",
    "                          'title_loc': \"article.h1.text\",\n",
    "                          'strain_bool': True,\n",
    "                          'journal_bool': False,\n",
    "                          'vehicle_specific': False,\n",
    "                          'rating': 3},\n",
    "                 'CompositesWorld': {'url': [f\"https://www.compositesworld.com/news/list/{page_no}/\" for page_no in range(1, 10)], \n",
    "                                     'source': 'CompositesWorld',\n",
    "                                     'url_list_query': \"['https://www.compositesworld.com' + p.a['href'] for p in self.base_soup.find_all('p', 'headline')]\",\n",
    "                                     'css_bool': False,\n",
    "                                     'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                                     'strain_tag': 'p',\n",
    "                                     'strain_attr_name': 'class',\n",
    "                                     'strain_attr_value': 'headline',\n",
    "                                     'date_loc': \"article.find('span', property = 'dc:created').text.split(': ')[1]\",\n",
    "                                     'date_format':None,\n",
    "                                     'sum_loc': \"article.find('div', id = 'short').find_all('p')\",\n",
    "                                     'title_loc': \"article.h1.text\",\n",
    "                                     'strain_bool': True,\n",
    "                                     'journal_bool': False,\n",
    "                                     'vehicle_specific': False,\n",
    "                                     'rating': 3},\n",
    "                'Lightweighting World': {'url':['http://lightweightingworld.com/category/in-the-news/',\n",
    "                                        'http://lightweightingworld.com/category/material-matter/',\n",
    "                                        'http://lightweightingworld.com/category/people-and-processes/',\n",
    "                                        'http://lightweightingworld.com/category/design-tooling/',\n",
    "                                        'http://lightweightingworld.com/category/supply-chains-logistics/'],\n",
    "                                     'source': 'Lightweighting World',\n",
    "                                     'css_bool': False,\n",
    "                                     'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                                     'strain_tag': 'h2',\n",
    "                                     'strain_attr_name': 'class',\n",
    "                                     'strain_attr_value': 'post-title',\n",
    "                                     'url_list_query': \"[header.a['href'] for header in self.base_soup.find_all('h2', 'post-title')]\",\n",
    "                                     'date_format': None,\n",
    "                                     'date_loc': \"article.find('span', 'meta-date').find('span', 'meta-inner').text.strip()\",\n",
    "                                     'sum_loc': [\"article.find('div', attrs={'class':'post-body'}).find_all('p')\",\"soup.find('div', class_='post-body').text\"],\n",
    "                                     'title_loc': \"article.h1.text\",\n",
    "                                     'strain_bool': True,\n",
    "                                     'journal_bool': False,\n",
    "                                     'vehicle_specific': True,\n",
    "                                     'rating': 3},\n",
    "                'SAE International': {'url': 'https://www.sae.org/news/',\n",
    "                                     'source': 'SAE International',\n",
    "                                     'url_list_query': \"[a['href'] for a in self.base_soup.find_all('a', ' nx-card-view-link') if 'sae' in a['href']]\",\n",
    "                                     'css_bool': True,\n",
    "                                     'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.1}, 'css_scrape_em':{'base_url': .1, 'sub_url':.1}},\n",
    "                                     'load_more_css': \"driver.find_element_by_css_selector(\\'button.nx-button.nx-button-more\\')\",\n",
    "                                     'max_scrapes': 100,\n",
    "                                     'max_loads': 25, # Make sure that loads is not too many -- otherwise load button may become inactive.  Test this.\n",
    "                                     'date_css': None,\n",
    "                                     'date_loc': \"article.find('span', 'nx-article-date').text.strip()\",\n",
    "                                     'date_format': '%Y-%m-%d',\n",
    "                                     'sum_css': None,\n",
    "                                     'sum_loc': \"article.find('article').find_all('p')\",\n",
    "                                     'title_loc': \"article.h2.text\",\n",
    "                                     'title_css': None,\n",
    "                                     'strain_bool': False,\n",
    "                                     'journal_bool': False,\n",
    "                                     'vehicle_specific': True,\n",
    "                                     'rating': 2},\n",
    "                 'Chemical & Engineering News': {'url': 'https://cen.acs.org/topics/materials.html',\n",
    "                                                 'source': 'Chemical & Engineering News',\n",
    "                                                 'url_list_query': \"['https://cen.acs.org' + header.a['href'] for header in self.base_soup.find_all('h2', 'topic-content-title') if header.a['href']!='' and 'postrelease' not in header.a['href']]\",\n",
    "                                                 'css_bool': True,\n",
    "                                                 'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.1}, 'css_scrape_em':{'base_url': .1, 'sub_url':.1}},\n",
    "                                                 'load_more_css': \"driver.find_element_by_css_selector(\\'button.load-more\\')\",\n",
    "                                                 'max_scrapes': 30,\n",
    "                                                 'max_loads': 10, # Make sure that loads is not too many -- otherwise load button may become inactive.  Test this.\n",
    "                                                 'date_css': None,\n",
    "                                                 'date_loc': \"article.find('div', 'article-intro-volume-number').text.split('|')[0].strip()\",\n",
    "                                                 'date_format': '%B %d, %Y',\n",
    "                                                 'sum_css': None,\n",
    "                                                 'sum_loc': \"[p for div in article.find_all('div', 'text-left article-content') for p in div.find_all('p')][1:]\",\n",
    "                                                 'title_loc': \"article.h1.text\",\n",
    "                                                 'title_css': None,\n",
    "                                                 'strain_bool': False,\n",
    "                                                 'journal_bool': False,\n",
    "                                                 'vehicle_specific': False,\n",
    "                                                 'rating': 2},\n",
    "                'Popular Science': {'url':  'https://www.popsci.com/cars',\n",
    "                                    'source': 'Popular Science',\n",
    "                                    'css_bool': True, #Using selenium because page has hidden elements that can't be accessed otherwise\n",
    "                                    'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.1}, 'css_scrape_em':{'base_url': .1, 'sub_url':.1}},\n",
    "                                    'max_loads':None,\n",
    "                                    'max_scrapes':200,\n",
    "                                    'load_more_css':None,\n",
    "                                    'date_css':None,\n",
    "                                    'title_css':None,\n",
    "                                    'sum_css':None,\n",
    "                                    'url_list_query': \"['https://www.popsci.com' + header.a['href'] for header in self.base_soup.find_all('h3')]\",\n",
    "                                    'date_loc': \"article.find('span', 'date timestamp-processed').text\",\n",
    "                                    'date_format': '%B %d, %Y',\n",
    "                                    'sum_loc': \"article.find('div', 'content-main basic-sidebar').find_all('p')\",\n",
    "                                    'title_loc': \"article.h1.text\",\n",
    "                                    'strain_bool': False,\n",
    "                                    'journal_bool': False,\n",
    "                                    'vehicle_specific': True,\n",
    "                                    'rating': 3},\n",
    "                 'National Center for Manufacturing Sciences': {'url': 'https://www.ncms.org/news/',\n",
    "                                                                'source': 'National Center for Manufacturing Sciences',\n",
    "                                                                'url_list_query': \"[header.a['href'] for header in self.base_soup.find_all('h4')]\",\n",
    "                                                                'css_bool': False,\n",
    "                                                                'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                                                                'date_loc': \"article.find('a', 'date updated').text\",\n",
    "                                                                'date_format':'%d %B %Y',\n",
    "                                                                'sum_loc': \"article.find('div', 'entry-content blog_postcontent').find_all('p')\",\n",
    "                                                                'title_loc': \"article.h3.text.strip()\",\n",
    "                                                                'strain_bool': False,\n",
    "                                                                'journal_bool': False,\n",
    "                                                                'vehicle_specific': False,\n",
    "                                                                'rating': 1},\n",
    "                'Composites Manufacturing': {'url': ['http://compositesmanufacturingmagazine.com/category/columns/tech-talk/'] + [f'http://compositesmanufacturingmagazine.com/category/automotive/page/{page_no}' for page_no in range(1,9)],\n",
    "                                             'source': 'Composites Manufacturing',\n",
    "                                             'url_list_query': \"[header.a['href'] for header in self.base_soup.find_all('h2', 'h4')]\",\n",
    "                                             'css_bool': False,\n",
    "                                             'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                                             'strain_tag': 'h2',\n",
    "                                             'strain_attr_name': 'class',\n",
    "                                             'strain_attr_value': 'h4',\n",
    "                                             'date_loc': \"article.find('time', 'updated').text\",\n",
    "                                             'date_format':None,\n",
    "                                             'sum_loc': \"article.find('div', itemprop='articleBody').find_all('p')\",\n",
    "                                             'title_loc':\"article.h1.text\",\n",
    "                                             'strain_bool': True,\n",
    "                                             'journal_bool': False,\n",
    "                                             'vehicle_specific': False,\n",
    "                                             'rating': 2},\n",
    "             'Nanowerk': {'url':  [f'https://www.nanowerk.com/category-nanoresearch.php?page={page_no}' for page_no in range(1,12)],\n",
    "                          'source': 'Nanowerk',\n",
    "                          'css_bool': False,\n",
    "                          'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                          'url_list_query': \"[header.a['href'] for header in self.base_soup.find_all('h2')]\",\n",
    "                          'date_loc': \"article.find('td', itemprop = 'datePublished').text\",\n",
    "                          'date_format': '%b %d, %Y',\n",
    "                          'sum_loc': \"' '.join([td.text for td in article.find_all('td')][1:])\",\n",
    "                          'title_loc': \"article.h1.text\",\n",
    "                          'strain_bool': False,\n",
    "                          'journal_bool': False,\n",
    "                          'vehicle_specific': False,\n",
    "                          'rating': 1},\n",
    "                'Engineering': {'url': \"https://www.engineering.com/Industries/IndustryArticle.aspx?industry=1\",\n",
    "                                'source': 'Engineering',\n",
    "                                'url_list_query': \"[a['href'] for a in self.base_soup.find_all('a', 'articleLinkTitle')]\",\n",
    "                                'css_bool': True,\n",
    "                                'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.1}, 'css_scrape_em':{'base_url': .1, 'sub_url':.1}},\n",
    "                                'load_more_css': \"\"\"driver.find_element_by_xpath(\\\"//a[contains(text(), \\'Next\\')]\\\")\"\"\",\n",
    "                                'max_scrapes': 100,\n",
    "                                'max_loads': 3, # Make sure that loads is not too many -- otherwise load button may become inactive.  Test this.\n",
    "                                'date_css': None,\n",
    "                                'date_loc': \"article.find('div', 'article').text.split('posted on ')[1].split(' |')[0]\",\n",
    "                                'date_format': '%B %d, %Y',\n",
    "                                'sum_css': None,\n",
    "                                'sum_loc': \"article.find('div', id = 'articleBodyArea').text\",\n",
    "                                'title_loc': \"article.find('span', 'article_title').text\",\n",
    "                                'title_css': None,\n",
    "                                'strain_bool': False,\n",
    "                                'journal_bool': False,\n",
    "                                'vehicle_specific': True,\n",
    "                                'rating': 2},\n",
    "           'Materials Science & Engineering': {'url': 'https://www.scientific.net/Search/Search?age=0&SortBy=1&IncludePapers=true&searchString=vehicle',\n",
    "                                               'source': 'Materials Science & Engineering',\n",
    "                                               'css_bool': False,\n",
    "                                               'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                                               'strain_tag': 'div',\n",
    "                                               'strain_attr_name': 'class', \n",
    "                                               'strain_attr_value': 'main-volume-item-block',\n",
    "                                               'url_list_query': \"['https://www.scientific.net' + div.a['href'] for div in self.base_soup.find_all('div', 'main-volume-item-block')]\",\n",
    "                                               'date_loc': \"[div for div in article.find_all('div', 'papers-block-info col-lg-12') if 'Online since' in div.text][0].find_all('div', 'row')[2].text.strip()\",\n",
    "                                               'date_format': '%B %Y',\n",
    "                                               'sum_loc': \"article.find('div', 'abstract-block-description').find_all('p')\",\n",
    "                                               'title_loc': \"article.h1.text\",\n",
    "                                               'strain_bool': True,\n",
    "                                               'journal_bool': True,\n",
    "                                               'vehicle_specific': True,\n",
    "                                               'rating': 0},\n",
    "                 'Daimler': {'url': 'https://daimler-trucksnorthamerica.com/influence/press-room/press-releases/',\n",
    "                             'source': 'Daimler',\n",
    "                             'url_list_query': \"['https://daimler-trucksnorthamerica.com/influence/press-room/press-releases/' + footer.a['href'] for footer in self.base_soup.find_all('footer') if 'PressDetail' in footer.a['href']]\",\n",
    "                             'css_bool': True,\n",
    "                             'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.1}, 'css_scrape_em':{'base_url': .1, 'sub_url':.1}},\n",
    "                             'load_more_css':None,\n",
    "                             'date_css': None,\n",
    "                             'sum_css': None,\n",
    "                             'title_css': None,\n",
    "                             'date_loc': \"article.time.text\",\n",
    "                             'date_format':'%b %d, %Y',\n",
    "                             'sum_loc': \"[p for p in article.find('inner-content').find_all('p') if '•' not in p.text]\",\n",
    "                             'title_loc': \"article.h1.text\",\n",
    "                             'strain_bool': False,\n",
    "                             'journal_bool': False,\n",
    "                             'vehicle_specific': True,\n",
    "                             'rating': 1},\n",
    "                 'Kenworth': {'url': 'https://kenworth.com/news/',\n",
    "                              'source': 'Kenworth',\n",
    "                              'css_bool': False,\n",
    "                              'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                              'strain_tag': 'div',\n",
    "                              'strain_attr_name': 'class',\n",
    "                              'strain_attr_value': 'column',\n",
    "                              'url_list_query': \"['https://kenworth.com' + a['href'] for a in self.base_soup.find_all('div', 'column')[-1].find_all('a')]\",\n",
    "                              'date_loc': \"', '.join(article.find('span', 'locationDate').text.split(', ')[-2:])\",\n",
    "                              'date_format':'%B %d, %Y',\n",
    "                              'sum_loc': \"article.find_all('div', 'column')[-1].find_all('p')\",\n",
    "                              'title_loc': \"article.h2.text\",\n",
    "                              'strain_bool': True,\n",
    "                              'journal_bool': False,\n",
    "                              'vehicle_specific': True,\n",
    "                              'rating': 1},\n",
    "                 'Peterbilt': {'url': 'https://www.peterbilt.com/about/news-events',\n",
    "                               'source': 'Peterbilt',\n",
    "                               'css_bool': False,\n",
    "                               'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                               'url_list_query': \"[a['href'] for a in self.base_soup.find('article').find_all('a', 'read-more-link')]\",\n",
    "                               'date_loc': \"article.time.text\",\n",
    "                               'date_format':'%B %d, %Y',\n",
    "                               'sum_loc': \"article.article.p.text\",\n",
    "                               'title_loc': \"' '.join([word.capitalize() for word in [w for w in article.h1.text.strip().lower().split() if w!='']])\", # Title was in full capitals--this reformats the title so \n",
    "                                                                                                                                                       # only the first letter of each word is capitalized.\n",
    "                               'strain_bool': False,\n",
    "                               'journal_bool': False,\n",
    "                               'vehicle_specific': True,\n",
    "                               'rating': 1},\n",
    "                 'Volvo': {'url': 'https://www.volvotrucks.us/news-and-stories/press-releases/',\n",
    "                           'source': 'Volvo',\n",
    "                           'css_bool': False,\n",
    "                           'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                           'strain_tag': 'h3',\n",
    "                           'strain_attr_name': 'class',\n",
    "                           'strain_attr_value': 'news-item-title',\n",
    "                           'url_list_query': \"['https://www.volvotrucks.us' + header.a['href'] for header in self.base_soup.find_all('h3', 'news-item-title')]\",\n",
    "                           'date_loc': \"article.find('div', 'postFullDate').text\",\n",
    "                           'date_format':'%m/%d/%Y',\n",
    "                           'sum_loc': \"article.find('div', 'postSummary').find_all('p')\",\n",
    "                           'title_loc': \"article.h1.text\", \n",
    "                           'strain_bool': True,\n",
    "                           'journal_bool': False,\n",
    "                           'vehicle_specific': True,\n",
    "                           'rating': 1},\n",
    "                 'Cummins': {'url': 'https://www.cummins.com/news',\n",
    "                             'source': 'Cummins',\n",
    "                             'css_bool': False,\n",
    "                             'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                             'url_list_query': \"['https://www.cummins.com' + header.a['href'] for header in self.base_soup.find_all('h3') if header.a!=None]\",\n",
    "                             'date_loc': \"article.time.text\",\n",
    "                             'date_format':'%b %d, %Y',\n",
    "                             'sum_loc': \"article.find('div', 'node__content clearfix').find_all(['h4','p'])\",\n",
    "                             'title_loc': \"article.h1.text\", \n",
    "                             'strain_bool': False,\n",
    "                             'journal_bool':False,\n",
    "                             'vehicle_specific': True,\n",
    "                             'rating': 1},\n",
    "                  'Eaton': {'url': 'https://www.eaton.com/us/en-us/company/news-insights/news-releases.html',\n",
    "                            'source': 'Eaton',\n",
    "                            'css_bool': False,\n",
    "                            'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                            'url_list_query': \"[header.a['href'] for header in self.base_soup.find_all('h4') if header.a!=None]\",\n",
    "                            'date_loc': \"article.find('div', id='news-content').find_all('div')[1].text.split(': ')[1].strip()\",\n",
    "                            'date_format':'%B %d, %Y',\n",
    "                            'sum_loc': \"article.find('div', id = 'news-content').find_all('p')\",\n",
    "                            'title_loc': \"article.find('div', id = 'news-content').find('div').text\", \n",
    "                            'strain_bool': False,\n",
    "                            'journal_bool': False,\n",
    "                            'vehicle_specific': True,\n",
    "                            'rating': 1},\n",
    "                 'Allison Transmission': {'url': 'https://ir.allisontransmission.com/news-releases',\n",
    "                                          'source': 'Allison Transmission',\n",
    "                                          'css_bool': False,\n",
    "                                          'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                                          'strain_tag': 'td',\n",
    "                                          'strain_attr_name': 'class',\n",
    "                                          'strain_attr_value': 'cus-title',\n",
    "                                          'url_list_query': \"['https://ir.allisontransmission.com/' + td.a['href'] for td in self.base_soup.find_all('td', 'cus_title')]\",\n",
    "                                          'date_loc': \"article.find('div', 'node__content').p.text.split('--')[2]\",\n",
    "                                          'date_format':'%b. %d, %Y',\n",
    "                                          'sum_loc': \"article.find('div', 'node__content').find_all('p')\",\n",
    "                                          'title_loc': \"article.h5.text\", \n",
    "                                          'strain_bool': True,\n",
    "                                          'journal_bool': False,\n",
    "                                          'vehicle_specific': True,\n",
    "                                          'rating': 1},\n",
    "                 'Mack': {'url': 'https://www.macktrucks.com/community/mack-news/',\n",
    "                          'source': 'Mack',\n",
    "                          'css_bool': True,\n",
    "                          'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.1}, 'css_scrape_em':{'base_url': .1, 'sub_url':.1}},\n",
    "                          'load_more_css':None,\n",
    "                          'date_css': None,\n",
    "                          'sum_css': None,\n",
    "                          'title_css':None,\n",
    "                          'url_list_query': \"['https://www.macktrucks.com' + a['href'] for a in self.base_soup.find('ul', 'searchresults').find_all('a') if a['href'][0]=='/']\",\n",
    "                          'date_loc': \"article.find('span', 'date').text.strip()\",\n",
    "                          'date_format':'%m-%d-%Y',\n",
    "                          'sum_loc': \"article.article.find_all('p')\",\n",
    "                          'title_loc': \"article.h1.text\", \n",
    "                          'strain_bool': False,\n",
    "                          'journal_bool': False,\n",
    "                          'vehicle_specific': True,\n",
    "                          'rating': 1},\n",
    "                 'Ford': {'url': 'https://media.ford.com/content/fordmedia/fna/us/en/news.html',\n",
    "                          'source': 'Ford',\n",
    "                          'css_bool': False,\n",
    "                          'time_sleep': {'get_urls':{'get_page':.1, 'parse_page':.05}, 'scrape_em':.1},\n",
    "                          'url_list_query': \"['https://media.ford.com' + div.a['href'] for div in self.base_soup.find_all('div', 'sub-text')]\",\n",
    "                          'date_loc': \"article.find('div', 'title').text.split(' |')[0]\",\n",
    "                          'date_format':'%b %d, %Y',\n",
    "                          'sum_loc': \"article.find('div', 'par parsys').find_all('p')\",\n",
    "                          'title_loc': \"article.find('h1', 'page-header').text\", \n",
    "                          'strain_bool': True,\n",
    "                          'strain_tag': 'div',\n",
    "                          'strain_attr_name': 'class',\n",
    "                          'strain_attr_value': 'sub-text',\n",
    "                          'journal_bool': False,\n",
    "                          'vehicle_specific': True,\n",
    "                          'rating': 1},\n",
    "               }\n",
    "\n",
    "# Resets scraper dict to just include keys from sites_to_scrape if you set limit_sites = True\n",
    "if limit_sites:\n",
    "    #scraper_dict = {site: scraper_dict[site] for site in sites_to_scrape}\n",
    "    scraper_dict = {site: scraper_dict[site] for site in scraper_dict if not scraper_dict[site]['css_bool']} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href = \"#test\">Jump to individual site testing</a>\n",
    "# Run the scrapers\n",
    "Note: there will be a couple errors, especially with Autoblog. The scraper for that site still picks up a couple irrelevant items that it can't handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T13:36:05.340083Z",
     "start_time": "2019-01-07T13:30:37.595775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MIT\n",
      "13 MIT article(s) scraped\n",
      "0 MIT article(s) skipped due to error\n",
      "19 MIT article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "Semiconductor Engineering\n",
      "0 Semiconductor Engineering article(s) scraped\n",
      "0 Semiconductor Engineering article(s) skipped due to error\n",
      "20 Semiconductor Engineering article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "Quartz\n",
      "3 Quartz article(s) scraped\n",
      "0 Quartz article(s) skipped due to error\n",
      "7 Quartz article(s) skipped due to age\n",
      "1 relevant article(s) collected\n",
      "\n",
      "Recode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebarnard\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\dateutil\\parser\\_parser.py:1204: UnknownTimezoneWarning: tzname EDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  category=UnknownTimezoneWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 Recode article(s) scraped\n",
      "0 Recode article(s) skipped due to error\n",
      "24 Recode article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "GovTech\n",
      "'NoneType' object has no attribute 'find_all': https://www.govtech.com/fs/automation/Waymo-Announces-Plans-to-Build-Self-Driving-Cars-in-Detroit.html \n",
      "date:2019-04-24\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.govtech.com/fs/automation/Utah-Embarks-on-a-Mission-to-Familiarize-Residents-with-AVs.html \n",
      "date:2019-04-19\n",
      "title:None\n",
      "summary:None\n",
      "3 GovTech article(s) scraped\n",
      "2 GovTech article(s) skipped due to error\n",
      "41 GovTech article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "Reuters\n",
      "147 Reuters article(s) scraped\n",
      "0 Reuters article(s) skipped due to error\n",
      "477 Reuters article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "CityLab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebarnard\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\dateutil\\parser\\_parser.py:1204: UnknownTimezoneWarning: tzname ET identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  category=UnknownTimezoneWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 CityLab article(s) scraped\n",
      "0 CityLab article(s) skipped due to error\n",
      "33 CityLab article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "Autoblog\n",
      "('Unknown string format:', '2 days ago'): https://www.autoblog.com/photos/20-most-efficient-crossovers-and-suvs/ \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "126 Autoblog article(s) scraped\n",
      "1 Autoblog article(s) skipped due to error\n",
      "355 Autoblog article(s) skipped due to age\n",
      "3 relevant article(s) collected\n",
      "\n",
      "Electrek\n",
      "68 Electrek article(s) scraped\n",
      "0 Electrek article(s) skipped due to error\n",
      "185 Electrek article(s) skipped due to age\n",
      "4 relevant article(s) collected\n",
      "\n",
      "The Verge\n",
      "'NoneType' object has no attribute 'text': https://www.theverge.com/2019/3/14/18261968/tesla-model-y-announcement-news-compact-suv-elon-musk-updates-highlights \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'text': https://www.theverge.com/2019/3/21/18274868/boeing-737-max-airplane-crash-updates-highlights \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "0 The Verge article(s) scraped\n",
      "2 The Verge article(s) skipped due to error\n",
      "76 The Verge article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "Crunchbase\n",
      "('Unknown string format:', '12 hours ago'): https://news.crunchbase.com/news/science-made-accessible-labster-scores-21m-in-series-b/ \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "('Unknown string format:', '4 hours ago'): https://news.crunchbase.com/news/mary-meekers-new-growth-fund-in-context/ \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "('Unknown string format:', '6 hours ago'): https://news.crunchbase.com/news/why-nea-led-a-23m-round-for-mejuri-a-d2c-jewelry-startup/ \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "('Unknown string format:', '4 hours ago'): https://news.crunchbase.com/news/salesloft-lands-70m-as-intelligent-tooling-heats-up/ \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "('Unknown string format:', '6 hours ago'): https://news.crunchbase.com/news/slack-may-post-direct-listing-filing-this-friday/ \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "22 Crunchbase article(s) scraped\n",
      "5 Crunchbase article(s) skipped due to error\n",
      "106 Crunchbase article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "Truck News\n",
      "26 Truck News article(s) scraped\n",
      "0 Truck News article(s) skipped due to error\n",
      "124 Truck News article(s) skipped due to age\n",
      "8 relevant article(s) collected\n",
      "\n",
      "Trucks.com\n",
      "17 Trucks.com article(s) scraped\n",
      "0 Trucks.com article(s) skipped due to error\n",
      "0 Trucks.com article(s) skipped due to age\n",
      "13 relevant article(s) collected\n",
      "\n",
      "Charged EVs\n",
      "19 Charged EVs article(s) scraped\n",
      "0 Charged EVs article(s) skipped due to error\n",
      "0 Charged EVs article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "ARS Technica\n",
      "14 ARS Technica article(s) scraped\n",
      "0 ARS Technica article(s) skipped due to error\n",
      "16 ARS Technica article(s) skipped due to age\n",
      "3 relevant article(s) collected\n",
      "\n",
      "Venture Beat\n",
      "12 Venture Beat article(s) scraped\n",
      "0 Venture Beat article(s) skipped due to error\n",
      "28 Venture Beat article(s) skipped due to age\n",
      "3 relevant article(s) collected\n",
      "\n",
      "IEEE Spectrum\n",
      "1 IEEE Spectrum article(s) scraped\n",
      "0 IEEE Spectrum article(s) skipped due to error\n",
      "22 IEEE Spectrum article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "Transport Topics\n",
      "61 Transport Topics article(s) scraped\n",
      "0 Transport Topics article(s) skipped due to error\n",
      "33 Transport Topics article(s) skipped due to age\n",
      "13 relevant article(s) collected\n",
      "\n",
      "GreenCarCongress\n",
      "29 GreenCarCongress article(s) scraped\n",
      "0 GreenCarCongress article(s) skipped due to error\n",
      "87 GreenCarCongress article(s) skipped due to age\n",
      "6 relevant article(s) collected\n",
      "\n",
      "Green Car Reports\n",
      "34 Green Car Reports article(s) scraped\n",
      "0 Green Car Reports article(s) skipped due to error\n",
      "110 Green Car Reports article(s) skipped due to age\n",
      "3 relevant article(s) collected\n",
      "\n",
      "The Fuse\n",
      "1 The Fuse article(s) scraped\n",
      "0 The Fuse article(s) skipped due to error\n",
      "5 The Fuse article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "Business Wire\n",
      "19 Business Wire article(s) scraped\n",
      "0 Business Wire article(s) skipped due to error\n",
      "0 Business Wire article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "U.S. Department of Energy\n",
      "3 U.S. Department of Energy article(s) scraped\n",
      "0 U.S. Department of Energy article(s) skipped due to error\n",
      "22 U.S. Department of Energy article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "Journal of Modern Transportation\n",
      "0 Journal of Modern Transportation article(s) scraped\n",
      "0 Journal of Modern Transportation article(s) skipped due to error\n",
      "2 Journal of Modern Transportation article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "Jalopnik\n",
      "16 Jalopnik article(s) scraped\n",
      "0 Jalopnik article(s) skipped due to error\n",
      "4 Jalopnik article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "Bloomberg\n",
      "0 Bloomberg article(s) scraped\n",
      "0 Bloomberg article(s) skipped due to error\n",
      "0 Bloomberg article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "Business Insider\n",
      "'NoneType' object is not subscriptable: https://www.businessinsider.com/sc/disaster-relief-ibm-call-for-code \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "13 Business Insider article(s) scraped\n",
      "1 Business Insider article(s) skipped due to error\n",
      "1 Business Insider article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "CNET\n",
      "8 CNET article(s) scraped\n",
      "0 CNET article(s) skipped due to error\n",
      "2 CNET article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "Electric VTOL News\n",
      "10 Electric VTOL News article(s) scraped\n",
      "0 Electric VTOL News article(s) skipped due to error\n",
      "1 Electric VTOL News article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "Detroit News\n",
      "'NoneType' object has no attribute 'text': https://www.detroitnews.com/story/business/autos/2019/04/24/study-finds-cars-need-better-protection-backseat-passengers/3565689002/ \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'text': https://www.detroitnews.com/story/business/autos/foreign/2019/04/25/bmw-review-test/39391607/ \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "1 Detroit News article(s) scraped\n",
      "2 Detroit News article(s) skipped due to error\n",
      "0 Detroit News article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "Science\n",
      "32 Science article(s) scraped\n",
      "0 Science article(s) skipped due to error\n",
      "143 Science article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "Biomass Magazine\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16097/ameresco-declares-commercial-operations-of-historic-rng-plant \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16073/socalgas-wins-3-million-cec-grant-to-advance-new-rng-technology \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16058/roeslein-alternative-energy-acquires-aerg \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16077/epa-seeks-comments-on-sre-portion-of-proposed-regs-rule \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16096/california-ethanol-power-wins-10-million-tax-credit \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/15961/feedstock-sourcing-for-project-success-us-south-advantages \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16043/rea-report-shows-bioenergy-is-the-leader-in-uk-renewables \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16084/rng-on-road-fuel-use-reaches-historic-high \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16005/neste-my-renewable-diesel-commercial-fueling-site-opens-in-calif \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16046/epa-approves-a-small-refinery-exemption-for-compliance-year-2017 \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16001/doe-50m-available-for-commercial-truck-gaseous-fuels-research \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16000/reg-announces-plans-to-sell-life-sciences-division \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16049/usda-january-wood-pellet-exports-reach-386-779-tons \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16018/enplus-the-us-perspective \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16109/valmet-converts-recovery-boiler-to-biomass-at-da-alizay-in-france \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16045/gevo-plans-to-decarbonize-luverne-plant-using-wind-rng \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16029/canada-to-invest-2-7-million-in-zooshare-biogas-project \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/15999/w2c-rotterdam-project-welcomes-shell-as-partner \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16105/rng-project-proposed-for-prince-george-british-columbia \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16094/neste-air-bp-to-deliver-sustainable-aviation-fuel-to-sweden \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16099/usda-us-wood-pellet-exports-top-500-000-metric-tons-in-february \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16110/quantum-qbi-partner-on-california-biogas-project \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16060/finkenauer-introduces-house-bill-to-extend-biodiesel-tax-credit \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16022/eia-updates-bioenergy-forecasts-predicts-capacity-increase \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16050/sab-sends-letter-to-epa-regarding-review-of-biogenic-framework \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/15964/critical-mass \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16107/nevada-governor-signs-bill-that-increases-rps-to-50-by-2030 \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16025/epa-schedules-hearing-on-e15-rin-reform-rule-for-march-29 \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16044/next-renewable-fuels-selects-site-for-proposed-biofuel-plant \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16042/report-details-employment-in-biofuel-biomass-power-industries \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16007/5-track-technical-agenda-announced-for-intundefinedl-few-in-indianapolis \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/15970/warmer-water-after-political-tide-change \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16019/heart-of-the-process \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16065/minnesota-offers-funding-for-bioenergy-biochemical-pilot-plants \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16095/naig-shaw-say-pending-sres-would-hurt-farmers-damage-rfs \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16041/ukundefineds-largest-waste-wood-gasifier-becomes-operational \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16024/xebec-to-deliver-biogas-upgrading-plant-for-rng-project-in-italy \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16010/epa-grants-more-rfs-waivers-undercuts-addundefinedl-demand-for-biofuels \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16039/food-waste-anaerobic-digestion-project-gains-approval-in-new-york \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16083/eu-approves-state-aid-for-irish-heat-scheme-polish-wte-plant \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/15965/from-the-ground-up \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16016/convincing-plant-managers-to-spend-money \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16052/fortistar-acquires-2-rng-facilities-in-pennsylvania \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16021/uk-renewable-energy-production-reaches-record-high-in-2018 \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16032/doe-biomass-rd-advisory-committee-to-meet-march-27-28 \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16114/national-grid-rng-can-help-decarbonize-the-gas-network \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16112/smithfield-foods-rae-form-monarch-bioenergy-to-produce-rng \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16098/epa-40-sre-applications-now-pending-for-compliance-year-2018 \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16076/grassley-asks-for-clarity-on-doeundefineds-review-of-sre-applications \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16074/2017-ag-census-includes-data-on-on-farm-methane-digesters \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/15971/introducing-the-rfs-power-coalition \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16111/enviva-helps-protect-threatened-habitat-watershed-in-virginia \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16037/abc-urges-lawmakers-to-support-ad-related-bills-in-maryland \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16088/biomass-in-the-hostess-city \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16017/through-a-glass-darkly \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16003/natural-resource-canada-funds-study-for-potential-pellet-project \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16100/spero-renewables-announces-cooperative-agreement-with-doe \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16040/enviva-announces-drop-down-transactions-related-to-hamlet-plant \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16113/new-york-opens-solicitation-for-renewable-energy-projects \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16075/cltcc-welcomes-drax-biomass-as-forestry-program-partner \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16087/the-twin-biomass-sisters-of-franklin-and-madison-counties \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16038/era-funds-wheat-ethanol-plant-woody-biomass-project \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16004/boeing-to-offer-biofuel-for-airlines-to-fly-new-airplanes-home \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16002/rng-coalition-announces-scholarship-awards-for-2019 \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16028/lygos-wins-3-grants-to-advance-applications-for-its-biochemicals \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16063/biogas-groups-sponsor-2-teams-at-honda-indy-grant-prix-of-alabama \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16048/pinnacle-resumes-pellet-production-at-entwistle-plant \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16101/lawmakers-urge-robust-funding-for-farm-bill-energy-title-programs \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/15972/the-many-dynamics-of-feedstock-supply \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16080/mti-funds-project-by-biofine-to-produce-biobased-levulinic-acid \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16035/neste-to-open-site-in-california-offering-renewable-diesel \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16047/biofuel-groups-express-support-for-e15-argue-against-rin-reform \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16054/eni-cib-to-promote-advanced-biomethane-in-italy \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16026/granbio-acquires-100-equity-in-american-process-inc \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16061/doe-offers-59-million-for-advanced-vehicle-technologies-research \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16030/albioma-to-convert-caribbean-power-plant-from-coal-to-biomass \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16023/washington-house-passes-bill-to-establish-clean-fuels-program \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16034/epa-32-02-million-cellulosic-rins-generated-in-february \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16106/epa-32-61-million-cellulosic-rins-generated-in-march \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16071/eia-updates-bioenergy-forecasts-predicts-capacity-increase \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16069/gevo-praj-to-commercialize-renewable-fuels-in-india \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16055/clean-methane-systems-eip-ventures-partner-on-ecorng \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16059/sustainability-requirements-in-japan-could-increase-pellet-demand \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16057/doe-journal-issue-addresses-challenges-to-bioenergy-production \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16014/the-value-of-experience \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16102/neste-city-of-oakland-partner-to-fuel-city-fleet-with-biofuel \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16115/abfa-files-for-injunction-against-epa-to-prevent-additional-sres \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16027/report-maine-can-benefit-by-encouraging-switch-to-pellet-heat \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16056/wheeler-addresses-sres-rvo-reallocations-during-recent-hearings \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16064/brightmark-energy-launches-washington-dairy-biogas-project \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16079/usda-eligibility-criteria-released-for-tribal-biomass-program \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16009/eia-800-000-tons-of-densified-biomass-fuel-sold-in-november \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16072/dsm-biomass-chp-project-reduces-co2-emissions-at-vitamin-plant \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16068/epa-to-propose-rule-on-biomass-carbon-neutrality-this-summer \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16036/uk-port-handles-30-000-ton-shipment-of-wood-pellets \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16033/socalgasundefineds-george-minter-named-rng-champion-by-climate-resolve \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16011/biomass-conversion-moving-advanced-carbon-materials-to-market \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16006/us-epa-releases-e15-rin-market-reform-proposed-rule \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16093/enerkem-closes-a-new-round-of-financing-for-c76-3-million \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16108/aries-green-biochar-certified-by-usda-ibi \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16051/us-gain-brings-a-new-rng-project-online-sends-fuel-to-california \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16086/building-globally-impacting-locally \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16013/a-new-wave-of-drying-techniques \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16012/built-to-last \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16020/marine-biofuel-test-at-port-of-rotterdam-gets-underway-march-19 \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16103/comment-period-on-e15-rin-market-reform-rule-to-close-april-29 \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16078/increases-in-biomass-power-generation-stop-after-decade-of-growth \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16067/pacific-bioenergy-no-injuries-sustained-during-april-6-fire \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16066/andritz-to-supply-biomass-handling-system-for-brazil-power-plant \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16062/eia-790-000-tons-of-densified-biomass-sold-in-december \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16053/uk-bioenergy-capacity-generation-increased-in-2018 \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16082/alabama-governor-awards-auburn-university-grant-to-study-carinata \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16070/epa-to-award-110-000-to-anaerobic-digestion-projects \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16031/jbei-biobased-jet-fuels-could-be-cost-competitive \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16008/dutch-biomass-consumption-to-reach-2-3-million-tons-by-2020 \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16104/southern-power-to-sell-115-mw-biomass-plant-to-austin-energy \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16116/letter-opposes-proposed-cuts-to-doeundefineds-renewable-energy-programs \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "EOL while scanning string literal (<string>, line 1): http://biomassmagazine.com/articles/16085/opportunities-for-biomass-utilization-in-the-last-frontier \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "0 Biomass Magazine article(s) scraped\n",
      "118 Biomass Magazine article(s) skipped due to error\n",
      "0 Biomass Magazine article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "Alternative Energy News\n",
      "0 Alternative Energy News article(s) scraped\n",
      "0 Alternative Energy News article(s) skipped due to error\n",
      "117 Alternative Energy News article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "New Atlas\n",
      "17 New Atlas article(s) scraped\n",
      "0 New Atlas article(s) skipped due to error\n",
      "113 New Atlas article(s) skipped due to age\n",
      "1 relevant article(s) collected\n",
      "\n",
      "AZoM\n",
      "58 AZoM article(s) scraped\n",
      "0 AZoM article(s) skipped due to error\n",
      "286 AZoM article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "CompositesWorld\n",
      "19 CompositesWorld article(s) scraped\n",
      "0 CompositesWorld article(s) skipped due to error\n",
      "71 CompositesWorld article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "Lightweighting World\n",
      "0 Lightweighting World article(s) scraped\n",
      "0 Lightweighting World article(s) skipped due to error\n",
      "45 Lightweighting World article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "National Center for Manufacturing Sciences\n",
      "'NoneType' object has no attribute 'text': https://www.ncms.org/ctma-technology-competition-open-deadline-april-6/ \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'text': https://www.ncms.org/join-ncms-universal-synaptics-automated-precision-today-at-mro/ \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'text': https://www.ncms.org/ncms-mma-present-2017-mfg-forum/ \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "0 National Center for Manufacturing Sciences article(s) scraped\n",
      "3 National Center for Manufacturing Sciences article(s) skipped due to error\n",
      "221 National Center for Manufacturing Sciences article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "Composites Manufacturing\n",
      "0 Composites Manufacturing article(s) scraped\n",
      "0 Composites Manufacturing article(s) skipped due to error\n",
      "54 Composites Manufacturing article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "Nanowerk\n",
      "43 Nanowerk article(s) scraped\n",
      "0 Nanowerk article(s) skipped due to error\n",
      "133 Nanowerk article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "Materials Science & Engineering\n",
      "0 Materials Science & Engineering article(s) scraped\n",
      "0 Materials Science & Engineering article(s) skipped due to error\n",
      "10 Materials Science & Engineering article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "Kenworth\n",
      "2 Kenworth article(s) scraped\n",
      "0 Kenworth article(s) skipped due to error\n",
      "18 Kenworth article(s) skipped due to age\n",
      "2 relevant article(s) collected\n",
      "\n",
      "Peterbilt\n",
      "0 Peterbilt article(s) scraped\n",
      "0 Peterbilt article(s) skipped due to error\n",
      "4 Peterbilt article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "Volvo\n",
      "1 Volvo article(s) scraped\n",
      "0 Volvo article(s) skipped due to error\n",
      "9 Volvo article(s) skipped due to age\n",
      "1 relevant article(s) collected\n",
      "\n",
      "Cummins\n",
      "4 Cummins article(s) scraped\n",
      "0 Cummins article(s) skipped due to error\n",
      "6 Cummins article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "Eaton\n",
      "'NoneType' object has no attribute 'find_all': https://www.eaton.com/us/en-us/company/news-insights/news-releases/2019/eaton-names-kirsten-park--senior-vice-president--treasury.html \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.eaton.com/us/en-us/company/news-insights/news-releases/2019/eaton-announces-intent-to-spin-off-its-lighting-business.html \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.eaton.com/us/en-us/company/news-insights/news-releases/2019/olivier-leonetti-appointed-to-eatons-board-of-directors--charles.html \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.eaton.com/us/en-us/company/news-insights/news-releases/2019/eaton-to-supply-decompression-engine-brakes-to-engine-manufactur.html \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.eaton.com/us/en-us/company/news-insights/news-releases/2019/eaton-introduces-new-technologies-for-chinas-growing-electric-ve.html \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.eaton.com/us/en-us/company/news-insights/news-releases/2019/eaton-helps-bridge-the-skilled-labor-gap.html \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.eaton.com/language-masters/en-us/company/news-insights/news-releases/2019/olivier-leonetti-appointed-to-eatons-board-of-directors--charles.html \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.eaton.com/language-masters/en-us/company/news-insights/news-releases/2019/eaton-declares-quarterly-dividend-payable-may-17--2019---eaton.html \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.eaton.com/us/en-us/company/news-insights/news-releases/2019/eaton-to-supply-high-performance-inverters-for-battery-electric-.html \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.eaton.com/us/en-us/company/news-insights/news-releases/2019/EMEA_Darkes.html \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.eaton.com/us/en-us/company/news-insights/news-releases/2019/eaton-completes-the-acquisition-of-a-controlling-interest-in-ulu.html \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.eaton.com/us/en-us/company/news-insights/news-releases/2019/eaton-contributing-leading-edge-technologies-to-supertruck-ii.html \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.eaton.com/us/en-us/company/news-insights/news-releases/2019/eaton-lighting-solutions-illuminate-new-sports-force-parks.html \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.eaton.com/us/en-us/company/news-insights/news-releases/2019/introducing-endurant-automated-transmission-to-china-commercial-.html \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.eaton.com/us/en-us/company/news-insights/news-releases/2019/eaton-declares-quarterly-dividend-payable-may-17--2019---eaton.html \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.eaton.com/us/en-us/company/news-insights/news-releases/2019/eaton-5p-lithium-ion-ups-enhances-business-continuity-for-distri.html \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.eaton.com/us/en-us/company/news-insights/news-releases/2019/eaton-named-to-ftse4good-index-series.html \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.eaton.com/us/en-us/company/news-insights/news-releases/2019/Eaton-launches-new-industrial-filter-cartridge-designed-for-high-temperature-and-chemical-exposure.html \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.eaton.com/us/en-us/company/news-insights/news-releases/2019/eaton-increases-quarterly-dividend-by-8-percent.html \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.eaton.com/language-masters/en-us/company/news-insights/news-releases/2019/eaton-introduces-new-technologies-for-chinas-growing-electric-ve.html \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.eaton.com/us/en-us/company/news-insights/news-releases/2019/eaton-names-yan-jin-senior-vice-president--investor-relations.html \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.eaton.com/language-masters/en-us/company/news-insights/news-releases/2019/eaton-to-supply-decompression-engine-brakes-to-engine-manufactur.html \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.eaton.com/us/en-us/company/news-insights/news-releases/2019/eaton-announces-live-webcast-of-march-1--2019-annual-investor-co.html \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.eaton.com/us/en-us/company/news-insights/news-releases/2019/eatons-industry-first-arrow-hart-color-coded-locking-devices.html \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.eaton.com/us/en-us/company/news-insights/news-releases/2019/eaton-to-participate-in-the-bank-of-america-merrill-lynch-global.html \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.eaton.com/us/en-us/company/news-insights/news-releases/2019/eaton-lighting-earns-energy-star-partner-of-the-year--sustained-.html \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.eaton.com/us/en-us/company/news-insights/news-releases/2019/eaton-names-paulo-ruiz-president--hydraulics-group.html \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.eaton.com/us/en-us/company/news-insights/news-releases/2019/kerry-tingley-named-vice-president-and-general-manager.html \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.eaton.com/us/en-us/company/news-insights/news-releases/2019/eaton-to-announce-first-quarter-2019-earnings---eaton.html \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.eaton.com/us/en-us/company/news-insights/news-releases/2019/HMI2019.html \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "0 Eaton article(s) scraped\n",
      "30 Eaton article(s) skipped due to error\n",
      "0 Eaton article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "Allison Transmission\n",
      "0 Allison Transmission article(s) scraped\n",
      "0 Allison Transmission article(s) skipped due to error\n",
      "0 Allison Transmission article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "Ford\n",
      "2 Ford article(s) scraped\n",
      "0 Ford article(s) skipped due to error\n",
      "8 Ford article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      " "
     ]
    },
    {
     "data": {
      "text/plain": [
       "         367765306 function calls (367362963 primitive calls) in 2423.943 seconds\n",
       "\n",
       "   Ordered by: internal time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "    40952 1126.710    0.028 1126.710    0.028 {built-in method _openssl.SSL_read}\n",
       "     4389  435.925    0.099  435.925    0.099 {built-in method time.sleep}\n",
       "     3749  220.500    0.059  220.500    0.059 {method 'recv_into' of '_socket.socket' objects}\n",
       "     4282  160.756    0.038  161.112    0.038 {built-in method _openssl.SSL_do_handshake}\n",
       "     4704   86.789    0.018   86.789    0.018 {method 'connect' of '_socket.socket' objects}\n",
       "     4282   74.362    0.017   74.362    0.017 {built-in method _openssl.SSL_CTX_load_verify_locations}\n",
       "  2712591   20.402    0.000   87.837    0.000 _lxml.py:149(start)\n",
       "8688/4344   17.943    0.002  143.066    0.033 parser.pxi:1242(feed)\n",
       " 23161385   15.922    0.000   15.922    0.000 codingstatemachine.py:66(next_state)\n",
       "      200   13.141    0.066   24.476    0.122 mbcharsetprober.py:61(feed)\n",
       "  8328443   10.499    0.000   29.951    0.000 __init__.py:392(endData)\n",
       "  8941573   10.427    0.000   10.690    0.000 element.py:248(setup)\n",
       "  2529223    9.776    0.000   30.149    0.000 element.py:873(__init__)\n",
       "  2052838    9.201    0.000   14.413    0.000 __init__.py:152(_replace_cdata_list_attribute_values)\n",
       "    19893    8.374    0.000    8.374    0.000 {method 'findall' of 're.Pattern' objects}\n",
       "  2712591    6.570    0.000   59.118    0.000 __init__.py:502(handle_starttag)\n",
       "     4703    6.036    0.001    6.094    0.001 {built-in method _socket.getaddrinfo}\n",
       "  2712578    5.588    0.000   29.368    0.000 _lxml.py:198(end)\n",
       "    18272    5.155    0.000    5.155    0.000 {built-in method nt.stat}\n",
       " 38410018    5.097    0.000    7.103    0.000 {built-in method builtins.isinstance}\n",
       "  2712578    5.034    0.000    8.033    0.000 __init__.py:479(_popToTag)\n",
       "  3206175    4.666    0.000   10.426    0.000 __init__.py:422(object_was_parsed)\n",
       "    17584    4.597    0.000    4.597    0.000 {method 'decompress' of 'zlib.Decompress' objects}\n",
       "  2712591    4.381    0.000   97.728    0.000 saxparser.pxi:374(_handleSaxTargetStartNoNs)\n",
       "       93    4.344    0.047    4.747    0.051 charsetprober.py:103(filter_with_english_letters)\n",
       "       93    4.287    0.046   10.123    0.109 utf8prober.py:57(feed)\n",
       "17821390/17811490    4.107    0.000    4.635    0.000 {built-in method builtins.len}\n",
       "  2529223    3.904    0.000    4.833    0.000 __init__.py:383(pushTag)\n",
       "  3206175    3.809    0.000    8.597    0.000 element.py:764(__new__)\n",
       "2065774/1839213    3.804    0.000    7.074    0.000 element.py:1792(_matches)\n",
       "     4693    3.714    0.001    4.431    0.001 connectionpool.py:407(close)\n",
       "     1302    3.522    0.003   11.859    0.009 sbcharsetprober.py:77(feed)\n",
       "8688/4344    2.905    0.000   20.612    0.005 parser.pxi:1368(close)\n",
       "  3176513    2.514    0.000   13.042    0.000 element.py:1766(search)\n",
       " 13379273    2.505    0.000    2.505    0.000 apihelpers.pxi:1400(funicode)\n",
       "  2712578    2.399    0.000   10.911    0.000 __init__.py:529(handle_endtag)\n",
       "  9370499    2.323    0.000    2.323    0.000 _lxml.py:80(_getNsTag)\n",
       "  2712578    2.260    0.000   33.619    0.000 saxparser.pxi:452(_handleSaxEndNoNs)\n",
       "  3695870    2.145    0.000    4.096    0.000 _lxml.py:219(data)\n",
       "  1340921    2.114    0.000    2.114    0.000 {method 'split' of 're.Pattern' objects}\n",
       "  1682588    2.102    0.000   10.907    0.000 element.py:1725(search_tag)\n",
       "  2524879    2.079    0.000    2.672    0.000 __init__.py:374(popTag)\n",
       "       40    2.020    0.051    4.460    0.111 sjisprober.py:56(feed)\n",
       " 23159650    1.880    0.000    1.880    0.000 codingstatemachine.py:80(get_current_charlen)\n",
       " 13339837    1.829    0.000    1.829    0.000 {method 'append' of 'list' objects}\n",
       "       40    1.773    0.044    4.138    0.103 eucjpprober.py:56(feed)\n",
       "  3413422    1.728    0.000    1.728    0.000 {built-in method __new__ of type object at 0x000007FEE6536BA0}\n",
       "  2092274    1.685    0.000    1.689    0.000 {method 'keys' of 'dict' objects}\n",
       "    85622    1.683    0.000    1.683    0.000 {method 'sub' of 're.Pattern' objects}\n",
       "  3695870    1.639    0.000    7.080    0.000 saxparser.pxi:493(_handleSaxData)\n",
       "  1237016    1.638    0.000    3.817    0.000 os.py:673(__getitem__)\n",
       " 15694156    1.598    0.000    1.598    0.000 chardistribution.py:70(feed)\n",
       "  2513408    1.505    0.000    2.574    0.000 jpcntx.py:143(feed)\n",
       "    16163    1.501    0.000   16.213    0.001 element.py:571(_find_all)\n",
       "  2712591    1.497    0.000   89.334    0.000 parsertarget.pxi:78(_handleSaxStart)\n",
       "  3786835    1.481    0.000    2.004    0.000 __init__.py:534(handle_data)\n",
       "  1288584    1.409    0.000    5.772    0.000 _collections_abc.py:742(__iter__)\n",
       "  3695870    1.345    0.000    5.441    0.000 parsertarget.pxi:87(_handleSaxData)\n",
       "  2956876    1.291    0.000    1.294    0.000 {method 'items' of 'dict' objects}\n",
       "  2712578    1.211    0.000   30.578    0.000 parsertarget.pxi:84(_handleSaxEnd)\n",
       "  4977219    1.201    0.000    1.201    0.000 {method 'get' of 'dict' objects}\n",
       "   144518    1.135    0.000    4.456    0.000 core.py:234(check_label)\n",
       "  7264462    1.134    0.000    1.134    0.000 {built-in method builtins.hasattr}\n",
       "  2529223    1.093    0.000    1.093    0.000 __init__.py:108(can_be_empty_element)\n",
       "   947828    1.075    0.000    2.148    0.000 intranges.py:38(intranges_contain)\n",
       "  4036295    1.050    0.000    1.919    0.000 apihelpers.pxi:1397(funicodeOrEmpty)\n",
       "  3856921    1.049    0.000    1.133    0.000 {built-in method _abc._abc_instancecheck}\n",
       "  1237016    1.021    0.000    2.179    0.000 os.py:743(encodekey)\n",
       "     9993    1.017    0.000    6.684    0.001 request.py:2456(getproxies_environment)\n",
       "  2529223    0.987    0.000    1.348    0.000 __init__.py:273(set_up_substitutions)\n",
       "  2712591    0.981    0.000   90.315    0.000 saxparser.pxi:401(_callTargetSaxStart)\n",
       "  4242298    0.944    0.000    0.944    0.000 {method 'lower' of 'str' objects}\n",
       "  9549471    0.941    0.000    0.941    0.000 element.py:1084(__bool__)\n",
       "     4351    0.929    0.000    0.929    0.000 {built-in method nt._isdir}\n",
       "  3856921    0.855    0.000    1.988    0.000 abc.py:137(__instancecheck__)\n",
       "3763394/3752227    0.775    0.000    0.822    0.000 {method 'join' of 'str' objects}\n",
       "  1199280    0.671    0.000    0.671    0.000 os.py:696(__iter__)\n",
       "  1237016    0.663    0.000    0.848    0.000 os.py:737(check_str)\n",
       "   494734    0.638    0.000    0.823    0.000 _collections_abc.py:672(keys)\n",
       "  3545170    0.619    0.000    0.653    0.000 element.py:1386(descendants)\n",
       "     4351    0.617    0.000    0.617    0.000 {built-in method _openssl.SSL_write}\n",
       "  2543566    0.609    0.000    0.609    0.000 {method 'pop' of 'list' objects}\n",
       "3246082/3226471    0.601    0.000    1.737    0.000 {built-in method builtins.next}\n",
       "     4282    0.593    0.000    5.019    0.001 decode_asn1.py:192(parse)\n",
       "     4279    0.577    0.000    0.577    0.000 {built-in method _openssl.SSL_CTX_set_cipher_list}\n",
       "    70701    0.566    0.000    0.683    0.000 _oid.py:11(__init__)\n",
       "     4279    0.559    0.000    0.559    0.000 {built-in method _openssl.SSL_CTX_new}\n",
       "       48    0.558    0.012 2237.117   46.607 <ipython-input-14-19c3509cb48d>:88(scrape_em)\n",
       "   947828    0.542    0.000    0.542    0.000 {built-in method _bisect.bisect_left}\n",
       "       93    0.535    0.006    5.281    0.057 latin1prober.py:116(feed)\n",
       "   440962    0.530    0.000    0.861    0.000 xmlerror.pxi:197(_receive)\n",
       "2003596/2003572    0.526    0.000    0.676    0.000 element.py:1689(_normalize_search_value)\n",
       "    40952    0.525    0.000 1127.805    0.028 SSL.py:1787(recv_into)\n",
       "  5425182    0.502    0.000    0.502    0.000 etree.pyx:109(__len__)\n",
       "   494793    0.490    0.000    0.552    0.000 _collections_abc.py:719(__iter__)\n",
       "  2712591    0.471    0.000    0.471    0.000 _lxml.py:189(_prefix_for_namespace)\n",
       "  1267808    0.468    0.000    0.541    0.000 jpcntx.py:192(get_order)\n",
       "   144518    0.468    0.000    0.623    0.000 core.py:67(check_bidi)\n",
       "  1243004    0.464    0.000    0.527    0.000 jpcntx.py:213(get_order)\n",
       "    75897    0.456    0.000    1.501    0.000 parse.py:361(urlparse)\n",
       "    89264    0.423    0.000    0.750    0.000 parse.py:394(urlsplit)\n",
       "     9993    0.421    0.000    0.421    0.000 {built-in method winreg.OpenKey}\n",
       "    49585    0.394    0.000    0.561    0.000 SSL.py:1600(_raise_ssl_error)\n",
       "    15207    0.389    0.000    0.389    0.000 {built-in method winreg.QueryValueEx}\n",
       "    28265    0.375    0.000    0.922    0.000 _collections_abc.py:824(update)\n",
       "   184374    0.375    0.000    0.419    0.000 parse.py:109(_coerce_args)\n",
       "  7477562    0.341    0.000    0.341    0.000 {method 'isalpha' of 'bytearray' objects}\n",
       "    71645    0.331    0.000    0.331    0.000 {method 'search' of 're.Pattern' objects}\n",
       "  1276131    0.327    0.000    0.327    0.000 {method 'upper' of 'str' objects}\n",
       "     4779    0.325    0.000    0.869    0.000 feedparser.py:471(_parse_headers)\n",
       "     6074    0.323    0.000   16.628    0.003 {built-in method builtins.eval}\n",
       "  2712578    0.315    0.000    0.315    0.000 saxparser.pxi:481(_pushSaxEndEvent)\n",
       "    19477    0.304    0.000    0.904    0.000 punkt.py:1325(_slices_from_text)\n",
       "    51710    0.293    0.000    0.721    0.000 queue.py:121(put)\n",
       "    20386    0.288    0.000    4.885    0.000 response.py:71(decompress)\n",
       "  1018732    0.281    0.000    0.281    0.000 intranges.py:34(_decode_range)\n",
       "   144518    0.280    0.000    4.924    0.000 core.py:291(ulabel)\n",
       "   560110    0.277    0.000    0.398    0.000 element.py:1049(get)\n",
       "     4542    0.269    0.000    0.269    0.000 {function socket.close at 0x00000000036516A8}\n",
       "      429    0.268    0.001    0.268    0.001 {method 'sendall' of '_socket.socket' objects}\n",
       "    59662    0.264    0.000    5.694    0.000 core.py:340(encode)\n",
       "   767926    0.255    0.000    0.255    0.000 {built-in method builtins.getattr}\n",
       "     9558    0.246    0.000    1.881    0.000 feedparser.py:218(_parsegen)\n",
       "   403506    0.245    0.000    0.247    0.000 {method 'encode' of 'str' objects}\n",
       "     4779    0.243    0.000    3.024    0.001 client.py:193(parse_headers)\n",
       "    70701    0.241    0.000    0.519    0.000 decode_asn1.py:27(_obj2txt)\n",
       "   947828    0.239    0.000    0.239    0.000 intranges.py:31(_encode_range)\n",
       "    56493    0.235    0.000   15.725    0.000 client.py:594(_safe_read)\n",
       "   530699    0.233    0.000    0.255    0.000 {method 'decode' of 'bytes' objects}\n",
       "    41631    0.231    0.000    0.231    0.000 {built-in method _openssl.X509V3_EXT_d2i}\n",
       "    44701    0.221    0.000 1348.798    0.030 socket.py:575(readinto)\n",
       "   186228    0.221    0.000    0.221    0.000 {method 'match' of 're.Pattern' objects}\n",
       "   532530    0.217    0.000    0.217    0.000 _collections_abc.py:698(__init__)\n",
       "    33088    0.214    0.000   47.727    0.001 response.py:629(read_chunked)\n",
       "     4705    0.211    0.000    0.211    0.000 socket.py:139(__init__)\n",
       "154767/98276    0.210    0.000   81.118    0.001 {method 'join' of 'bytes' objects}\n",
       "4777/4344    0.210    0.000 1716.969    0.395 sessions.py:617(send)\n",
       "    74528    0.209    0.000    0.367    0.000 _policybase.py:293(header_source_parse)\n",
       "   238219    0.207    0.000    0.207    0.000 {method 'split' of 'str' objects}\n",
       "   117345    0.203    0.000 1302.782    0.011 {method 'readline' of '_io.BufferedReader' objects}\n",
       "    43011    0.199    0.000    0.417    0.000 message.py:462(get)\n",
       "    27077    0.198    0.000    0.198    0.000 threading.py:75(RLock)\n",
       "     4282    0.194    0.000  328.232    0.077 connection.py:299(connect)\n",
       "   308015    0.187    0.000    0.187    0.000 {method 'startswith' of 'str' objects}\n",
       "    93613    0.187    0.000    0.237    0.000 _collections.py:151(__getitem__)\n",
       "   103419    0.186    0.000    0.305    0.000 threading.py:335(notify)\n",
       "     5097    0.182    0.000    0.684    0.000 cookiejar.py:452(parse_ns_headers)\n",
       "     4779    0.181    0.000 1622.682    0.340 connectionpool.py:319(_make_request)\n",
       "   131862    0.180    0.000    0.217    0.000 structures.py:46(__setitem__)\n",
       "    77176    0.178    0.000    0.461    0.000 _collections_abc.py:657(get)\n",
       "    56402    0.178    0.000    0.439    0.000 queue.py:153(get)\n",
       "   144518    0.174    0.000    5.229    0.000 core.py:266(alabel)\n",
       "    90906    0.173    0.000    1.783    0.000 _lxml.py:227(comment)\n",
       "     4779    0.172    0.000 1624.512    0.340 connectionpool.py:446(urlopen)\n",
       "    20844    0.169    0.000    0.279    0.000 message.py:497(get_all)\n",
       "    14365    0.163    0.000    0.261    0.000 element.py:599(<genexpr>)\n",
       "    73138    0.163    0.000    0.163    0.000 decode_asn1.py:724(_asn1_string_to_bytes)\n",
       "    73138    0.159    0.000    0.504    0.000 decode_asn1.py:92(_decode_general_name)\n",
       "   947828    0.155    0.000    0.155    0.000 {built-in method unicodedata.bidirectional}\n",
       "    30408    0.155    0.000    0.872    0.000 sessions.py:49(merge_setting)\n",
       "     4344    0.153    0.000  226.817    0.052 __init__.py:88(__init__)\n",
       "     4282    0.151    0.000    0.454    0.000 decode_asn1.py:380(_decode_authority_information_access)\n",
       "    59662    0.151    0.000    5.887    0.000 pyopenssl.py:170(idna_encode)\n",
       "    10422    0.150    0.000    3.318    0.000 cookiejar.py:1654(extract_cookies)\n",
       "   115193    0.150    0.000    0.312    0.000 _policybase.py:281(_sanitize_header)\n",
       "    56493    0.148    0.000   15.390    0.000 {method 'read' of '_io.BufferedReader' objects}\n",
       "   110847    0.147    0.000    0.179    0.000 structures.py:51(__getitem__)\n",
       "    13837    0.145    0.000    0.424    0.000 ntpath.py:287(expanduser)\n",
       "    40952    0.144    0.000 1127.949    0.028 pyopenssl.py:292(recv_into)\n",
       "     4777    0.142    0.000 1632.024    0.342 adapters.py:394(send)\n",
       "     4344    0.140    0.000    0.373    0.000 inspect.py:2115(_signature_from_function)\n",
       "     4779    0.140    0.000    0.175    0.000 poolmanager.py:58(_default_key_normalizer)\n",
       "    70701    0.140    0.000    0.140    0.000 {built-in method _openssl.OBJ_obj2txt}\n",
       "     4344    0.138    0.000    5.932    0.001 sessions.py:426(prepare_request)\n",
       "   465067    0.138    0.000    0.217    0.000 utils.py:34(<lambda>)\n",
       "     4279    0.138    0.000    0.780    0.000 SSL.py:705(__init__)\n",
       "    12846    0.137    0.000    0.356    0.000 SSL.py:306(wrapper)\n",
       "   128378    0.137    0.000    0.270    0.000 _oid.py:43(__eq__)\n",
       "    30408    0.137    0.000    0.378    0.000 utils.py:284(to_key_val_list)\n",
       "    88865    0.137    0.000    0.157    0.000 feedparser.py:78(readline)\n",
       "   440962    0.136    0.000    0.997    0.000 parser.pxi:612(_forwardParserError)\n",
       "     4694    0.134    0.000    1.037    0.000 connectionpool.py:159(__init__)\n",
       "    33505    0.133    0.000   27.696    0.001 response.py:593(_update_chunk_length)\n",
       "    23495    0.133    0.000    1.007    0.000 structures.py:40(__init__)\n",
       "     4344    0.133    0.000 1730.549    0.398 sessions.py:466(request)\n",
       "     2407    0.133    0.000    0.704    0.000 {built-in method pandas._libs.tslib.array_to_datetime}\n",
       "    24277    0.132    0.000    0.243    0.000 client.py:1185(putheader)\n",
       "     4779    0.128    0.000 1278.668    0.268 client.py:289(begin)\n",
       "   202491    0.127    0.000    0.161    0.000 backend.py:114(openssl_assert)\n",
       "     9194    0.126    0.000    0.683    0.000 decode_asn1.py:81(_decode_general_names)\n",
       "    39093    0.122    0.000    0.295    0.000 parse.py:154(hostname)\n",
       "    72802    0.122    0.000    0.226    0.000 enum.py:283(__call__)\n",
       "    59662    0.117    0.000    0.131    0.000 general_name.py:138(_init_without_validation)\n",
       "    14082    0.117    0.000    0.117    0.000 threading.py:216(__init__)\n",
       "     4344    0.117    0.000    0.636    0.000 inspect.py:1087(getfullargspec)\n",
       "    30081    0.117    0.000   15.841    0.001 response.py:607(_handle_chunk)\n",
       "     4392    0.117    0.000    1.117    0.000 sessions.py:365(__init__)\n",
       "    49585    0.116    0.000    0.116    0.000 {built-in method _openssl.SSL_get_error}\n",
       "     4779    0.115    0.000    3.021    0.001 utils.py:168(get_netrc_auth)\n",
       "    74528    0.113    0.000    0.210    0.000 _collections.py:209(add)\n",
       "    18292    0.112    0.000    0.331    0.000 cookiejar.py:1236(__init__)\n",
       "     4698    0.112    0.000   90.509    0.019 connection.py:33(create_connection)\n",
       "     8788    0.111    0.000    0.566    0.000 cookiejar.py:1461(_cookie_from_cookie_tuple)\n",
       "    21675    0.111    0.000    0.192    0.000 parse.py:810(quote_from_bytes)\n",
       "     4279    0.110    0.000    2.203    0.001 ssl_.py:229(create_urllib3_context)\n",
       "     4779    0.109    0.000    0.236    0.000 response.py:160(__init__)\n",
       "     4779    0.109    0.000 1275.314    0.267 client.py:256(_read_status)\n",
       "    10422    0.108    0.000    1.823    0.000 cookiejar.py:1574(make_cookies)\n",
       "   159796    0.108    0.000    0.108    0.000 {method 'find' of 'str' objects}\n",
       "     8208    0.108    0.000    1.193    0.000 cookiejar.py:933(set_ok)\n",
       "     4779    0.108    0.000   15.018    0.003 client.py:1231(_send_request)\n",
       "    39096    0.106    0.000    0.144    0.000 parse.py:190(_hostinfo)\n",
       "    72802    0.104    0.000    0.104    0.000 enum.py:525(__new__)\n",
       "     4779    0.103    0.000    1.082    0.000 response.py:499(from_httplib)\n",
       "   953318    0.103    0.000    0.103    0.000 {built-in method builtins.ord}\n",
       "     4777    0.103    0.000    3.042    0.001 adapters.py:255(build_response)\n",
       "     8688    0.102    0.000   62.496    0.007 _lxml.py:88(prepare_markup)\n",
       "    10194    0.101    0.000    0.112    0.000 cookiejar.py:1364(_normalized_cookie_tuples)\n",
       "   124893    0.100    0.000    0.196    0.000 _oid.py:58(__hash__)\n",
       "     8784    0.098    0.000    0.407    0.000 adapters.py:113(__init__)\n",
       "     4282    0.097    0.000   11.910    0.003 pyopenssl.py:345(getpeercert)\n",
       "     8561    0.097    0.000    0.350    0.000 SSL.py:303(__init__)\n",
       "     9559    0.096    0.000    0.096    0.000 {method 'readlines' of '_io._IOBase' objects}\n",
       "     4777    0.096    0.000    0.410    0.000 models.py:596(__init__)\n",
       "   440962    0.096    0.000    0.096    0.000 xmlerror.pxi:585(_getThreadErrorLog)\n",
       "     8561    0.096    0.000    0.154    0.000 functools.py:37(update_wrapper)\n",
       "    40952    0.095    0.000    0.095    0.000 {built-in method allocator}\n",
       "     4282    0.095    0.000    0.378    0.000 decode_asn1.py:260(_decode_certificate_policies)\n",
       "     9123    0.094    0.000    0.286    0.000 url.py:132(parse_url)\n",
       "    17128    0.093    0.000    0.143    0.000 crypto.py:1090(_from_raw_x509_ptr)\n",
       "     9554    0.093    0.000    0.520    0.000 utils.py:767(select_proxy)\n",
       "     4282    0.092    0.000    0.092    0.000 {built-in method today}\n",
       "     4344    0.090    0.000    0.612    0.000 models.py:355(prepare_url)\n",
       "     4912    0.090    0.000    0.121    0.000 extensions.py:489(__init__)\n",
       "     4779    0.090    0.000    0.133    0.000 socket.py:652(close)\n",
       "     4351    0.090    0.000    0.127    0.000 makefile.py:14(backport_makefile)\n",
       "   144518    0.089    0.000    0.134    0.000 core.py:127(check_initial_combiner)\n",
       "     4779    0.089    0.000    0.334    0.000 _collections.py:225(extend)\n",
       "     4282    0.088    0.000  161.622    0.038 pyopenssl.py:438(wrap_socket)\n",
       "    13513    0.088    0.000    0.553    0.000 cookies.py:508(cookiejar_from_dict)\n",
       "     7125    0.088    0.000    0.215    0.000 cookiejar.py:142(_str2time)\n",
       "    17376    0.088    0.000    0.140    0.000 inspect.py:2465(__init__)\n",
       "     4479    0.087    0.000   33.042    0.007 response.py:404(read)\n",
       "   119972    0.087    0.000    0.143    0.000 utils.py:51(_has_surrogates)\n",
       "    88865    0.087    0.000    0.243    0.000 feedparser.py:128(__next__)\n",
       "    19116    0.087    0.000    0.378    0.000 message.py:564(get_content_type)\n",
       "     4344    0.086    0.000    0.393    0.000 models.py:441(prepare_headers)\n",
       "     4279    0.086    0.000    0.889    0.000 pyopenssl.py:389(__init__)\n",
       "    29386    0.085    0.000    0.147    0.000 cookiejar.py:1197(vals_sorted_by_key)\n",
       "   440962    0.085    0.000    0.085    0.000 xmlerror.pxi:69(_setError)\n",
       "   144518    0.085    0.000    0.085    0.000 core.py:134(check_hyphen_ok)\n",
       "    16396    0.085    0.000    0.155    0.000 element.py:1662(__init__)\n",
       "    35570    0.085    0.000    0.288    0.000 _collections_abc.py:664(__contains__)\n",
       "8688/4344    0.085    0.000    0.472    0.000 inspect.py:2196(_signature_from_callable)\n",
       "   144518    0.084    0.000    0.137    0.000 core.py:143(check_nfc)\n",
       "    59662    0.083    0.000    5.990    0.000 pyopenssl.py:157(_dnsname_to_stdlib)\n",
       "     5214    0.083    0.000    4.734    0.001 utils.py:694(should_bypass_proxies)\n",
       "    17670    0.082    0.000    0.502    0.000 cookiejar.py:606(request_host)\n",
       "   440962    0.082    0.000    0.085    0.000 xmlerror.pxi:493(receive)\n",
       "    15201    0.081    0.000    0.583    0.000 cookies.py:37(__init__)\n",
       "   115193    0.079    0.000    0.391    0.000 _policybase.py:311(header_fetch_parse)\n",
       "    10422    0.078    0.000    3.699    0.000 cookies.py:118(extract_cookies_to_jar)\n",
       "    17118    0.078    0.000    0.193    0.000 enum.py:809(__or__)\n",
       "    18260    0.077    0.000    5.228    0.000 genericpath.py:16(exists)\n",
       "    74528    0.075    0.000    0.075    0.000 {method 'setdefault' of 'collections.OrderedDict' objects}\n",
       "     4282    0.074    0.000    0.250    0.000 SSL.py:1534(__init__)\n",
       "    74528    0.074    0.000    0.094    0.000 message.py:479(set_raw)\n",
       "   108112    0.074    0.000    0.116    0.000 threading.py:240(__enter__)\n",
       "    21675    0.074    0.000    0.291    0.000 parse.py:746(quote)\n",
       "    90906    0.073    0.000    1.969    0.000 saxparser.pxi:580(_handleSaxTargetComment)\n",
       "      864    0.073    0.000    0.073    0.000 <ipython-input-13-8fa8ea28c6a0>:88(<listcomp>)\n",
       "     4208    0.073    0.000    0.120    0.000 extensions.py:1377(__init__)\n",
       "     5214    0.072    0.000    4.267    0.001 utils.py:86(proxy_bypass)\n",
       "     8208    0.072    0.000    0.103    0.000 cookiejar.py:747(__init__)\n",
       "     4344    0.072    0.000    7.455    0.002 sessions.py:690(merge_environment_settings)\n",
       "     4779    0.071    0.000    2.617    0.001 parser.py:60(parsestr)\n",
       "     2239    0.071    0.000    0.378    0.000 _parser.py:668(_parse)\n",
       "     8561    0.071    0.000    0.447    0.000 SSL.py:1083(set_verify)\n",
       "     3904    0.070    0.000    0.073    0.000 datetimes.py:606(<lambda>)\n",
       "    34484    0.070    0.000    4.947    0.000 response.py:318(_decode)\n",
       "    40635    0.069    0.000    0.150    0.000 punkt.py:396(__init__)\n",
       "    19029    0.069    0.000    0.098    0.000 timeout.py:93(__init__)\n",
       "     4779    0.069    0.000    0.297    0.000 cookiejar.py:1331(add_cookie_header)\n",
       "    38841    0.069    0.000   80.859    0.002 response.py:473(stream)\n",
       "     8074    0.069    0.000    0.163    0.000 client.py:403(close)\n",
       "     8785    0.068    0.000    0.177    0.000 poolmanager.py:152(__init__)\n",
       "    18292    0.068    0.000    0.068    0.000 cookiejar.py:870(__init__)\n",
       "     1022    0.068    0.000    0.068    0.000 socket.py:334(send)\n",
       "     4779    0.067    0.000    0.276    0.000 message.py:459(<listcomp>)\n",
       "     4698    0.067    0.000    0.067    0.000 {method 'setsockopt' of '_socket.socket' objects}\n",
       "     4282    0.066    0.000    0.183    0.000 crypto.py:585(__getattr__)\n",
       "     1503    0.066    0.000    0.090    0.000 {pandas._libs.tslibs.strptime.array_strptime}\n",
       "    74983    0.066    0.000    0.066    0.000 {method 'new' of 'CompiledFFI' objects}\n",
       "     4779    0.066    0.000    0.138    0.000 feedparser.py:139(__init__)\n",
       "   103419    0.066    0.000    0.119    0.000 threading.py:255(_is_owned)\n",
       "   667771    0.066    0.000    0.066    0.000 {method 'extend' of 'bytearray' objects}\n",
       "   440962    0.066    0.000    0.066    0.000 xmlerror.pxi:473(receive)\n",
       "     4282    0.065    0.000    0.099    0.000 ssl_.py:360(is_ipaddress)\n",
       "   144518    0.063    0.000    0.085    0.000 core.py:53(valid_label_length)\n",
       "    38381    0.063    0.000    0.321    0.000 {built-in method builtins.all}\n",
       "   108112    0.062    0.000    0.086    0.000 threading.py:243(__exit__)\n",
       "     4703    0.062    0.000    6.291    0.001 socket.py:731(getaddrinfo)\n",
       "    38841    0.062    0.000   80.927    0.002 models.py:746(generate)\n",
       "     8561    0.062    0.000    0.062    0.000 {method 'callback' of 'CompiledFFI' objects}\n",
       "     4279    0.062    0.000    0.286    0.000 connectionpool.py:807(_new_conn)\n",
       "   113688    0.062    0.000    0.062    0.000 structures.py:58(<genexpr>)\n",
       "   494733    0.061    0.000    0.061    0.000 etree.pyx:112(__iter__)\n",
       "    67320    0.061    0.000    0.061    0.000 {method 'gc' of 'CompiledFFI' objects}\n",
       "     4208    0.061    0.000    0.229    0.000 decode_asn1.py:614(_decode_precert_signed_certificate_timestamps)\n",
       "   184076    0.060    0.000    0.060    0.000 {method 'partition' of 'str' objects}\n",
       "     3910    0.060    0.000    1.339    0.000 datetimes.py:106(to_datetime)\n",
       "   194764    0.060    0.000    0.060    0.000 {method 'strip' of 'str' objects}\n",
       "     9481    0.060    0.000    0.060    0.000 {method 'settimeout' of '_socket.socket' objects}\n",
       "     4777    0.060    0.000    3.783    0.001 adapters.py:203(cert_verify)\n",
       "32020/28728    0.060    0.000    0.207    0.000 cookiejar.py:1201(deepvalues)\n",
       "     4282    0.060    0.000    0.060    0.000 {built-in method _openssl.SSL_new}\n",
       "     7603    0.059    0.000    0.404    0.000 cookiejar.py:220(http2time)\n",
       "     4779    0.059    0.000    2.545    0.001 parser.py:42(parse)\n",
       "   108112    0.058    0.000    0.080    0.000 queue.py:14(_qsize)\n",
       "    41631    0.058    0.000    0.068    0.000 extensions.py:1127(__init__)\n",
       "     5214    0.058    0.000    0.602    0.000 utils.py:47(proxy_bypass_registry)\n",
       "     4779    0.058    0.000    0.155    0.000 client.py:1061(putrequest)\n",
       "    37795    0.058    0.000    0.091    0.000 _collections_abc.py:676(items)\n",
       "    44701    0.058    0.000    0.092    0.000 {method '_checkReadable' of '_io._IOBase' objects}\n",
       "     8784    0.057    0.000    4.542    0.001 _collections.py:87(clear)\n",
       "    90906    0.057    0.000    1.840    0.000 parsertarget.pxi:96(_handleSaxComment)\n",
       "     4694    0.057    0.000    0.222    0.000 queue.py:33(__init__)\n",
       "5645/4777    0.057    0.000  196.377    0.041 sessions.py:143(resolve_redirects)\n",
       "    59319    0.057    0.000    0.294    0.000 punkt.py:549(_tokenize_words)\n",
       "     9123    0.056    0.000    0.092    0.000 url.py:22(__new__)\n",
       "    19560    0.056    0.000    0.096    0.000 _parser.py:83(get_token)\n",
       "     4344    0.056    0.000    0.073    0.000 __init__.py:42(lookup)\n",
       "     4282    0.056    0.000    0.253    0.000 decode_asn1.py:483(_decode_extended_key_usage)\n",
       "    62710    0.056    0.000    0.056    0.000 {built-in method builtins.min}\n",
       "     4694    0.056    0.000    0.120    0.000 connection.py:103(__init__)\n",
       "     8784    0.056    0.000    0.056    0.000 retry.py:159(__init__)\n",
       "    13117    0.056    0.000    0.071    0.000 contextlib.py:81(__init__)\n",
       "     8784    0.056    0.000    0.233    0.000 adapters.py:146(init_poolmanager)\n",
       "    22586    0.055    0.000    0.070    0.000 structures.py:57(__iter__)\n",
       "    13117    0.055    0.000    0.300    0.000 contextlib.py:116(__exit__)\n",
       "83370/83319    0.054    0.000    0.054    0.000 {built-in method _abc._abc_subclasscheck}\n",
       "   144519    0.054    0.000    0.054    0.000 {method 'startswith' of 'bytes' objects}\n",
       "   104441    0.054    0.000    0.054    0.000 {method 'acquire' of '_thread.lock' objects}\n",
       "   144518    0.053    0.000    0.053    0.000 {built-in method unicodedata.normalize}\n",
       "     4282    0.053    0.000   11.462    0.003 pyopenssl.py:195(get_subj_alt_name)\n",
       "     4779    0.052    0.000    0.191    0.000 feedparser.py:101(push)\n",
       "     8952    0.052    0.000    0.126    0.000 _strptime.py:318(_strptime)\n",
       "     9123    0.052    0.000    0.073    0.000 url.py:99(split_first)\n",
       "     4779    0.052    0.000    0.405    0.000 _collections.py:136(__init__)\n",
       "     4344    0.052    0.000    0.236    0.000 dammit.py:299(find_declared_encoding)\n",
       "     4779    0.052    0.000    1.248    0.000 poolmanager.py:243(connection_from_pool_key)\n",
       "    30718    0.051    0.000    0.051    0.000 {built-in method builtins.sorted}\n",
       "    26260    0.051    0.000    0.164    0.000 cookies.py:51(get_full_url)\n",
       "    40635    0.051    0.000    0.088    0.000 punkt.py:595(_first_pass_annotation)\n",
       "    49585    0.051    0.000    0.051    0.000 SSL.py:284(raise_if_problem)\n",
       "    16150    0.051    0.000   16.264    0.001 element.py:1361(find_all)\n",
       "    18976    0.051    0.000    0.074    0.000 message.py:29(_splitparam)\n",
       "     4344    0.050    0.000    0.686    0.000 parsertarget.pxi:29(__cinit__)\n",
       "     4779    0.050    0.000 1278.903    0.268 client.py:1277(getresponse)\n",
       "     4392    0.050    0.000    0.297    0.000 utils.py:802(default_headers)\n",
       "     9590    0.049    0.000    0.082    0.000 parse.py:460(urlunsplit)\n",
       "     4067    0.049    0.000    0.049    0.000 {built-in method zlib.decompressobj}\n",
       "    76146    0.049    0.000    0.052    0.000 _collections.py:181(__iter__)\n",
       "     9556    0.049    0.000   81.135    0.008 models.py:815(content)\n",
       "     4344    0.049    0.000  163.889    0.038 _lxml.py:250(feed)\n",
       "     4282    0.049    0.000  236.190    0.055 ssl_.py:291(ssl_wrap_socket)\n",
       "    16896    0.048    0.000    0.272    0.000 cookiejar.py:663(escape_path)\n",
       "    21156    0.048    0.000    0.133    0.000 socket.py:97(_intenum_converter)\n",
       "     4694    0.048    0.000    0.056    0.000 client.py:827(__init__)\n",
       "     4344    0.048    0.000    0.075    0.000 inspect.py:2748(__init__)\n",
       "    51710    0.048    0.000    0.066    0.000 queue.py:17(_put)\n",
       "     8208    0.047    0.000    0.158    0.000 cookiejar.py:999(set_ok_domain)\n",
       "    14872    0.047    0.000   15.551    0.001 element.py:1350(find)\n",
       "     8564    0.047    0.000    0.169    0.000 extensions.py:1182(get_values_for_type)\n",
       "   314513    0.047    0.000    0.047    0.000 {built-in method builtins.setattr}\n",
       "    64574    0.046    0.000    0.046    0.000 {built-in method _openssl.sk_GENERAL_NAME_value}\n",
       "     8649    0.046    0.000    0.110    0.000 cookies.py:343(set_cookie)\n",
       "    41631    0.046    0.000    0.046    0.000 {built-in method _openssl.X509_get_ext}\n",
       "    86861    0.046    0.000    0.046    0.000 {method 'lstrip' of 'str' objects}\n",
       "   184374    0.046    0.000    0.046    0.000 parse.py:98(_noop)\n",
       "     4344    0.046    0.000    1.950    0.000 models.py:307(prepare)\n",
       "     9590    0.045    0.000    0.180    0.000 parse.py:449(urlunparse)\n",
       "   434292    0.045    0.000    0.045    0.000 xmlerror.pxi:63(__dealloc__)\n",
       "    10187    0.045    0.000    3.281    0.000 element.py:1103(__getattr__)\n",
       "     4477    0.045    0.000   31.167    0.007 client.py:468(readinto)\n",
       "   144518    0.045    0.000    0.045    0.000 {built-in method unicodedata.category}\n",
       "     4779    0.045    0.000    0.045    0.000 feedparser.py:53(__init__)\n",
       "     4344    0.044    0.000    0.070    0.000 parser.pxi:521(__cinit__)\n",
       "    68226    0.044    0.000    0.114    0.000 extensions.py:1188(<genexpr>)\n",
       "     8785    0.044    0.000    0.091    0.000 _collections.py:44(__init__)\n",
       "     4344    0.043    0.000    0.162    0.000 _lxml.py:61(parser_for)\n",
       "     4777    0.043    0.000    2.106    0.000 adapters.py:292(get_connection)\n",
       "    84743    0.043    0.000    0.043    0.000 {method 'rstrip' of 'str' objects}\n",
       "     4344    0.042    0.000    0.057    0.000 parser.pxi:798(__init__)\n",
       "   108112    0.042    0.000    0.042    0.000 {method '__enter__' of '_thread.lock' objects}\n",
       "     4698    0.042    0.000   90.551    0.019 connection.py:145(_new_conn)\n",
       "     8649    0.042    0.000    0.054    0.000 cookiejar.py:1641(set_cookie)\n",
       "     4344    0.042    0.000    0.044    0.000 parser.pxi:924(_newPushParserCtxt)\n",
       "     4479    0.042    0.000   31.223    0.007 client.py:436(read)\n",
       "    49480    0.041    0.000    0.041    0.000 socket.py:614(readable)\n",
       "     4742    0.041    0.000    0.062    0.000 utils.py:450(_parse_content_type_header)\n",
       "     7603    0.041    0.000    0.045    0.000 calendar.py:655(timegm)\n",
       "     4282    0.041    0.000    6.031    0.001 pyopenssl.py:236(<listcomp>)\n",
       "    34537    0.040    0.000    0.040    0.000 {method 'split' of 'bytes' objects}\n",
       "     4344    0.040    0.000    0.167    0.000 utils.py:906(get_auth_from_url)\n",
       "     4282    0.040    0.000    0.167    0.000 ssl.py:276(match_hostname)\n",
       "     8590    0.040    0.000    0.276    0.000 cookiejar.py:633(request_path)\n",
       "     4477    0.040    0.000   31.015    0.007 {method 'readinto' of '_io.BufferedReader' objects}\n",
       "     4703    0.040    0.000    0.058    0.000 idna.py:147(encode)\n",
       "    16163    0.039    0.000    0.300    0.000 element.py:1878(__init__)\n",
       "     4344    0.039    0.000    0.103    0.000 models.py:452(prepare_body)\n",
       "     4777    0.039    0.000    0.430    0.000 adapters.py:329(request_url)\n",
       "     9558    0.039    0.000    0.039    0.000 message.py:120(__init__)\n",
       "    21675    0.039    0.000    0.039    0.000 {method 'rstrip' of 'bytes' objects}\n",
       "     8688    0.039    0.000   62.326    0.007 dammit.py:240(encodings)\n",
       "     4779    0.039    0.000    0.153    0.000 response.py:38(assert_header_parsing)\n",
       "     4694    0.038    0.000    1.113    0.000 poolmanager.py:171(_new_pool)\n",
       "     4282    0.038    0.000    0.056    0.000 decode_asn1.py:355(_decode_authority_key_identifier)\n",
       "     4344    0.038    0.000    0.133    0.000 __init__.py:349(reset)\n",
       "    30390    0.038    0.000    0.063    0.000 ssl.py:191(_dnsname_match)\n",
       "    49306    0.038    0.000    0.107    0.000 {method 'add' of 'set' objects}\n",
       "     4344    0.038    0.000    0.038    0.000 parser.pxi:1409(_htmlCtxtResetPush)\n",
       "    31887    0.037    0.000    0.037    0.000 {method 'acquire' of '_thread.RLock' objects}\n",
       "     9558    0.037    0.000    1.918    0.000 feedparser.py:178(_call_parse)\n",
       "     8644    0.037    0.000    0.591    0.000 cookiejar.py:712(is_third_party)\n",
       "     4351    0.037    0.000  328.273    0.075 connectionpool.py:831(_validate_conn)\n",
       "    24540    0.037    0.000    0.037    0.000 {method 'replace' of 'str' objects}\n",
       "     4445    0.036    0.000    0.036    0.000 {built-in method numpy.core.multiarray.array}\n",
       "     4779    0.036    0.000    0.183    0.000 client.py:226(__init__)\n",
       "     4344    0.036    0.000    0.049    0.000 models.py:226(__init__)\n",
       "    12846    0.036    0.000    0.036    0.000 {built-in method _socket.inet_pton}\n",
       "    44701    0.036    0.000    0.036    0.000 {method '_checkClosed' of '_io._IOBase' objects}\n",
       "    56334    0.036    0.000    0.036    0.000 {method 'count' of 'str' objects}\n",
       "    17376    0.036    0.000    0.100    0.000 utils.py:927(check_header_validity)\n",
       "     4779    0.036    0.000    0.036    0.000 {method 'Close' of 'PyHKEY' objects}\n",
       "     7903    0.036    0.000    0.170    0.000 response.py:303(_init_decoder)\n",
       "    10194    0.036    0.000    0.716    0.000 cookiejar.py:1553(_cookies_from_attrs_set)\n",
       "    13513    0.035    0.000    0.158    0.000 cookies.py:521(<listcomp>)\n",
       "     4779    0.035    0.000    0.035    0.000 socket.py:563(__init__)\n",
       "     4066    0.035    0.000 1589.921    0.391 sessions.py:537(get)\n",
       "     4282    0.035    0.000    0.083    0.000 crypto.py:1098(to_cryptography)\n",
       "    27850    0.035    0.000    0.035    0.000 cookiejar.py:1729(__iter__)\n",
       "     3904    0.035    0.000    0.126    0.000 datetimes.py:329(__new__)\n",
       "     4344    0.035    0.000    0.062    0.000 dammit.py:218(__init__)\n",
       "    13117    0.035    0.000    0.106    0.000 contextlib.py:237(helper)\n",
       "      864    0.035    0.000    0.035    0.000 <ipython-input-13-8fa8ea28c6a0>:87(<listcomp>)\n",
       "     8644    0.035    0.000    0.079    0.000 cookiejar.py:536(domain_match)\n",
       "     3910    0.034    0.000    1.091    0.000 datetimes.py:276(_convert_listlike)\n",
       "     4779    0.034    0.000    1.509    0.000 poolmanager.py:207(connection_from_host)\n",
       "     4779    0.034    0.000    0.336    0.000 request.py:2617(getproxies_registry)\n",
       "     4282    0.034    0.000    0.074    0.000 decode_asn1.py:396(_decode_key_usage)\n",
       "   202491    0.034    0.000    0.034    0.000 binding.py:54(_openssl_assert)\n",
       "    82101    0.034    0.000    0.034    0.000 {method 'endswith' of 'str' objects}\n",
       "     3167    0.034    0.000    0.239    0.000 decode_asn1.py:502(_decode_dist_points)\n",
       "     4351    0.033    0.000    0.694    0.000 SSL.py:1705(send)\n",
       "    41631    0.033    0.000    0.033    0.000 {built-in method _openssl.X509_EXTENSION_get_critical}\n",
       "     2351    0.033    0.000    0.033    0.000 {built-in method time.localtime}\n",
       "    15806    0.033    0.000    0.217    0.000 response.py:347(_error_catcher)\n",
       "     8644    0.033    0.000    0.079    0.000 cookiejar.py:677(reach)\n",
       "     8784    0.033    0.000    0.059    0.000 sessions.py:738(mount)\n",
       "     4344    0.033    0.000    0.095    0.000 models.py:82(_encode_params)\n",
       "     4010    0.033    0.000    0.033    0.000 {built-in method pandas._libs.algos.ensure_object}\n",
       "     4779    0.033    0.000    0.050    0.000 models.py:290(__init__)\n",
       "     4344    0.033    0.000    0.033    0.000 parser.pxi:556(_resetParserContext)\n",
       "     9123    0.032    0.000    0.141    0.000 cookies.py:529(merge_cookies)\n",
       "   144518    0.032    0.000    0.032    0.000 {method 'lower' of 'bytes' objects}\n",
       "    21654    0.032    0.000    0.062    0.000 cookiejar.py:521(is_HDN)\n",
       "     4779    0.032    0.000    1.456    0.000 poolmanager.py:230(connection_from_context)\n",
       "     4279    0.032    0.000    0.986    0.000 connectionpool.py:757(__init__)\n",
       "    24277    0.032    0.000    0.032    0.000 {method 'fullmatch' of 're.Pattern' objects}\n",
       "     4422    0.032    0.000    0.338    0.000 _lxml.py:222(doctype)\n",
       "    38728    0.032    0.000    0.068    0.000 punkt.py:1515(_second_pass_annotation)\n",
       "    68226    0.032    0.000    0.045    0.000 extensions.py:1186(<genexpr>)\n",
       "        1    0.032    0.032    0.032    0.032 {built-in method _winapi.CreateProcess}\n",
       "     4779    0.032    0.000    0.628    0.000 cookies.py:135(get_cookie_header)\n",
       "     4282    0.032    0.000    0.060    0.000 ssl.py:240(_inet_paton)\n",
       "    41631    0.032    0.000    0.078    0.000 decode_asn1.py:835(<lambda>)\n",
       "    57412    0.031    0.000    0.540    0.000 punkt.py:1505(_annotate_second_pass)\n",
       "     4701    0.031    0.000    0.031    0.000 {built-in method _thread.allocate_lock}\n",
       "    13117    0.031    0.000    0.055    0.000 contextlib.py:107(__enter__)\n",
       "     4779    0.031    0.000    0.068    0.000 response.py:255(_init_length)\n",
       "     4282    0.031    0.000    0.884    0.000 decode_asn1.py:429(_decode_subject_alt_name)\n",
       "76889/19477    0.031    0.000    0.912    0.000 punkt.py:310(_pair_iter)\n",
       "    14385    0.031    0.000    0.031    0.000 {method 'format' of 'str' objects}\n",
       "     4779    0.031    0.000    0.094    0.000 message.py:213(get_payload)\n",
       "    13515    0.031    0.000    0.048    0.000 hooks.py:17(default_hooks)\n",
       "     4282    0.031    0.000    0.079    0.000 crypto.py:1419(_get_name)\n",
       "   125049    0.031    0.000    0.031    0.000 {built-in method builtins.hash}\n",
       "     4392    0.030    0.000    0.030    0.000 utils.py:793(default_user_agent)\n",
       "    10455    0.030    0.000    0.030    0.000 {built-in method _locale.setlocale}\n",
       "     4282    0.030    0.000  161.211    0.038 SSL.py:1898(do_handshake)\n",
       "     9558    0.030    0.000    0.067    0.000 timeout.py:156(clone)\n",
       "     4777    0.030    0.000    1.667    0.000 poolmanager.py:267(connection_from_url)\n",
       "     7603    0.030    0.000    0.075    0.000 cookiejar.py:72(_timegm)\n",
       "     8784    0.030    0.000    4.591    0.001 adapters.py:319(close)\n",
       "     4693    0.030    0.000    4.461    0.001 poolmanager.py:156(<lambda>)\n",
       "83370/83319    0.029    0.000    0.084    0.000 abc.py:141(__subclasscheck__)\n",
       "     4782    0.029    0.000    0.049    0.000 parse.py:386(_splitnetloc)\n",
       "    41631    0.029    0.000    0.029    0.000 {method 'cast' of 'CompiledFFI' objects}\n",
       "     8564    0.029    0.000    0.038    0.000 crypto.py:549(__setattr__)\n",
       "     4984    0.029    0.000    0.363    0.000 client.py:934(close)\n",
       "     4282    0.029    0.000    0.057    0.000 weakref.py:165(__setitem__)\n",
       "     3904    0.029    0.000    0.063    0.000 datetimes.py:627(_simple_new)\n",
       "    57087    0.028    0.000    0.028    0.000 timeout.py:103(_validate_timeout)\n",
       "    45746    0.028    0.000    0.028    0.000 {method 'read' of '_io.StringIO' objects}\n",
       "     4279    0.028    0.000    0.659    0.000 pyopenssl.py:418(set_ciphers)\n",
       "    18684    0.028    0.000    0.576    0.000 punkt.py:1370(text_contains_sentbreak)\n",
       "     4279    0.028    0.000    0.617    0.000 SSL.py:1175(set_cipher_list)\n",
       "     4694    0.028    0.000    0.035    0.000 _collections.py:58(__setitem__)\n",
       "    41631    0.028    0.000    0.028    0.000 {built-in method _openssl.X509_EXTENSION_get_object}\n",
       "     9558    0.028    0.000    0.105    0.000 cookies.py:348(update)\n",
       "    14337    0.028    0.000    0.276    0.000 message.py:588(get_content_maintype)\n",
       "    59319    0.028    0.000    0.409    0.000 punkt.py:574(_annotate_first_pass)\n",
       "    19379    0.027    0.000    0.946    0.000 punkt.py:1340(_realign_boundaries)\n",
       "    59662    0.027    0.000    0.034    0.000 core.py:60(valid_string_length)\n",
       "     4779    0.027    0.000    0.059    0.000 feedparser.py:70(close)\n",
       "    48504    0.026    0.000    0.059    0.000 element.py:980(_all_strings)\n",
       "     4779    0.026    0.000    0.076    0.000 cookiejar.py:1710(clear_expired_cookies)\n",
       "     4777    0.026    0.000    0.108    0.000 utils.py:475(get_encoding_from_headers)\n",
       "     4282    0.026    0.000    0.041    0.000 extensions.py:821(__init__)\n",
       "     4279    0.026    0.000    0.137    0.000 connection.py:228(__init__)\n",
       "     3426    0.026    0.000    0.058    0.000 response.py:336(_flush_decoder)\n",
       "     4282    0.025    0.000    0.295    0.000 pyopenssl.py:335(close)\n",
       "     8561    0.025    0.000    0.041    0.000 ssl_.py:190(resolve_cert_reqs)\n",
       "     4282    0.025    0.000   74.394    0.017 SSL.py:751(load_verify_locations)\n",
       "    51709    0.025    0.000    0.036    0.000 queue.py:20(_get)\n",
       "     4779    0.025    0.000    0.436    0.000 connectionpool.py:212(_get_conn)\n",
       "    59979    0.025    0.000    0.025    0.000 {method 'pop' of 'dict' objects}\n",
       "     4777    0.025    0.000    0.054    0.000 models.py:61(path_url)\n",
       "     4344    0.025    0.000    0.082    0.000 parser.pxi:1669(__init__)\n",
       "    12846    0.025    0.000    0.025    0.000 weakref.py:134(__getitem__)\n",
       "    30957    0.025    0.000    0.029    0.000 element.py:341(_last_descendant)\n",
       "     4779    0.025    0.000    7.817    0.002 utils.py:755(get_environ_proxies)\n",
       "     8564    0.025    0.000    0.039    0.000 extensions.py:695(__init__)\n",
       "    63944    0.024    0.000    0.162    0.000 extensions.py:1168(<genexpr>)\n",
       "       48    0.024    0.001  181.889    3.789 <ipython-input-14-19c3509cb48d>:49(get_urls)\n",
       "     4777    0.024    0.000    0.036    0.000 sessions.py:719(get_adapter)\n",
       "     8561    0.024    0.000    0.024    0.000 functools.py:67(wraps)\n",
       "     4779    0.024    0.000    0.033    0.000 utils.py:565(unquote_unreserved)\n",
       "     5214    0.024    0.000    0.115    0.000 utils.py:702(<lambda>)\n",
       "   108112    0.024    0.000    0.024    0.000 {method '__exit__' of '_thread.lock' objects}\n",
       "     4779    0.024    0.000    0.284    0.000 feedparser.py:184(close)\n",
       "     1503    0.024    0.000    0.024    0.000 {pandas._libs.tslibs.parsing._format_is_iso}\n",
       "     8649    0.024    0.000    0.043    0.000 copy.py:66(copy)\n",
       "     4344    0.024    0.000    0.055    0.000 models.py:175(register_hook)\n",
       "    12319    0.024    0.000    0.044    0.000 __init__.py:1361(debug)\n",
       "     4779    0.023    0.000    0.037    0.000 feedparser.py:197(_new_message)\n",
       "    41400    0.023    0.000    0.023    0.000 {method 'values' of 'collections.OrderedDict' objects}\n",
       "     9061    0.023    0.000    0.028    0.000 connection.py:116(host)\n",
       "    38538    0.023    0.000    0.023    0.000 {built-in method _openssl.ASN1_BIT_STRING_get_bit}\n",
       "     4694    0.023    0.000    0.039    0.000 connectionpool.py:64(__init__)\n",
       "16678/16665    0.023    0.000    0.033    0.000 common.py:1835(_get_dtype_type)\n",
       "     4344    0.022    0.000    0.814    0.000 parser.pxi:868(_createContext)\n",
       "    24274    0.022    0.000    0.040    0.000 client.py:1233(<genexpr>)\n",
       "     4282    0.022    0.000    0.203    0.000 extensions.py:1166(__init__)\n",
       "     8561    0.022    0.000    0.469    0.000 pyopenssl.py:408(verify_mode)\n",
       "     4282    0.022    0.000    5.041    0.001 x509.py:133(extensions)\n",
       "     6680    0.022    0.000    0.029    0.000 _parser.py:433(append)\n",
       "     4282    0.022    0.000    0.038    0.000 decode_asn1.py:329(_decode_basic_constraints)\n",
       "     4282    0.022    0.000    0.042    0.000 extensions.py:83(get_extension_for_class)\n",
       "     8208    0.022    0.000    0.599    0.000 cookiejar.py:967(set_ok_verifiability)\n",
       "     8564    0.021    0.000    0.048    0.000 pyopenssl.py:404(verify_mode)\n",
       "     4779    0.021    0.000    0.651    0.000 models.py:556(prepare_cookies)\n",
       "     4344    0.021    0.000    0.711    0.000 parsertarget.pxi:107(_setTarget)\n",
       "     4279    0.021    0.000    0.021    0.000 {built-in method _openssl.SSLv23_method}\n",
       "     4279    0.021    0.000    0.066    0.000 connectionpool.py:782(_prepare_conn)\n",
       "     4344    0.021    0.000    0.021    0.000 parse.py:599(unquote)\n",
       "     3904    0.021    0.000    0.102    0.000 datetimelike.py:389(__getitem__)\n",
       "     8784    0.021    0.000    0.026    0.000 sessions.py:744(<listcomp>)\n",
       "     9558    0.021    0.000    0.089    0.000 connectionpool.py:290(_get_timeout)\n",
       "     4779    0.021    0.000    0.154    0.000 {method 'close' of '_io.BufferedReader' objects}\n",
       "     4779    0.021    0.000    0.033    0.000 _collections.py:51(__getitem__)\n",
       "     4282    0.021    0.000    0.032    0.000 SSL.py:1679(set_tlsext_host_name)\n",
       "    13032    0.021    0.000    0.099    0.000 sessions.py:73(<listcomp>)\n",
       "     4344    0.021    0.000    0.024    0.000 dammit.py:273(strip_byte_order_mark)\n",
       "     4392    0.021    0.000    4.614    0.001 sessions.py:733(close)\n",
       "     8784    0.021    0.000    0.021    0.000 adapters.py:58(__init__)\n",
       "     8561    0.021    0.000    0.021    0.000 {built-in method _openssl.SSL_CTX_set_verify}\n",
       "    43732    0.020    0.000    0.020    0.000 {method 'rpartition' of 'str' objects}\n",
       "    19014    0.020    0.000    0.025    0.000 {method 'extend' of 'list' objects}\n",
       "     8561    0.020    0.000    0.037    0.000 utils.py:126(__getattr__)\n",
       "    12319    0.020    0.000    0.020    0.000 __init__.py:1619(isEnabledFor)\n",
       "    21720    0.020    0.000    0.027    0.000 inspect.py:2797(<genexpr>)\n",
       "    22590    0.020    0.000    0.025    0.000 _internal_utils.py:14(to_native_string)\n",
       "     4698    0.020    0.000    0.087    0.000 connection.py:85(_set_socket_options)\n",
       "     4777    0.020    0.000    0.021    0.000 models.py:729(iter_content)\n",
       "     4282    0.020    0.000    0.028    0.000 decode_asn1.py:345(_decode_subject_key_identifier)\n",
       "    79307    0.020    0.000    0.020    0.000 {method 'popleft' of 'collections.deque' objects}\n",
       "     8644    0.020    0.000    0.128    0.000 cookies.py:45(get_host)\n",
       "     4282    0.020    0.000   74.421    0.017 pyopenssl.py:423(load_verify_locations)\n",
       "    19258    0.020    0.000    0.096    0.000 element.py:1005(get_text)\n",
       "    34256    0.020    0.000    0.020    0.000 crypto.py:205(__init__)\n",
       "     4779    0.020    0.000    0.160    0.000 connectionpool.py:250(_put_conn)\n",
       "     4771    0.020    0.000    0.077    0.000 _parser.py:877(_parse_numeric_token)\n",
       "    40635    0.020    0.000    0.052    0.000 punkt.py:419(_get_type)\n",
       "     4067    0.020    0.000    0.069    0.000 response.py:64(__init__)\n",
       "     4780    0.020    0.000   14.392    0.003 client.py:948(send)\n",
       "    22428    0.020    0.000    0.020    0.000 {built-in method time.time}\n",
       "     4779    0.020    0.000    0.020    0.000 {method 'write' of '_io.StringIO' objects}\n",
       "    12846    0.020    0.000    0.020    0.000 {built-in method _openssl.X509_STORE_CTX_get_current_cert}\n",
       "     4284    0.019    0.000    0.019    0.000 parser.pxi:530(__dealloc__)\n",
       "     9026    0.019    0.000    0.279    0.000 cookiejar.py:622(eff_request_host)\n",
       "     4779    0.019    0.000    0.141    0.000 utils.py:589(requote_uri)\n",
       "    13515    0.019    0.000    0.019    0.000 {method 'update' of 'dict' objects}\n",
       "      864    0.019    0.000    0.136    0.000 <ipython-input-13-8fa8ea28c6a0>:69(page_scan)\n",
       "     3167    0.019    0.000    0.271    0.000 decode_asn1.py:597(_decode_crl_distribution_points)\n",
       "    26555    0.019    0.000    0.019    0.000 {method 'copy' of 'dict' objects}\n",
       "     4344    0.019    0.000    0.019    0.000 parser.pxi:1193(_initSaxDocument)\n",
       "     4279    0.019    0.000    0.043    0.000 connection.py:274(set_cert)\n",
       "     5214    0.019    0.000    0.179    0.000 response.py:224(release_conn)\n",
       "     4282    0.019    0.000    0.066    0.000 SSL.py:2191(get_peer_certificate)\n",
       "     4694    0.018    0.000    0.052    0.000 timeout.py:140(from_float)\n",
       "     4351    0.018    0.000    0.727    0.000 pyopenssl.py:325(sendall)\n",
       "    52732    0.018    0.000    0.018    0.000 {method 'append' of 'collections.deque' objects}\n",
       "     4779    0.018    0.000   14.417    0.003 client.py:1007(_send_output)\n",
       "    22608    0.018    0.000    0.082    0.000 <frozen importlib._bootstrap>:1009(_handle_fromlist)\n",
       "     8688    0.018    0.000    0.906    0.000 parser.pxi:852(_getPushParserContext)\n",
       "    29056    0.018    0.000    0.023    0.000 client.py:986(_output)\n",
       "     4344    0.018    0.000  163.933    0.038 __init__.py:339(_feed)\n",
       "    12853    0.018    0.000    0.040    0.000 {function SocketIO.close at 0x00000000036530D0}\n",
       "     4344    0.018    0.000    0.030    0.000 utils.py:107(super_len)\n",
       "     2802    0.018    0.000    0.018    0.000 {method 'flush' of 'zlib.Decompress' objects}\n",
       "    13515    0.018    0.000    0.018    0.000 hooks.py:18(<dictcomp>)\n",
       "     4779    0.017    0.000    2.045    0.000 feedparser.py:173(feed)\n",
       "    31887    0.017    0.000    0.017    0.000 {method 'release' of '_thread.RLock' objects}\n",
       "     4779    0.017    0.000    0.028    0.000 cookiejar.py:1272(_cookie_attrs)\n",
       "    76976    0.017    0.000    0.017    0.000 {built-in method builtins.callable}\n",
       "    13476    0.017    0.000    0.022    0.000 general_name.py:183(_init_without_validation)\n",
       "     4344    0.017    0.000    0.025    0.000 models.py:339(prepare_method)\n",
       "     8784    0.017    0.000    4.559    0.001 poolmanager.py:198(clear)\n",
       "     4282    0.016    0.000    0.024    0.000 weakref.py:109(remove)\n",
       "    13479    0.016    0.000    0.016    0.000 request.py:41(__init__)\n",
       "     4694    0.016    0.000    0.016    0.000 queue.py:11(_init)\n",
       "     5210    0.016    0.000    0.044    0.000 models.py:707(is_redirect)\n",
       "     4779    0.016    0.000    0.023    0.000 feedparser.py:210(_pop_message)\n",
       "     9558    0.016    0.000    0.027    0.000 feedparser.py:122(pushlines)\n",
       "    17471    0.016    0.000    0.019    0.000 dammit.py:230(_usable)\n",
       "     4779    0.016    0.000    0.060    0.000 client.py:369(_check_close)\n",
       "    11852    0.016    0.000    0.016    0.000 {method 'groups' of 're.Match' objects}\n",
       "     4779    0.016    0.000    3.443    0.001 request.py:2662(getproxies)\n",
       "     4779    0.016    0.000    0.018    0.000 retry.py:295(_is_method_retryable)\n",
       "    29962    0.016    0.000    0.016    0.000 _util.py:62(openssl_assert)\n",
       "     8208    0.016    0.000    0.260    0.000 cookiejar.py:988(set_ok_path)\n",
       "     4282    0.016    0.000    0.016    0.000 {built-in method _openssl.SSL_set_fd}\n",
       "    33834    0.016    0.000    0.016    0.000 cookiejar.py:43(_debug)\n",
       "    12846    0.016    0.000    0.016    0.000 {built-in method _openssl.X509_STORE_CTX_get_ex_data}\n",
       "     4779    0.016    0.000    0.022    0.000 client.py:415(flush)\n",
       "     4344    0.016    0.000    0.020    0.000 parse.py:832(urlencode)\n",
       "     4344    0.016    0.000    0.016    0.000 _lxml.py:70(__init__)\n",
       "     4344    0.015    0.000    0.021    0.000 parser.pxi:564(prepare)\n",
       "     4282    0.015    0.000    0.015    0.000 {built-in method _openssl.OBJ_txt2nid}\n",
       "     4477    0.015    0.000    0.015    0.000 {method 'tobytes' of 'memoryview' objects}\n",
       "    10168    0.015    0.000    0.015    0.000 x509.py:503(__init__)\n",
       "     4351    0.015    0.000    1.539    0.000 utils.py:227(extract_zipped_paths)\n",
       "     4282    0.015    0.000    0.026    0.000 extensions.py:659(__init__)\n",
       "     4344    0.015    0.000    0.185    0.000 models.py:534(prepare_auth)\n",
       "     4422    0.015    0.000    0.062    0.000 element.py:853(for_name_and_ids)\n",
       "     8564    0.015    0.000    0.184    0.000 extensions.py:1223(get_values_for_type)\n",
       "     4282    0.014    0.000    0.014    0.000 weakref.py:339(__init__)\n",
       "     4422    0.014    0.000    0.370    0.000 saxparser.pxi:508(_handleSaxTargetDoctype)\n",
       "    25632    0.014    0.000    0.018    0.000 generic.py:7(_check)\n",
       "     4779    0.014    0.000    0.033    0.000 retry.py:304(is_retry)\n",
       "     4779    0.014    0.000   15.032    0.003 client.py:1226(request)\n",
       "     4781    0.014    0.000    0.021    0.000 response.py:547(closed)\n",
       "      428    0.014    0.000    0.020    0.000 socket.py:221(makefile)\n",
       "     8564    0.014    0.000    0.054    0.000 extensions.py:301(__init__)\n",
       "    55393    0.014    0.000    0.014    0.000 {method 'group' of 're.Match' objects}\n",
       "    61740    0.014    0.000    0.014    0.000 {built-in method builtins.iter}\n",
       "     5210    0.014    0.000    0.061    0.000 sessions.py:97(get_redirect_target)\n",
       "     5097    0.014    0.000    0.017    0.000 cookiejar.py:1562(_process_rfc2109_cookies)\n",
       "     4279    0.014    0.000    0.029    0.000 SSL.py:1327(set_options)\n",
       "     5178    0.014    0.000    0.015    0.000 _parser.py:1242(_to_decimal)\n",
       "     4347    0.014    0.000    0.017    0.000 parse.py:178(_userinfo)\n",
       "     4279    0.014    0.000    0.014    0.000 {built-in method _openssl.SSL_CTX_set_mode}\n",
       "     4040    0.014    0.000    0.030    0.000 common.py:1784(_get_dtype)\n",
       "     4279    0.013    0.000    0.013    0.000 {built-in method _openssl.SSL_CTX_set_options}\n",
       "     4344    0.013    0.000    0.068    0.000 models.py:576(prepare_hooks)\n",
       "     4282    0.013    0.000    0.023    0.000 crypto.py:86(_get_backend)\n",
       "     8564    0.013    0.000    0.013    0.000 {built-in method _openssl.SSL_CTX_get_verify_mode}\n",
       "     2239    0.013    0.000    0.014    0.000 _parser.py:62(__init__)\n",
       "     4282    0.013    0.000    0.102    0.000 crypto.py:1464(get_subject)\n",
       "     4279    0.013    0.000    0.013    0.000 {built-in method _openssl.SSL_CTX_set_ecdh_auto}\n",
       "    19116    0.013    0.000    0.013    0.000 {method 'seek' of '_io.StringIO' objects}\n",
       "    39839    0.013    0.000    0.026    0.000 punkt.py:460(is_ellipsis)\n",
       "     8564    0.013    0.000    0.027    0.000 SSL.py:1125(get_verify_mode)\n",
       "    11860    0.013    0.000    0.040    0.000 common.py:332(is_datetime64_dtype)\n",
       "     7603    0.013    0.000    0.023    0.000 cookiejar.py:445(strip_quotes)\n",
       "     4779    0.013    0.000    0.289    0.000 message.py:451(items)\n",
       "     5796    0.013    0.000   15.723    0.003 <ipython-input-13-8fa8ea28c6a0>:21(try_locs)\n",
       "    10455    0.013    0.000    0.018    0.000 locale.py:384(normalize)\n",
       "     4282    0.013    0.000    0.215    0.000 extensions.py:1214(__init__)\n",
       "     4282    0.013    0.000    0.025    0.000 extensions.py:265(__init__)\n",
       "    13846    0.013    0.000    0.013    0.000 {built-in method nt.fspath}\n",
       "     9056    0.013    0.000    0.013    0.000 {built-in method builtins.max}\n",
       "     4282    0.012    0.000    0.023    0.000 SSL.py:608(_asFileDescriptor)\n",
       "     4282    0.012    0.000    0.012    0.000 {built-in method _openssl.ASN1_STRING_to_UTF8}\n",
       "     4279    0.012    0.000    0.028    0.000 SSL.py:1340(set_mode)\n",
       "     4779    0.012    0.000    0.012    0.000 {method 'pop' of 'collections.OrderedDict' objects}\n",
       "     2239    0.012    0.000    0.132    0.000 _parser.py:205(split)\n",
       "     4282    0.012    0.000    0.180    0.000 connection.py:372(_match_hostname)\n",
       "     4282    0.012    0.000    0.637    0.000 decode_asn1.py:422(_decode_general_names_extension)\n",
       "     4351    0.012    0.000    0.706    0.000 pyopenssl.py:314(_send_until_done)\n",
       "     8561    0.012    0.000    0.012    0.000 SSL.py:281(__init__)\n",
       "     1209    0.012    0.000    8.337    0.007 charsetprober.py:66(filter_international_words)\n",
       "     4779    0.012    0.000   14.428    0.003 client.py:1213(endheaders)\n",
       "     4279    0.012    0.000    0.041    0.000 pyopenssl.py:399(options)\n",
       "     4067    0.012    0.000    0.081    0.000 response.py:114(_get_decoder)\n",
       "   161099    0.012    0.000    0.012    0.000 element.py:803(name)\n",
       "     4282    0.011    0.000    0.011    0.000 extensions.py:953(__init__)\n",
       "    18595    0.011    0.000    0.011    0.000 client.py:426(isclosed)\n",
       "     4282    0.011    0.000    5.055    0.001 utils.py:158(inner)\n",
       "    12846    0.011    0.000    0.011    0.000 {built-in method _openssl.X509_STORE_CTX_set_error}\n",
       "     4779    0.011    0.000    0.099    0.000 cookiejar.py:1265(_cookies_for_request)\n",
       "    12846    0.011    0.000    0.011    0.000 {built-in method _openssl.SSL_get_ex_data_X509_STORE_CTX_idx}\n",
       "     4345    0.011    0.000    0.024    0.000 xmlerror.pxi:430(__init__)\n",
       "    10455    0.011    0.000    0.068    0.000 locale.py:571(getlocale)\n",
       "     5830    0.011    0.000    0.015    0.000 response.py:7(is_fp_closed)\n",
       "    23455    0.011    0.000    0.011    0.000 {method 'setdefault' of 'dict' objects}\n",
       "     2136    0.011    0.000    0.023    0.000 _parser.py:461(_resolve_from_stridxs)\n",
       "    18445    0.011    0.000    0.013    0.000 punkt.py:423(type_no_period)\n",
       "     2239    0.011    0.000    0.431    0.000 _parser.py:577(parse)\n",
       "    34752    0.011    0.000    0.011    0.000 inspect.py:2515(name)\n",
       "     4344    0.011    0.000  196.388    0.045 sessions.py:668(<listcomp>)\n",
       "    12846    0.011    0.000    0.011    0.000 {built-in method _openssl.X509_STORE_CTX_get_error}\n",
       "     4345    0.011    0.000    0.013    0.000 xmlerror.pxi:274(__init__)\n",
       "     4282    0.011    0.000    0.011    0.000 {built-in method _openssl.SSL_set_tlsext_host_name}\n",
       "     9558    0.011    0.000    0.011    0.000 {method 'extend' of 'collections.deque' objects}\n",
       "     4344    0.011    0.000    0.021    0.000 _internal_utils.py:30(unicode_is_ascii)\n",
       "     8784    0.011    0.000    0.017    0.000 six.py:577(itervalues)\n",
       "     4347    0.011    0.000    0.028    0.000 parse.py:146(username)\n",
       " 1657/365    0.010    0.000    0.029    0.000 element.py:1181(decode)\n",
       "     8644    0.010    0.000    0.014    0.000 cookies.py:84(unverifiable)\n",
       "    19258    0.010    0.000    0.069    0.000 element.py:1010(<listcomp>)\n",
       "     8208    0.010    0.000    0.010    0.000 cookiejar.py:952(set_ok_version)\n",
       "    51709    0.010    0.000    0.010    0.000 {method 'pop' of 'collections.deque' objects}\n",
       "     4282    0.010    0.000    0.010    0.000 {built-in method _openssl.X509_NAME_get_index_by_NID}\n",
       "     8564    0.010    0.000    0.010    0.000 {built-in method _openssl.sk_ACCESS_DESCRIPTION_value}\n",
       "     4694    0.010    0.000    0.015    0.000 connectionpool.py:878(_ipv6_host)\n",
       "     4282    0.010    0.000    0.010    0.000 pyopenssl.py:254(__init__)\n",
       "     8564    0.010    0.000    0.010    0.000 {built-in method _openssl.sk_POLICYINFO_value}\n",
       "    18684    0.010    0.000    0.075    0.000 punkt.py:263(word_tokenize)\n",
       "     8688    0.010    0.000    0.013    0.000 structures.py:60(__len__)\n",
       "     9554    0.010    0.000    0.010    0.000 {built-in method time.perf_counter}\n",
       "     4779    0.010    0.000    0.016    0.000 <string>:1(__new__)\n",
       "     4282    0.010    0.000    0.014    0.000 weakref.py:334(__new__)\n",
       "     8644    0.010    0.000    0.147    0.000 cookies.py:88(origin_req_host)\n",
       "     9558    0.010    0.000    0.017    0.000 message.py:181(is_multipart)\n",
       "     4779    0.010    0.000    0.016    0.000 timeout.py:171(start_connect)\n",
       "     4779    0.010    0.000    0.010    0.000 parser.py:18(__init__)\n",
       "     4344    0.009    0.000    0.011    0.000 docloader.pxi:172(_initResolverContext)\n",
       "     8791    0.009    0.000    0.013    0.000 {built-in method builtins.any}\n",
       "    12846    0.009    0.000    0.009    0.000 {built-in method _openssl.X509_up_ref}\n",
       "     4351    0.009    0.000    0.037    0.000 pyopenssl.py:311(settimeout)\n",
       "    14252    0.009    0.000    0.009    0.000 timeout.py:195(connect_timeout)\n",
       "    19560    0.009    0.000    0.105    0.000 _parser.py:195(__next__)\n",
       "     4779    0.009    0.000    0.013    0.000 message.py:303(set_payload)\n",
       "    10168    0.009    0.000    0.009    0.000 {built-in method _openssl.sk_SCT_value}\n",
       "     7603    0.009    0.000    0.009    0.000 {method 'index' of 'list' objects}\n",
       "     8644    0.009    0.000    0.137    0.000 cookies.py:48(get_origin_req_host)\n",
       "     4282    0.009    0.000    0.009    0.000 {built-in method _openssl.SSL_set_mode}\n",
       "     4344    0.009    0.000    0.011    0.000 saxparser.pxi:89(__cinit__)\n",
       "     8630    0.009    0.000    0.014    0.000 _util.py:127(text_to_bytes_and_warn)\n",
       "     8688    0.009    0.000    0.011    0.000 inspect.py:158(isfunction)\n",
       "     4282    0.009    0.000    0.010    0.000 extensions.py:340(__init__)\n",
       "     4779    0.009    0.000    0.163    0.000 client.py:398(_close_conn)\n",
       "     3424    0.009    0.000    0.013    0.000 response.py:75(is_response_to_head)\n",
       "     9558    0.009    0.000    0.011    0.000 response.py:200(<genexpr>)\n",
       "     4783    0.009    0.000    0.027    0.000 response.py:532(getheader)\n",
       "     4779    0.009    0.000    0.013    0.000 poolmanager.py:282(_merge_pool_kwargs)\n",
       "     8952    0.009    0.000    0.059    0.000 _strptime.py:26(_getlang)\n",
       "     4777    0.009    0.000    0.012    0.000 hooks.py:23(dispatch_hook)\n",
       "     8457    0.009    0.000    0.009    0.000 {built-in method _openssl.sk_ASN1_OBJECT_value}\n",
       "     4422    0.009    0.000    0.347    0.000 parsertarget.pxi:90(_handleSaxDoctype)\n",
       "     4781    0.009    0.000    0.010    0.000 {method 'sort' of 'list' objects}\n",
       "     5097    0.009    0.000    0.012    0.000 cookiejar.py:334(split_header_words)\n",
       "    20844    0.009    0.000    0.009    0.000 cookies.py:111(info)\n",
       "     9558    0.009    0.000    0.012    0.000 ntpath.py:34(_get_bothseps)\n",
       "     4666    0.009    0.000    0.285    0.000 socket.py:416(close)\n",
       "     4781    0.009    0.000    0.017    0.000 _parser.py:1008(_find_hms_idx)\n",
       "    19379    0.009    0.000    0.955    0.000 punkt.py:1305(span_tokenize)\n",
       "     4344    0.009    0.000    0.049    0.000 parser.pxi:579(cleanup)\n",
       "     4351    0.009    0.000    0.009    0.000 pyopenssl.py:265(_decref_socketios)\n",
       "      864    0.008    0.000    0.963    0.001 punkt.py:1323(<listcomp>)\n",
       "     6993    0.008    0.000    0.008    0.000 {method 'rfind' of 'str' objects}\n",
       "     9558    0.008    0.000    0.008    0.000 {method 'truncate' of '_io.StringIO' objects}\n",
       "     4282    0.008    0.000    0.008    0.000 {built-in method _openssl.SSL_get_peer_certificate}\n",
       "     4779    0.008    0.000    0.011    0.000 request.py:77(set_file_position)\n",
       "    10455    0.008    0.000    0.026    0.000 locale.py:467(_parse_localename)\n",
       "     7125    0.008    0.000    0.008    0.000 cookiejar.py:128(offset_from_tz_string)\n",
       "     4282    0.008    0.000    0.008    0.000 extensions.py:146(__init__)\n",
       "     4371    0.008    0.000    0.016    0.000 common.py:369(is_datetime64tz_dtype)\n",
       "     9194    0.008    0.000    0.008    0.000 {built-in method _openssl.sk_GENERAL_NAME_num}\n",
       "     3257    0.008    0.000    0.016    0.000 __init__.py:1221(getLogger)\n",
       "    12846    0.008    0.000    0.008    0.000 {built-in method _openssl.X509_STORE_CTX_get_error_depth}\n",
       "    17407    0.008    0.000    0.008    0.000 {method 'isidentifier' of 'str' objects}\n",
       "    10422    0.008    0.000    0.008    0.000 cookies.py:104(__init__)\n",
       "     8838    0.008    0.000    0.009    0.000 _parser.py:328(weekday)\n",
       "     4912    0.008    0.000    0.071    0.000 decode_asn1.py:568(_decode_distpoint)\n",
       "     6848    0.008    0.000    0.011    0.000 response.py:584(supports_chunked_reads)\n",
       "     4698    0.008    0.000    0.008    0.000 connection.py:93(allowed_gai_family)\n",
       "     4344    0.008    0.000    0.010    0.000 sessions.py:80(merge_hooks)\n",
       "     4392    0.008    0.000    4.622    0.001 sessions.py:423(__exit__)\n",
       "     4344    0.008    0.000    0.029    0.000 parser.pxi:605(_initParserContext)\n",
       "     4344    0.008    0.000    0.008    0.000 _lxml.py:247(default_parser)\n",
       "     4282    0.008    0.000    0.008    0.000 {built-in method _weakref._remove_dead_weakref}\n",
       "     2239    0.008    0.000    0.439    0.000 _parser.py:1258(parse)\n",
       "     2227    0.008    0.000    0.033    0.000 _parser.py:479(resolve_ymd)\n",
       "     2239    0.008    0.000    0.015    0.000 _parser.py:227(__init__)\n",
       "     4542    0.007    0.000    0.276    0.000 socket.py:412(_real_close)\n",
       "    28454    0.007    0.000    0.011    0.000 _parser.py:209(isword)\n",
       "     6970    0.007    0.000    0.008    0.000 extensions.py:704(<genexpr>)\n",
       "    10428    0.007    0.000    0.007    0.000 utils.py:672(set_environ)\n",
       "     3167    0.007    0.000    0.014    0.000 extensions.py:410(__init__)\n",
       "     8704    0.007    0.000    0.007    0.000 parse.py:432(<genexpr>)\n",
       "      278    0.007    0.000  141.255    0.508 api.py:63(get)\n",
       "    17809    0.007    0.000    0.007    0.000 {method 'items' of 'collections.OrderedDict' objects}\n",
       "     4282    0.007    0.000    0.007    0.000 x509.py:28(__init__)\n",
       "     4282    0.007    0.000    0.007    0.000 {built-in method _openssl.X509_get_ext_count}\n",
       "    14376    0.007    0.000    0.039    0.000 extensions.py:1380(<genexpr>)\n",
       "     3910    0.007    0.000    0.040    0.000 common.py:1083(is_datetime64_ns_dtype)\n",
       "    11064    0.007    0.000    0.008    0.000 _parser.py:335(month)\n",
       "     4344    0.007    0.000    0.007    0.000 {method 'values' of 'mappingproxy' objects}\n",
       "     8208    0.007    0.000    0.007    0.000 cookiejar.py:979(set_ok_name)\n",
       "     4344    0.007    0.000    0.016    0.000 parser.pxi:117(initThreadDictRef)\n",
       "      163    0.007    0.000    0.019    0.000 socket.py:162(__repr__)\n",
       "     8688    0.007    0.000    0.007    0.000 {method 'update' of 'collections.OrderedDict' objects}\n",
       "    18684    0.007    0.000    0.007    0.000 punkt.py:1406(_annotate_tokens)\n",
       "     2258    0.007    0.000    0.007    0.000 {method 'replace' of 'datetime.datetime' objects}\n",
       "     4344    0.007    0.000    0.009    0.000 apihelpers.pxi:1420(_utf8)\n",
       "     4344    0.007    0.000    0.010    0.000 saxparser.pxi:98(_initParserContext)\n",
       "    10108    0.007    0.000    0.008    0.000 _parser.py:342(hms)\n",
       "      435    0.007    0.000    1.035    0.002 sessions.py:276(rebuild_proxies)\n",
       "     2227    0.007    0.000    0.015    0.000 _parser.py:1209(_build_naive)\n",
       "     4282    0.007    0.000    0.007    0.000 {built-in method _openssl.X509_get_subject_name}\n",
       "     2604    0.007    0.000    0.008    0.000 sbcharsetprober.py:53(reset)\n",
       "     4779    0.007    0.000    0.008    0.000 client.py:795(_get_content_length)\n",
       "     4779    0.007    0.000    0.007    0.000 {built-in method time.monotonic}\n",
       "     8784    0.007    0.000    0.007    0.000 {method 'clear' of 'collections.OrderedDict' objects}\n",
       "     4119    0.007    0.000    0.012    0.000 SSL.py:1588(__getattr__)\n",
       "    28951    0.006    0.000    0.009    0.000 _parser.py:240(<genexpr>)\n",
       "    25898    0.006    0.000    0.009    0.000 _parser.py:214(isnum)\n",
       "     1633    0.006    0.000    0.017    0.000 element.py:77(__new__)\n",
       "     4282    0.006    0.000    0.009    0.000 crypto.py:208(add)\n",
       "      864    0.006    0.000    1.064    0.001 <ipython-input-13-8fa8ea28c6a0>:37(clean_summary)\n",
       "     5212    0.006    0.000    0.088    0.000 parse.py:325(geturl)\n",
       "     4779    0.006    0.000    0.006    0.000 timeout.py:213(read_timeout)\n",
       "    18218    0.006    0.000    0.013    0.000 punkt.py:470(is_initial)\n",
       "     8952    0.006    0.000    0.132    0.000 _strptime.py:574(_strptime_datetime)\n",
       "     8590    0.006    0.000    0.006    0.000 cookiejar.py:910(is_blocked)\n",
       "     4282    0.006    0.000    0.046    0.000 _util.py:111(byte_string)\n",
       "     3283    0.006    0.000    0.009    0.000 element.py:57(__new__)\n",
       "     4344    0.006    0.000    0.006    0.000 models.py:521(prepare_content_length)\n",
       "     4344    0.006    0.000    0.011    0.000 parsertarget.pxi:129(_handleParseResult)\n",
       "     3306    0.006    0.000    0.015    0.000 {built-in method builtins.sum}\n",
       "     4344    0.006    0.000    0.006    0.000 xmlerror.pxi:457(clear)\n",
       "     4279    0.006    0.000    0.006    0.000 ssl_.py:213(resolve_ssl_version)\n",
       "     9633    0.006    0.000    0.007    0.000 _parser.py:348(ampm)\n",
       "     8564    0.006    0.000    0.008    0.000 extensions.py:1176(__iter__)\n",
       "     5094    0.006    0.000    0.009    0.000 base.py:61(is_dtype)\n",
       "    12031    0.006    0.000    0.051    0.000 <ipython-input-13-8fa8ea28c6a0>:40(<genexpr>)\n",
       "     4282    0.006    0.000    0.007    0.000 _util.py:93(path_string)\n",
       "     3904    0.006    0.000    0.006    0.000 {built-in method pandas._libs.tslibs.timezones.tz_standardize}\n",
       "    21720    0.006    0.000    0.006    0.000 inspect.py:2519(default)\n",
       "    12739    0.006    0.000    0.007    0.000 extensions.py:823(<genexpr>)\n",
       "     4344    0.005    0.000    0.022    0.000 _collections_abc.py:701(__len__)\n",
       "     4282    0.005    0.000    0.005    0.000 {built-in method _openssl.SSL_set_connect_state}\n",
       "     1496    0.005    0.000    0.007    0.000 punycode.py:127(decode_generalized_number)\n",
       "     4344    0.005    0.000    0.005    0.000 parser.pxi:70(_findThreadParserContext)\n",
       "     4282    0.005    0.000    0.005    0.000 {built-in method _openssl.X509_NAME_ENTRY_get_data}\n",
       "     4282    0.005    0.000    0.005    0.000 {built-in method _openssl.OPENSSL_free}\n",
       "     4912    0.005    0.000    0.005    0.000 {built-in method _openssl.sk_DIST_POINT_value}\n",
       "     2136    0.005    0.000    0.007    0.000 _parser.py:468(<listcomp>)\n",
       "     4282    0.005    0.000    0.012    0.000 decode_asn1.py:834(<lambda>)\n",
       "      436    0.005    0.000    0.052    0.000 cookiejar.py:1079(return_ok)\n",
       "     4282    0.005    0.000    0.005    0.000 {built-in method _openssl.X509_NAME_get_entry}\n",
       "      748    0.005    0.000    0.015    0.000 punycode.py:157(insertion_sort)\n",
       "    12846    0.005    0.000    0.006    0.000 extensions.py:661(<genexpr>)\n",
       "     4916    0.005    0.000    0.005    0.000 element.py:1088(__setitem__)\n",
       "    17376    0.005    0.000    0.005    0.000 inspect.py:2527(kind)\n",
       "      382    0.005    0.000    0.085    0.000 cookiejar.py:1247(_cookies_for_domain)\n",
       "    13325    0.005    0.000    0.009    0.000 apihelpers.pxi:1394(funicodeOrNone)\n",
       "      251    0.005    0.000    0.011    0.000 charsetgroupprober.py:85(get_confidence)\n",
       "     4282    0.005    0.000    0.010    0.000 SSL.py:2248(set_connect_state)\n",
       "      497    0.005    0.000    0.005    0.000 {method 'clear' of 'dict' objects}\n",
       "     4208    0.005    0.000    0.005    0.000 {built-in method _openssl.sk_SCT_num}\n",
       "     4344    0.005    0.000    0.005    0.000 docloader.pxi:116(__cinit__)\n",
       "     8532    0.005    0.000    0.006    0.000 _parser.py:325(jump)\n",
       "    12846    0.005    0.000    0.006    0.000 extensions.py:267(<genexpr>)\n",
       "    17376    0.005    0.000    0.005    0.000 inspect.py:2523(annotation)\n",
       "        2    0.005    0.002    0.008    0.004 element.py:1408(select)\n",
       "     2239    0.005    0.000    0.005    0.000 _parser.py:400(__init__)\n",
       "     4065    0.005    0.000    0.005    0.000 base.py:635(_reset_identity)\n",
       "        1    0.005    0.005 2424.107 2424.107 <string>:3(<module>)\n",
       "      415    0.005    0.000    0.015    0.000 connectionpool.py:199(_new_conn)\n",
       "     4282    0.005    0.000    0.005    0.000 {built-in method _openssl.sk_ASN1_OBJECT_num}\n",
       "    12846    0.005    0.000    0.005    0.000 pyopenssl.py:465(_verify_callback)\n",
       "     8564    0.005    0.000    0.005    0.000 decode_asn1.py:717(_asn1_integer_to_int_or_none)\n",
       "     3485    0.005    0.000    0.005    0.000 {built-in method _openssl.sk_POLICYQUALINFO_value}\n",
       "      164    0.004    0.000    0.004    0.000 {method 'getsockname' of '_socket.socket' objects}\n",
       "     4282    0.004    0.000    0.004    0.000 {built-in method _openssl.sk_POLICYINFO_num}\n",
       "     4197    0.004    0.000    0.010    0.000 inference.py:251(is_list_like)\n",
       "      278    0.004    0.000  141.248    0.508 api.py:16(request)\n",
       "     4282    0.004    0.000    0.004    0.000 extensions.py:73(__init__)\n",
       "     4282    0.004    0.000    0.004    0.000 {built-in method _openssl.sk_ACCESS_DESCRIPTION_num}\n",
       "    16437    0.004    0.000    0.004    0.000 {method 'values' of 'dict' objects}\n",
       "     2802    0.004    0.000    0.007    0.000 response.py:68(__getattr__)\n",
       "     8590    0.004    0.000    0.004    0.000 cookiejar.py:925(is_not_allowed)\n",
       "     8455    0.004    0.000    0.004    0.000 __init__.py:162(deprecated_argument)\n",
       "     1619    0.004    0.000    0.007    0.000 punkt.py:1598(_ortho_heuristic)\n",
       "     9824    0.004    0.000    0.025    0.000 extensions.py:498(<genexpr>)\n",
       "     3424    0.004    0.000    0.004    0.000 {method 'flush' of '_io.BufferedReader' objects}\n",
       "     2418    0.004    0.000    0.018    0.000 charsetprober.py:39(__init__)\n",
       "     4344    0.004    0.000    0.004    0.000 _lxml.py:146(close)\n",
       "     4694    0.004    0.000    0.004    0.000 client.py:871(_get_hostport)\n",
       "     4344    0.004    0.000    0.008    0.000 docloader.pxi:166(clear)\n",
       "     8208    0.004    0.000    0.004    0.000 cookiejar.py:1058(set_ok_port)\n",
       "     4344    0.004    0.000    0.009    0.000 parser.pxi:101(_getThreadDict)\n",
       "     7603    0.004    0.000    0.004    0.000 {method 'toordinal' of 'datetime.date' objects}\n",
       "    19824    0.004    0.000    0.004    0.000 punkt.py:432(type_no_sentperiod)\n",
       "     1302    0.004    0.000    0.006    0.000 mbcharsetprober.py:45(reset)\n",
       "     4694    0.004    0.000    0.004    0.000 connection.py:135(host)\n",
       "     1022    0.004    0.000    0.076    0.000 iostream.py:195(schedule)\n",
       "     2227    0.004    0.000    0.005    0.000 _parser.py:386(validate)\n",
       "      186    0.004    0.000   56.633    0.304 charsetgroupprober.py:65(feed)\n",
       "       95    0.004    0.000   62.013    0.653 universaldetector.py:111(feed)\n",
       "     3485    0.004    0.000    0.004    0.000 {built-in method _openssl.sk_POLICYQUALINFO_num}\n",
       "     4178    0.004    0.000    0.011    0.000 common.py:89(is_object_dtype)\n",
       "     9558    0.004    0.000    0.004    0.000 feedparser.py:125(__iter__)\n",
       "    22227    0.004    0.000    0.004    0.000 {built-in method builtins.issubclass}\n",
       "    28454    0.004    0.000    0.004    0.000 {method 'isalpha' of 'str' objects}\n",
       "     1302    0.004    0.000    0.016    0.000 sbcharsetprober.py:39(__init__)\n",
       "      435    0.004    0.000    0.039    0.000 models.py:328(copy)\n",
       "     4282    0.004    0.000    0.005    0.000 extensions.py:99(__iter__)\n",
       "     4344    0.004    0.000    0.020    0.000 parser.pxi:127(initParserDict)\n",
       "     4279    0.003    0.000    0.003    0.000 pyopenssl.py:395(options)\n",
       "    18231    0.003    0.000    0.003    0.000 {method 'end' of 're.Match' objects}\n",
       "    18684    0.003    0.000    0.005    0.000 punkt.py:247(_word_tokenizer_re)\n",
       "      810    0.003    0.000    0.082    0.000 iostream.py:382(write)\n",
       "     1299    0.003    0.000    0.006    0.000 _collections_abc.py:790(pop)\n",
       "     4514    0.003    0.000    0.003    0.000 {method 'fileno' of '_socket.socket' objects}\n",
       "     4344    0.003    0.000    0.003    0.000 dammit.py:222(<listcomp>)\n",
       "     8688    0.003    0.000    0.003    0.000 __init__.py:105(reset)\n",
       "     2227    0.003    0.000    0.007    0.000 _parser.py:1169(_build_tzaware)\n",
       "     4777    0.003    0.000    0.003    0.000 utils.py:514(iter_slices)\n",
       "     4000    0.003    0.000    0.003    0.000 {built-in method pandas._libs.lib.is_integer}\n",
       "     8644    0.003    0.000    0.003    0.000 cookies.py:65(is_unverifiable)\n",
       "     3167    0.003    0.000    0.003    0.000 {built-in method _openssl.sk_DIST_POINT_num}\n",
       "      864    0.003    0.000    0.033    0.000 <ipython-input-13-8fa8ea28c6a0>:12(replace_em)\n",
       "    17505    0.003    0.000    0.003    0.000 {method 'start' of 're.Match' objects}\n",
       "  231/122    0.003    0.000    0.014    0.000 base.py:255(__new__)\n",
       "     1346    0.003    0.000    0.015    0.000 re.py:271(_compile)\n",
       "     4900    0.003    0.000    0.003    0.000 element.py:1068(__getitem__)\n",
       "     3904    0.003    0.000    0.003    0.000 {built-in method pandas._libs.tslibs.timezones.maybe_get_tz}\n",
       "     1355    0.003    0.000    0.003    0.000 response.py:279(<listcomp>)\n",
       "     4779    0.003    0.000    0.003    0.000 cookies.py:81(get_new_headers)\n",
       "    26260    0.003    0.000    0.003    0.000 {method 'isdigit' of 'str' objects}\n",
       "     4344    0.003    0.000    0.003    0.000 etree.pyx:280(clear)\n",
       "     3257    0.003    0.000    0.019    0.000 __init__.py:1930(getLogger)\n",
       "      441    0.003    0.000    0.003    0.000 {method '__reduce_ex__' of 'object' objects}\n",
       "     8079    0.003    0.000    0.004    0.000 extensions.py:413(<genexpr>)\n",
       "      435    0.003    0.000    0.016    0.000 cookies.py:414(copy)\n",
       "      441    0.003    0.000    0.006    0.000 copy.py:268(_reconstruct)\n",
       "      186    0.003    0.000    0.010    0.000 charsetgroupprober.py:39(reset)\n",
       "     2227    0.003    0.000    0.017    0.000 _parser.py:239(__len__)\n",
       " 1657/365    0.003    0.000    0.025    0.000 element.py:1281(decode_contents)\n",
       "       81    0.003    0.000    0.003    0.000 wait.py:41(_retry_on_intr)\n",
       "     6305    0.003    0.000    0.005    0.000 _parser.py:1058(_could_be_tzname)\n",
       "     4282    0.003    0.000    0.003    0.000 extensions.py:236(__init__)\n",
       "      748    0.003    0.000    0.019    0.000 punycode.py:182(punycode_decode)\n",
       "      861    0.003    0.000    0.029    0.000 structures.py:80(copy)\n",
       "     4344    0.003    0.000    0.003    0.000 {method 'reverse' of 'list' objects}\n",
       "     1209    0.003    0.000    8.321    0.007 re.py:215(findall)\n",
       "     4360    0.002    0.000    0.002    0.000 {built-in method pandas._libs.lib.is_scalar}\n",
       "      416    0.002    0.000   13.375    0.032 connection.py:180(connect)\n",
       "     4344    0.002    0.000    0.002    0.000 apihelpers.pxi:1363(_is_valid_xml_utf8)\n",
       "     4344    0.002    0.000    0.002    0.000 models.py:184(<genexpr>)\n",
       "     3904    0.002    0.000    0.002    0.000 datetimes.py:604(_box_func)\n",
       "      382    0.002    0.000    0.015    0.000 cookiejar.py:1166(domain_return_ok)\n",
       "      435    0.002    0.000    0.236    0.001 sessions.py:256(rebuild_auth)\n",
       "     4344    0.002    0.000    0.002    0.000 inspect.py:2831(return_annotation)\n",
       "     2136    0.002    0.000    0.002    0.000 _parser.py:476(<dictcomp>)\n",
       "     3258    0.002    0.000    0.004    0.000 __init__.py:212(_acquireLock)\n",
       "     4777    0.002    0.000    0.002    0.000 adapters.py:358(add_headers)\n",
       "      435    0.002    0.000    0.002    0.000 sessions.py:317(rebuild_method)\n",
       "     4345    0.002    0.000    0.002    0.000 xmlerror.pxi:427(__cinit__)\n",
       "     4282    0.002    0.000    0.002    0.000 pyopenssl.py:240(<genexpr>)\n",
       "     4779    0.002    0.000    0.002    0.000 connectionpool.py:280(_validate_conn)\n",
       "     4779    0.002    0.000    0.002    0.000 {function HTTPResponse.flush at 0x000000000410B400}\n",
       "       93    0.002    0.000    0.026    0.000 sbcsgroupprober.py:44(__init__)\n",
       "     3258    0.002    0.000    0.003    0.000 __init__.py:221(_releaseLock)\n",
       "     3193    0.002    0.000    0.009    0.000 element.py:203(format_string)\n",
       "     2352    0.002    0.000    0.002    0.000 {built-in method builtins.abs}\n",
       "     6878    0.002    0.000    0.003    0.000 _parser.py:219(isspace)\n",
       "       93    0.002    0.000    0.036    0.000 mbcsgroupprober.py:42(__init__)\n",
       "     5268    0.002    0.000    0.003    0.000 base.py:641(__len__)\n",
       "     4344    0.002    0.000    0.002    0.000 saxparser.pxi:95(_setSaxParserTarget)\n",
       "     4344    0.002    0.000    0.002    0.000 etree.pyx:273(__init__)\n",
       "      367    0.002    0.000    0.003    0.000 {built-in method _warnings.warn}\n",
       "     3898    0.002    0.000    0.002    0.000 {method 'date' of 'datetime.datetime' objects}\n",
       "       95    0.002    0.000   62.030    0.653 __init__.py:24(detect)\n",
       "      744    0.002    0.000    0.006    0.000 codingstatemachine.py:55(__init__)\n",
       "     4392    0.002    0.000    0.002    0.000 sessions.py:420(__enter__)\n",
       "   201/17    0.002    0.000    0.005    0.000 sre_parse.py:475(_parse)\n",
       "     4344    0.002    0.000    0.002    0.000 parser.pxi:893(_registerHtmlErrorHandler)\n",
       "     4371    0.002    0.000    0.002    0.000 charsetprober.py:44(reset)\n",
       "     4344    0.002    0.000    0.002    0.000 inspect.py:2827(parameters)\n",
       "     1022    0.002    0.000    0.004    0.000 threading.py:1080(is_alive)\n",
       "      864    0.002    0.000    0.966    0.001 punkt.py:1265(tokenize)\n",
       "     4345    0.002    0.000    0.002    0.000 xmlerror.pxi:183(__init__)\n",
       "     4344    0.002    0.000    0.002    0.000 saxparser.pxi:229(__cinit__)\n",
       "      864    0.002    0.000    0.002    0.000 {method 'finditer' of 're.Pattern' objects}\n",
       "   251/17    0.002    0.000    0.004    0.000 sre_compile.py:71(_compile)\n",
       "      580    0.002    0.000    0.002    0.000 cookiejar.py:1668(clear)\n",
       "      436    0.002    0.000    0.013    0.000 cookiejar.py:1144(return_ok_domain)\n",
       "      404    0.002    0.000    0.084    0.000 {built-in method builtins.print}\n",
       "     2227    0.002    0.000    0.002    0.000 _parser.py:487(<dictcomp>)\n",
       "       93    0.002    0.000    1.572    0.017 hebrewprober.py:196(feed)\n",
       "      864    0.002    0.000    0.002    0.000 <ipython-input-13-8fa8ea28c6a0>:86(<listcomp>)\n",
       "      651    0.002    0.000    0.002    0.000 chardistribution.py:46(__init__)\n",
       "      109    0.001    0.000    0.004    0.000 internals.py:3363(_rebuild_blknos_and_blklocs)\n",
       "     4862    0.001    0.000    0.001    0.000 punycode.py:70(T)\n",
       "     1496    0.001    0.000    0.001    0.000 punycode.py:91(adapt)\n",
       "     3904    0.001    0.000    0.001    0.000 frequencies.py:74(to_offset)\n",
       "      864    0.001    0.000    0.001    0.000 <ipython-input-13-8fa8ea28c6a0>:76(<dictcomp>)\n",
       "      651    0.001    0.000    0.007    0.000 mbcharsetprober.py:39(__init__)\n",
       "     5178    0.001    0.000    0.001    0.000 {method 'is_finite' of 'decimal.Decimal' objects}\n",
       "       95    0.001    0.000   62.032    0.653 dammit.py:33(chardet_dammit)\n",
       "     4344    0.001    0.000    0.001    0.000 parser.pxi:884(_configureSaxContext)\n",
       "     3193    0.001    0.000    0.005    0.000 element.py:109(_substitute_if_appropriate)\n",
       "      873    0.001    0.000    0.001    0.000 cookiejar.py:796(is_expired)\n",
       "     4344    0.001    0.000    0.001    0.000 saxparser.pxi:204(flushEvents)\n",
       "     4344    0.001    0.000    0.001    0.000 saxparser.pxi:43(__cinit__)\n",
       "     3904    0.001    0.000    0.001    0.000 datetimelike.py:232(freq)\n",
       "       95    0.001    0.000    0.013    0.000 universaldetector.py:220(close)\n",
       "     4344    0.001    0.000    0.001    0.000 parser.pxi:552(_initParserContext)\n",
       "      326    0.001    0.000    0.001    0.000 enum.py:562(__str__)\n",
       "      864    0.001    0.000    0.965    0.001 punkt.py:1316(sentences_from_text)\n",
       "     4344    0.001    0.000    0.001    0.000 saxparser.pxi:105(_connectTarget)\n",
       "     1973    0.001    0.000    0.001    0.000 sbcharsetprober.py:124(get_confidence)\n",
       "     2629    0.001    0.000    0.002    0.000 punkt.py:442(first_upper)\n",
       "     3904    0.001    0.000    0.001    0.000 datetimes.py:684(tz)\n",
       "      428    0.001    0.000    0.010    0.000 socket.py:406(_decref_socketios)\n",
       "      416    0.001    0.000    0.001    0.000 connection.py:172(_prepare_conn)\n",
       "     8688    0.001    0.000    0.001    0.000 etree.pyx:305(_has_raised)\n",
       "     6878    0.001    0.000    0.001    0.000 {method 'isspace' of 'str' objects}\n",
       "     1953    0.001    0.000    0.001    0.000 chardistribution.py:61(reset)\n",
       "     3904    0.001    0.000    0.001    0.000 base.py:547(_deepcopy_if_needed)\n",
       "       61    0.001    0.000    0.001    0.000 {pandas._libs.lib.fast_multiget}\n",
       "      388    0.001    0.000    0.001    0.000 {built-in method numpy.core.multiarray.empty}\n",
       "     2136    0.001    0.000    0.001    0.000 _parser.py:469(<listcomp>)\n",
       "     2736    0.001    0.000    0.001    0.000 _parser.py:1063(<genexpr>)\n",
       "      381    0.001    0.000    0.003    0.000 cookies.py:68(has_header)\n",
       "      435    0.001    0.000    0.002    0.000 models.py:942(close)\n",
       "      719    0.001    0.000    0.001    0.000 <ipython-input-13-8fa8ea28c6a0>:47(<listcomp>)\n",
       "      187    0.001    0.000    0.001    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
       "      435    0.001    0.000    0.017    0.000 cookies.py:426(_copy_cookie_jar)\n",
       "     2232    0.001    0.000    0.001    0.000 codingstatemachine.py:63(reset)\n",
       "      436    0.001    0.000    0.029    0.000 cookiejar.py:1106(return_ok_verifiability)\n",
       "      810    0.001    0.000    0.001    0.000 iostream.py:307(_is_master_process)\n",
       "     1022    0.001    0.000    0.001    0.000 iostream.py:93(_event_pipe)\n",
       "      864    0.001    0.000    0.002    0.000 punkt.py:279(period_context_re)\n",
       "      748    0.001    0.000    0.020    0.000 punycode.py:204(decode)\n",
       "      382    0.001    0.000    0.013    0.000 cookiejar.py:1188(path_return_ok)\n",
       "      810    0.001    0.000    0.021    0.000 iostream.py:320(_schedule_flush)\n",
       "      441    0.001    0.000    0.001    0.000 copyreg.py:87(__newobj__)\n",
       "     3193    0.001    0.000    0.006    0.000 element.py:146(substitute)\n",
       "      558    0.001    0.000    0.001    0.000 jpcntx.py:131(reset)\n",
       "       48    0.001    0.000    0.003    0.000 internals.py:4880(form_blocks)\n",
       "      161    0.001    0.000    0.001    0.000 {pandas._libs.lib.infer_dtype}\n",
       "     3193    0.001    0.000    0.003    0.000 dammit.py:139(substitute_xml)\n",
       "     3193    0.001    0.000    0.006    0.000 element.py:124(substitute_xml)\n",
       "     4094    0.001    0.000    0.001    0.000 charsetprober.py:54(state)\n",
       "      435    0.001    0.000    0.001    0.000 structures.py:54(__delitem__)\n",
       "      163    0.001    0.000    0.003    0.000 socket.py:432(family)\n",
       "      436    0.001    0.000    0.001    0.000 cookiejar.py:1124(return_ok_expires)\n",
       "       95    0.001    0.000    0.003    0.000 universaldetector.py:81(__init__)\n",
       "     2226    0.001    0.000    0.001    0.000 _parser.py:366(convertyear)\n",
       "      163    0.001    0.000    0.001    0.000 {method 'getpeername' of '_socket.socket' objects}\n",
       "     1355    0.001    0.000    0.001    0.000 {method 'pop' of 'set' objects}\n",
       "       93    0.001    0.000    0.005    0.000 sjisprober.py:37(__init__)\n",
       "     9282    0.001    0.000    0.001    0.000 {method 'isalpha' of 'bytes' objects}\n",
       "     2136    0.001    0.000    0.001    0.000 _parser.py:411(has_month)\n",
       "      218    0.001    0.000    0.003    0.000 common.py:301(_asarray_tuplesafe)\n",
       "      242    0.001    0.000    0.006    0.000 parse.py:83(clear_cache)\n",
       "      516    0.001    0.000    0.001    0.000 generic.py:4378(__setattr__)\n",
       "      397    0.001    0.000    0.001    0.000 _parser.py:1100(_parse_min_sec)\n",
       "     2719    0.001    0.000    0.001    0.000 dammit.py:102(quoted_attribute_value)\n",
       "      186    0.001    0.000    0.003    0.000 charsetgroupprober.py:33(__init__)\n",
       "       48    0.001    0.000    0.008    0.000 generic.py:669(transpose)\n",
       "     1022    0.001    0.000    0.001    0.000 threading.py:1038(_wait_for_tstate_lock)\n",
       "        3    0.001    0.000    0.001    0.000 <frozen importlib._bootstrap_external>:914(get_data)\n",
       "       93    0.001    0.000    1.569    0.017 re.py:185(sub)\n",
       "       93    0.001    0.000    0.004    0.000 eucjpprober.py:37(__init__)\n",
       "       93    0.001    0.000    0.003    0.000 utf8prober.py:38(__init__)\n",
       "     1731    0.001    0.000    0.001    0.000 sre_parse.py:164(__getitem__)\n",
       "      433    0.001    0.000    0.001    0.000 {method 'insert' of 'list' objects}\n",
       "     4344    0.001    0.000    0.001    0.000 etree.pyx:289(clear)\n",
       "      436    0.001    0.000    0.001    0.000 cookiejar.py:1282(<lambda>)\n",
       "      248    0.001    0.000    0.001    0.000 utf8prober.py:76(get_confidence)\n",
       "     1956    0.001    0.000    0.001    0.000 sre_parse.py:233(__next)\n",
       "      186    0.001    0.000    0.001    0.000 jpcntx.py:123(__init__)\n",
       "     1657    0.001    0.000    0.001    0.000 element.py:940(is_empty_element)\n",
       "       96    0.001    0.000    0.030    0.000 frame.py:334(__init__)\n",
       "   259/51    0.001    0.000    0.001    0.000 sre_parse.py:174(getwidth)\n",
       "      282    0.001    0.000    0.001    0.000 hebrewprober.py:255(charset_name)\n",
       "     1509    0.001    0.000    0.001    0.000 punkt.py:447(first_lower)\n",
       "       33    0.001    0.000    0.002    0.000 parse.py:479(urljoin)\n",
       "      130    0.001    0.000    0.001    0.000 sre_compile.py:276(_optimize_charset)\n",
       "      143    0.001    0.000    0.002    0.000 internals.py:4137(iget)\n",
       "     1521    0.001    0.000    0.001    0.000 {built-in method builtins.chr}\n",
       "      375    0.001    0.000    0.001    0.000 internals.py:3307(shape)\n",
       "     2629    0.001    0.000    0.001    0.000 {method 'isupper' of 'str' objects}\n",
       "      435    0.001    0.000    0.001    0.000 cookiejar.py:1244(set_policy)\n",
       "      186    0.000    0.000    0.001    0.000 utf8prober.py:44(reset)\n",
       "     2604    0.000    0.000    0.000    0.000 enums.py:59(get_num_categories)\n",
       "       93    0.000    0.000    0.003    0.000 euckrprober.py:35(__init__)\n",
       "     1317    0.000    0.000    0.001    0.000 sbcharsetprober.py:70(language)\n",
       "       93    0.000    0.000    0.003    0.000 gb2312prober.py:34(__init__)\n",
       "      265    0.000    0.000    0.000    0.000 generic.py:124(__init__)\n",
       "      186    0.000    0.000    0.002    0.000 sjisprober.py:44(reset)\n",
       "       93    0.000    0.000    0.003    0.000 big5prober.py:35(__init__)\n",
       "       93    0.000    0.000    0.003    0.000 euctwprober.py:34(__init__)\n",
       "     2226    0.000    0.000    0.000    0.000 _parser.py:407(has_year)\n",
       "      208    0.000    0.000    0.001    0.000 internals.py:116(__init__)\n",
       "       48    0.000    0.000    0.005    0.000 frame.py:7644(_homogenize)\n",
       "      435    0.000    0.000    0.000    0.000 {method 'copy' of 'collections.OrderedDict' objects}\n",
       "      169    0.000    0.000    0.003    0.000 series.py:166(__init__)\n",
       "       81    0.000    0.000    0.003    0.000 wait.py:68(select_wait_for_socket)\n",
       "      161    0.000    0.000    0.001    0.000 base.py:473(_simple_new)\n",
       "     2239    0.000    0.000    0.000    0.000 _parser.py:192(__iter__)\n",
       "     1009    0.000    0.000    0.001    0.000 element.py:1120(__eq__)\n",
       "      109    0.000    0.000    0.006    0.000 internals.py:3265(__init__)\n",
       "      475    0.000    0.000    0.000    0.000 {method 'startswith' of 'bytearray' objects}\n",
       "      4/1    0.000    0.000 2424.108 2424.108 {built-in method builtins.exec}\n",
       "      436    0.000    0.000    0.000    0.000 cookiejar.py:1097(return_ok_version)\n",
       "       93    0.000    0.000    0.003    0.000 cp949prober.py:35(__init__)\n",
       "     1531    0.000    0.000    0.001    0.000 sre_parse.py:254(get)\n",
       "      252    0.000    0.000    0.001    0.000 common.py:1688(is_extension_array_dtype)\n",
       "     1658    0.000    0.000    0.001    0.000 element.py:1073(__iter__)\n",
       "       93    0.000    0.000    1.570    0.017 charsetprober.py:61(filter_high_byte_only)\n",
       "      186    0.000    0.000    0.001    0.000 chardistribution.py:133(__init__)\n",
       "      186    0.000    0.000    0.001    0.000 eucjpprober.py:44(reset)\n",
       "      397    0.000    0.000    0.001    0.000 _parser.py:360(tzoffset)\n",
       "       96    0.000    0.000    0.001    0.000 cast.py:1207(construct_1d_object_array_from_listlike)\n",
       "      810    0.000    0.000    0.000    0.000 {built-in method nt.getpid}\n",
       "      748    0.000    0.000    0.000    0.000 {method 'rfind' of 'bytes' objects}\n",
       "      270    0.000    0.000    0.001    0.000 dtypes.py:707(is_dtype)\n",
       "      163    0.000    0.000    0.001    0.000 socket.py:438(type)\n",
       "      143    0.000    0.000    0.007    0.000 generic.py:2484(_get_item_cache)\n",
       "      113    0.000    0.000    0.001    0.000 internals.py:3148(get_block_type)\n",
       "       48    0.000    0.000    0.004    0.000 frame.py:461(_init_ndarray)\n",
       "      109    0.000    0.000    0.001    0.000 internals.py:3488(_verify_integrity)\n",
       "       95    0.000    0.000    0.000    0.000 universaldetector.py:94(reset)\n",
       "       48    0.000    0.000    0.014    0.000 <ipython-input-13-8fa8ea28c6a0>:58(print_results)\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method io.open}\n",
       "      381    0.000    0.000    0.000    0.000 cookies.py:78(add_unredirected_header)\n",
       "       12    0.000    0.000    0.000    0.000 {built-in method builtins.__build_class__}\n",
       "       74    0.000    0.000    0.001    0.000 cast.py:867(maybe_infer_to_datetimelike)\n",
       "       93    0.000    0.000    0.001    0.000 hebrewprober.py:154(__init__)\n",
       "      186    0.000    0.000    0.000    0.000 hebrewprober.py:178(is_final)\n",
       "     1318    0.000    0.000    0.001    0.000 sbcharsetprober.py:63(charset_name)\n",
       "      745    0.000    0.000    0.001    0.000 common.py:513(is_categorical_dtype)\n",
       "      143    0.000    0.000    0.003    0.000 internals.py:4108(get)\n",
       "    64/17    0.000    0.000    0.005    0.000 sre_parse.py:417(_parse_sub)\n",
       "       48    0.000    0.000    0.025    0.001 frame.py:426(_init_dict)\n",
       "      638    0.000    0.000    0.001    0.000 mbcharsetprober.py:90(get_confidence)\n",
       "      182    0.000    0.000    0.001    0.000 internals.py:2298(__init__)\n",
       "      100    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.arange}\n",
       "      112    0.000    0.000    0.001    0.000 sjisprober.py:89(get_confidence)\n",
       "       93    0.000    0.000    0.001    0.000 chardistribution.py:193(__init__)\n",
       "      169    0.000    0.000    0.001    0.000 internals.py:4639(__init__)\n",
       "       48    0.000    0.000    0.000    0.000 <ipython-input-14-19c3509cb48d>:11(__init__)\n",
       "      208    0.000    0.000    0.002    0.000 internals.py:3191(make_block)\n",
       "       48    0.000    0.000    0.005    0.000 frame.py:7367(extract_index)\n",
       "      121    0.000    0.000    0.000    0.000 latin1prober.py:130(get_confidence)\n",
       "       93    0.000    0.000    0.001    0.000 jpcntx.py:184(__init__)\n",
       "      365    0.000    0.000    0.000    0.000 element.py:241(_formatter_for_name)\n",
       "       48    0.000    0.000    0.008    0.000 frame.py:2371(transpose)\n",
       "      858    0.000    0.000    0.000    0.000 chardistribution.py:84(get_confidence)\n",
       "       85    0.000    0.000    0.004    0.000 connection.py:7(is_connection_dropped)\n",
       "      208    0.000    0.000    0.000    0.000 internals.py:237(mgr_locs)\n",
       "       93    0.000    0.000    0.001    0.000 latin1prober.py:97(__init__)\n",
       "      344    0.000    0.000    0.000    0.000 _parser.py:354(pertain)\n",
       "       87    0.000    0.000    0.002    0.000 series.py:4019(_sanitize_array)\n",
       "      286    0.000    0.000    0.000    0.000 {method 'get_loc' of 'pandas._libs.index.IndexEngine' objects}\n",
       "      143    0.000    0.000    0.002    0.000 frame.py:3100(_box_col_values)\n",
       "       13    0.000    0.000    0.015    0.001 frame.py:4340(duplicated)\n",
       "      436    0.000    0.000    0.000    0.000 cookiejar.py:1118(return_ok_secure)\n",
       "      435    0.000    0.000    0.000    0.000 cookies.py:421(get_policy)\n",
       "      474    0.000    0.000    0.002    0.000 element.py:799(output_ready)\n",
       "      143    0.000    0.000    0.003    0.000 frame.py:3093(_box_item_values)\n",
       "       93    0.000    0.000    0.000    0.000 latin1prober.py:103(reset)\n",
       "      688    0.000    0.000    0.000    0.000 base.py:4914(_ensure_index)\n",
       "     1022    0.000    0.000    0.000    0.000 threading.py:507(is_set)\n",
       "     1509    0.000    0.000    0.000    0.000 {method 'islower' of 'str' objects}\n",
       "     1125    0.000    0.000    0.001    0.000 internals.py:3309(<genexpr>)\n",
       "       93    0.000    0.000    0.001    0.000 chardistribution.py:152(__init__)\n",
       "       13    0.000    0.000    0.002    0.000 base.py:2886(difference)\n",
       "      273    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:416(parent)\n",
       "      253    0.000    0.000    0.000    0.000 generic.py:377(_get_axis_name)\n",
       "       48    0.000    0.000    0.000    0.000 {built-in method builtins.round}\n",
       "      436    0.000    0.000    0.000    0.000 cookiejar.py:1130(return_ok_port)\n",
       "       59    0.000    0.000    0.002    0.000 saxparser.pxi:542(_handleSaxPI)\n",
       "       13    0.000    0.000    0.000    0.000 arraysetops.py:438(in1d)\n",
       "       48    0.000    0.000    0.007    0.000 internals.py:4869(create_block_manager_from_arrays)\n",
       "      501    0.000    0.000    0.002    0.000 common.py:477(is_interval_dtype)\n",
       "       48    0.000    0.000    0.000    0.000 generic.py:317(_construct_axes_from_arguments)\n",
       "       48    0.000    0.000    0.017    0.000 frame.py:7349(_arrays_to_mgr)\n",
       "      397    0.000    0.000    0.000    0.000 _parser.py:1065(_ampm_valid)\n",
       "       13    0.000    0.000    0.001    0.000 base.py:850(_try_convert_to_int_index)\n",
       "      169    0.000    0.000    0.000    0.000 series.py:365(_set_axis)\n",
       "       81    0.000    0.000    0.004    0.000 wait.py:139(wait_for_read)\n",
       "       26    0.000    0.000    0.001    0.000 base.py:1569(is_unique)\n",
       "       93    0.000    0.000    0.001    0.000 chardistribution.py:218(__init__)\n",
       "       61    0.000    0.000    0.001    0.000 cast.py:971(maybe_cast_to_datetime)\n",
       "      173    0.000    0.000    0.000    0.000 element.py:1137(__ne__)\n",
       "      109    0.000    0.000    0.000    0.000 internals.py:3784(_consolidate_check)\n",
       "      204    0.000    0.000    0.000    0.000 base.py:2067(__getitem__)\n",
       "       59    0.000    0.000    0.001    0.000 _lxml.py:214(pi)\n",
       "       13    0.000    0.000    0.000    0.000 {method 'get_labels' of 'pandas._libs.hashtable.StringHashTable' objects}\n",
       "       93    0.000    0.000    0.000    0.000 chardistribution.py:171(__init__)\n",
       "       48    0.000    0.000    0.026    0.001 frame.py:905(from_dict)\n",
       "      397    0.000    0.000    0.000    0.000 _parser.py:357(utczone)\n",
       "       93    0.000    0.000    0.000    0.000 chardistribution.py:114(__init__)\n",
       "      365    0.000    0.000    0.029    0.000 element.py:1153(__unicode__)\n",
       "      130    0.000    0.000    0.000    0.000 sre_compile.py:249(_compile_charset)\n",
       "       13    0.000    0.000    0.001    0.000 sorting.py:20(get_group_index)\n",
       "      187    0.000    0.000    0.001    0.000 common.py:1578(is_bool_dtype)\n",
       "      108    0.000    0.000    0.000    0.000 eucjpprober.py:89(get_confidence)\n",
       "      182    0.000    0.000    0.000    0.000 series.py:401(name)\n",
       "      161    0.000    0.000    0.001    0.000 {method 'any' of 'numpy.ndarray' objects}\n",
       "        3    0.000    0.000    0.000    0.000 {method 'read' of '_io.FileIO' objects}\n",
       "      193    0.000    0.000    0.000    0.000 sjisprober.py:48(charset_name)\n",
       "      361    0.000    0.000    0.001    0.000 numeric.py:433(asarray)\n",
       "      109    0.000    0.000    0.001    0.000 dtypes.py:675(construct_from_string)\n",
       "      195    0.000    0.000    0.000    0.000 generic.py:4362(__getattr__)\n",
       "       69    0.000    0.000    0.000    0.000 pyopenssl.py:261(fileno)\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method marshal.loads}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method nt.listdir}\n",
       "      286    0.000    0.000    0.000    0.000 base.py:3071(get_loc)\n",
       "       87    0.000    0.000    0.002    0.000 series.py:4036(_try_cast)\n",
       "      218    0.000    0.000    0.000    0.000 {method 'fill' of 'numpy.ndarray' objects}\n",
       "      655    0.000    0.000    0.000    0.000 sre_parse.py:172(append)\n",
       "      186    0.000    0.000    0.000    0.000 hebrewprober.py:164(reset)\n",
       "       13    0.000    0.000    0.001    0.000 algorithms.py:449(_factorize_array)\n",
       "     1657    0.000    0.000    0.000    0.000 element.py:1173(_should_pretty_print)\n",
       "       96    0.000    0.000    0.000    0.000 __init__.py:1605(getEffectiveLevel)\n",
       "       48    0.000    0.000    0.001    0.000 generic.py:4563(values)\n",
       "      274    0.000    0.000    0.001    0.000 common.py:227(is_datetimetz)\n",
       "      334    0.000    0.000    0.000    0.000 base.py:672(values)\n",
       "       13    0.000    0.000    0.000    0.000 {method 'get_indexer' of 'pandas._libs.index.IndexEngine' objects}\n",
       "      135    0.000    0.000    0.000    0.000 dtypes.py:266(construct_from_string)\n",
       "       13    0.000    0.000    0.019    0.001 frame.py:4309(drop_duplicates)\n",
       "       13    0.000    0.000    0.003    0.000 internals.py:4518(take)\n",
       "      143    0.000    0.000    0.000    0.000 missing.py:112(_isna_new)\n",
       "       13    0.000    0.000    0.001    0.000 algorithms.py:1548(take_nd)\n",
       "      182    0.000    0.000    0.000    0.000 series.py:405(name)\n",
       "        4    0.000    0.000    3.019    0.755 socket.py:691(create_connection)\n",
       "      161    0.000    0.000    0.000    0.000 __init__.py:205(iteritems)\n",
       "      556    0.000    0.000    0.000    0.000 sre_parse.py:249(match)\n",
       "      174    0.000    0.000    0.000    0.000 common.py:858(is_signed_integer_dtype)\n",
       "       13    0.000    0.000    0.000    0.000 indexing.py:2441(maybe_convert_indices)\n",
       "      397    0.000    0.000    0.000    0.000 _parser.py:1093(_adjust_ampm)\n",
       "      134    0.000    0.000    0.000    0.000 __init__.py:411(FixedOffset)\n",
       "      330    0.000    0.000    0.001    0.000 common.py:122(is_sparse)\n",
       "      156    0.000    0.000    0.007    0.000 frame.py:713(iteritems)\n",
       "       13    0.000    0.000    0.000    0.000 {pandas._libs.algos.take_2d_axis0_object_object}\n",
       "       48    0.000    0.000    0.003    0.000 internals.py:4846(create_block_manager_from_blocks)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'bind' of '_socket.socket' objects}\n",
       "       52    0.000    0.000    0.000    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
       "      148    0.000    0.000    0.000    0.000 common.py:407(is_timedelta64_dtype)\n",
       "       13    0.000    0.000    0.000    0.000 internals.py:5017(_stack_arrays)\n",
       "       39    0.000    0.000    0.000    0.000 base.py:510(_shallow_copy)\n",
       "        1    0.000    0.000    2.019    2.019 remote_connection.py:106(__init__)\n",
       "       13    0.000    0.000    0.000    0.000 {pandas._libs.hashtable.duplicated_int64}\n",
       "      156    0.000    0.000    0.001    0.000 internals.py:269(make_block_same_class)\n",
       "       93    0.000    0.000    0.000    0.000 hebrewprober.py:286(state)\n",
       "        4    0.000    0.000    0.000    0.000 shlex.py:129(read_token)\n",
       "      347    0.000    0.000    0.000    0.000 {method 'view' of 'numpy.ndarray' objects}\n",
       "       48    0.000    0.000    0.000    0.000 internals.py:3922(as_array)\n",
       "      135    0.000    0.000    0.001    0.000 common.py:1043(is_datetime64_any_dtype)\n",
       "      408    0.000    0.000    0.000    0.000 internals.py:3384(_get_items)\n",
       "      220    0.000    0.000    0.000    0.000 jpcntx.py:173(get_confidence)\n",
       "      109    0.000    0.000    0.000    0.000 internals.py:3266(<listcomp>)\n",
       "       88    0.000    0.000    0.000    0.000 sre_parse.py:343(_escape)\n",
       "       48    0.000    0.000    0.000    0.000 generic.py:1524(empty)\n",
       "      403    0.000    0.000    0.000    0.000 sre_parse.py:160(__len__)\n",
       "       96    0.000    0.000    0.000    0.000 frame.py:555(shape)\n",
       "       13    0.000    0.000    0.002    0.000 algorithms.py:576(factorize)\n",
       "       48    0.000    0.000    0.000    0.000 generic.py:303(_construct_axes_dict_from)\n",
       "      143    0.000    0.000    0.000    0.000 generic.py:2498(_set_as_cached)\n",
       "      365    0.000    0.000    0.000    0.000 element.py:217(_is_xml)\n",
       "       26    0.000    0.000    0.000    0.000 numeric.py:35(__new__)\n",
       "       26    0.000    0.000    0.001    0.000 algorithms.py:48(_ensure_data)\n",
       "       39    0.000    0.000    0.000    0.000 dtypes.py:459(construct_from_string)\n",
       "      157    0.000    0.000    0.000    0.000 generic.py:390(_get_axis)\n",
       "       17    0.000    0.000    0.010    0.001 sre_compile.py:759(compile)\n",
       "       61    0.000    0.000    0.000    0.000 base.py:2445(equals)\n",
       "      260    0.000    0.000    0.000    0.000 sre_parse.py:111(__init__)\n",
       "       87    0.000    0.000    0.000    0.000 cast.py:853(maybe_castable)\n",
       "       17    0.000    0.000    0.000    0.000 sre_compile.py:536(_compile_info)\n",
       "      257    0.000    0.000    0.000    0.000 {method 'find' of 'bytearray' objects}\n",
       "       13    0.000    0.000    0.000    0.000 numeric.py:630(require)\n",
       "      161    0.000    0.000    0.001    0.000 _methods.py:42(_any)\n",
       "      161    0.000    0.000    0.000    0.000 common.py:1527(is_float_dtype)\n",
       "      109    0.000    0.000    0.000    0.000 inference.py:119(is_iterator)\n",
       "      109    0.000    0.000    0.000    0.000 generic.py:1571(<genexpr>)\n",
       "       96    0.000    0.000    0.000    0.000 chardistribution.py:100(got_enough_data)\n",
       "     19/2    0.000    0.000    0.000    0.000 webdriver.py:267(_wrap_value)\n",
       "      143    0.000    0.000    0.000    0.000 internals.py:372(iget)\n",
       "      174    0.000    0.000    0.000    0.000 common.py:907(is_unsigned_integer_dtype)\n",
       "       13    0.000    0.000    0.001    0.000 base.py:3219(get_indexer)\n",
       "       52    0.000    0.000    0.000    0.000 {method 'take' of 'numpy.ndarray' objects}\n",
       "        5    0.000    0.000    0.003    0.001 <frozen importlib._bootstrap_external>:1356(find_spec)\n",
       "      143    0.000    0.000    0.000    0.000 concat.py:105(_get_sliced_frame_result_type)\n",
       "      145    0.000    0.000    0.000    0.000 <ipython-input-13-8fa8ea28c6a0>:50(<listcomp>)\n",
       "      144    0.000    0.000    0.000    0.000 generic.py:675(<genexpr>)\n",
       "      451    0.000    0.000    0.000    0.000 {built-in method _sre.unicode_iscased}\n",
       "       48    0.000    0.000    0.000    0.000 frame.py:7419(_prep_ndarray)\n",
       "      208    0.000    0.000    0.000    0.000 internals.py:127(_check_ndim)\n",
       "       59    0.000    0.000    0.001    0.000 parsertarget.pxi:93(_handleSaxPi)\n",
       "      143    0.000    0.000    0.000    0.000 {built-in method pandas._libs.missing.checknull}\n",
       "      169    0.000    0.000    0.000    0.000 series.py:391(_set_subtyp)\n",
       "       48    0.000    0.000    0.000    0.000 generic.py:684(<listcomp>)\n",
       "       13    0.000    0.000    0.000    0.000 sorting.py:55(maybe_lift)\n",
       "        6    0.000    0.000    0.000    0.000 {pandas._libs.tslibs.conversion.datetime_to_datetime64}\n",
       "       61    0.000    0.000    0.000    0.000 common.py:1629(is_extension_type)\n",
       "       96    0.000    0.000    0.000    0.000 generic.py:364(_get_axis_number)\n",
       "    30/17    0.000    0.000    0.000    0.000 sre_compile.py:461(_get_literal_prefix)\n",
       "      148    0.000    0.000    0.000    0.000 common.py:195(is_categorical)\n",
       "       13    0.000    0.000    0.000    0.000 element.py:477(find_all_next)\n",
       "       48    0.000    0.000    0.000    0.000 common.py:191(_dict_keys_to_ordered_list)\n",
       "        2    0.000    0.000    0.000    0.000 enum.py:869(<listcomp>)\n",
       "       13    0.000    0.000    0.000    0.000 cast.py:257(maybe_promote)\n",
       "      122    0.000    0.000    0.000    0.000 internals.py:4101(_consolidate_inplace)\n",
       "       17    0.000    0.000    0.006    0.000 sre_parse.py:919(parse)\n",
       "      197    0.000    0.000    0.000    0.000 sre_parse.py:286(tell)\n",
       "       61    0.000    0.000    0.000    0.000 generic.py:4345(__finalize__)\n",
       "       26    0.000    0.000    0.007    0.000 frame.py:4383(<genexpr>)\n",
       "      156    0.000    0.000    0.000    0.000 inference.py:415(is_hashable)\n",
       "      140    0.000    0.000    0.000    0.000 message.py:606(get_default_type)\n",
       "      193    0.000    0.000    0.000    0.000 jpcntx.py:188(charset_name)\n",
       "       39    0.000    0.000    0.000    0.000 dtypes.py:401(__new__)\n",
       "       48    0.000    0.000    0.000    0.000 function.py:267(validate_transpose_for_generic)\n",
       "       96    0.000    0.000    0.000    0.000 {method 'transpose' of 'numpy.ndarray' objects}\n",
       "       26    0.000    0.000    0.000    0.000 base.py:1935(_engine)\n",
       "       48    0.000    0.000    0.000    0.000 generic.py:4436(f)\n",
       "     13/1    0.000    0.000    0.000    0.000 copy.py:132(deepcopy)\n",
       "      143    0.000    0.000    0.000    0.000 missing.py:32(isna)\n",
       "        3    0.000    0.000    0.006    0.002 <frozen importlib._bootstrap>:978(_find_and_load)\n",
       "        2    0.000    0.000    0.000    0.000 encoder.py:204(iterencode)\n",
       "       13    0.000    0.000    0.000    0.000 base.py:1364(set_names)\n",
       "       60    0.000    0.000    0.000    0.000 _collections_abc.py:760(__iter__)\n",
       "       48    0.000    0.000    0.000    0.000 frame.py:478(_get_axes)\n",
       "      130    0.000    0.000    0.000    0.000 charsetgroupprober.py:49(charset_name)\n",
       "       48    0.000    0.000    0.000    0.000 generic.py:4433(_consolidate_inplace)\n",
       "       74    0.000    0.000    0.000    0.000 {built-in method pandas._libs.lib.infer_datetimelike_array}\n",
       "      144    0.000    0.000    0.000    0.000 generic.py:677(<genexpr>)\n",
       "       52    0.000    0.000    0.000    0.000 {method 'ravel' of 'numpy.ndarray' objects}\n",
       "       10    0.000    0.000    0.000    0.000 {pandas._libs.lib.fast_unique_multiple_list}\n",
       "       94    0.000    0.000    0.000    0.000 __init__.py:381(utcoffset)\n",
       "      325    0.000    0.000    0.000    0.000 internals.py:233(mgr_locs)\n",
       "       93    0.000    0.000    0.000    0.000 hebrewprober.py:174(set_model_probers)\n",
       "       13    0.000    0.000    0.001    0.000 internals.py:1237(take_nd)\n",
       "       13    0.000    0.000    0.001    0.000 base.py:1283(astype)\n",
       "       34    0.000    0.000    0.000    0.000 sre_parse.py:84(opengroup)\n",
       "        3    0.000    0.000    0.002    0.001 <frozen importlib._bootstrap_external>:793(get_code)\n",
       "      170    0.000    0.000    0.000    0.000 internals.py:3776(is_consolidated)\n",
       "       61    0.000    0.000    0.000    0.000 <ipython-input-13-8fa8ea28c6a0>:104(<dictcomp>)\n",
       "       13    0.000    0.000    0.001    0.000 algorithms.py:141(_reconstruct_data)\n",
       "       48    0.000    0.000    0.000    0.000 generic.py:4423(_protect_consolidate)\n",
       "       48    0.000    0.000    0.000    0.000 generic.py:349(<dictcomp>)\n",
       "       65    0.000    0.000    0.000    0.000 common.py:692(is_dtype_equal)\n",
       "       53    0.000    0.000    0.000    0.000 sre_parse.py:408(_uniq)\n",
       "       52    0.000    0.000    0.000    0.000 dtypes.py:584(is_dtype)\n",
       "       13    0.000    0.000    0.000    0.000 fromnumeric.py:64(_wrapreduction)\n",
       "       52    0.000    0.000    0.000    0.000 base.py:920(_get_attributes_dict)\n",
       "        2    0.000    0.000    0.000    0.000 shlex.py:21(__init__)\n",
       "       17    0.000    0.000    0.000    0.000 sre_compile.py:492(_get_charset_prefix)\n",
       "      284    0.000    0.000    0.000    0.000 utf8prober.py:49(charset_name)\n",
       "       13    0.000    0.000    0.000    0.000 generic.py:3171(_update_inplace)\n",
       "       28    0.000    0.000    0.000    0.000 re.py:307(_subx)\n",
       "       17    0.000    0.000    0.000    0.000 {built-in method _sre.compile}\n",
       "      149    0.000    0.000    0.000    0.000 sjisprober.py:52(language)\n",
       "      405    0.000    0.000    0.000    0.000 {built-in method _sre.unicode_tolower}\n",
       "       13    0.000    0.000    0.002    0.000 internals.py:4388(reindex_indexer)\n",
       "       39    0.000    0.000    0.000    0.000 internals.py:356(ftype)\n",
       "        2    0.000    0.000    2.026    1.013 remote_connection.py:376(_request)\n",
       "       61    0.000    0.000    0.000    0.000 base.py:615(is_)\n",
       "       13    0.000    0.000    0.001    0.000 base.py:973(copy)\n",
       "       13    0.000    0.000    0.002    0.000 frame.py:4364(f)\n",
       "      109    0.000    0.000    0.000    0.000 internals.py:3785(<listcomp>)\n",
       "       13    0.000    0.000    0.002    0.000 _decorators.py:136(wrapper)\n",
       "       13    0.000    0.000    0.001    0.000 generic.py:1117(__neg__)\n",
       "       78    0.000    0.000    0.000    0.000 chardistribution.py:158(get_order)\n",
       "       53    0.000    0.000    0.000    0.000 types.py:164(__get__)\n",
       "        2    0.000    0.000    2.027    1.013 remote_connection.py:355(execute)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _winapi.CreatePipe}\n",
       "        2    0.000    0.000    2.026    1.013 poolmanager.py:302(urlopen)\n",
       "       13    0.000    0.000    0.001    0.000 base.py:520(_shallow_copy_with_infer)\n",
       "       13    0.000    0.000    0.001    0.000 series.py:643(__array_wrap__)\n",
       "        4    0.000    0.000    3.019    0.755 utils.py:97(is_connectable)\n",
       "       61    0.000    0.000    0.000    0.000 internals.py:5020(_asarray_compat)\n",
       "       17    0.000    0.000    0.000    0.000 enum.py:815(__and__)\n",
       "      148    0.000    0.000    0.000    0.000 internals.py:3490(<genexpr>)\n",
       "        2    0.000    0.000    0.005    0.002 __init__.py:71(search_function)\n",
       "       13    0.000    0.000    0.000    0.000 base.py:1332(_set_names)\n",
       "      130    0.000    0.000    0.000    0.000 base.py:677(_values)\n",
       "       13    0.000    0.000    0.000    0.000 sorting.py:389(safe_sort)\n",
       "        6    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:271(cache_from_source)\n",
       "        1    0.000    0.000    0.033    0.033 subprocess.py:650(__init__)\n",
       "       13    0.000    0.000    0.000    0.000 arraysetops.py:675(setdiff1d)\n",
       "       13    0.000    0.000    0.000    0.000 internals.py:3351(_is_single_block)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1404(_fill_cache)\n",
       "       51    0.000    0.000    0.000    0.000 chardistribution.py:177(get_order)\n",
       "       13    0.000    0.000    0.000    0.000 {method 'to_array' of 'pandas._libs.hashtable.ObjectVector' objects}\n",
       "       17    0.000    0.000    0.000    0.000 sre_parse.py:224(__init__)\n",
       "      220    0.000    0.000    0.000    0.000 utf8prober.py:53(language)\n",
       "        1    0.000    0.000    0.032    0.032 subprocess.py:1101(_execute_child)\n",
       "        1    0.000    0.000    0.001    0.001 linecache.py:82(updatecache)\n",
       "        3    0.000    0.000    0.003    0.001 <frozen importlib._bootstrap>:882(_find_spec)\n",
       "       33    0.000    0.000    0.000    0.000 sre_compile.py:423(_simple)\n",
       "       13    0.000    0.000    0.000    0.000 base.py:996(_validate_names)\n",
       "      195    0.000    0.000    0.000    0.000 eucjpprober.py:48(charset_name)\n",
       "      195    0.000    0.000    0.000    0.000 euckrprober.py:41(charset_name)\n",
       "       34    0.000    0.000    0.000    0.000 sre_parse.py:295(_class_escape)\n",
       "      196    0.000    0.000    0.000    0.000 euctwprober.py:40(charset_name)\n",
       "       13    0.000    0.000    0.001    0.000 api.py:80(_union_indexes)\n",
       "       13    0.000    0.000    0.000    0.000 base.py:893(tolist)\n",
       "        2    0.000    0.000    0.000    0.000 enum.py:779(_create_pseudo_member_)\n",
       "       13    0.000    0.000    0.000    0.000 algorithms.py:1421(_get_take_nd_function)\n",
       "       52    0.000    0.000    0.000    0.000 common.py:444(is_period_dtype)\n",
       "       39    0.000    0.000    0.000    0.000 {pandas._libs.lib.values_from_object}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'listen' of '_socket.socket' objects}\n",
       "       31    0.000    0.000    0.000    0.000 sre_parse.py:267(getuntil)\n",
       "       39    0.000    0.000    0.000    0.000 base.py:789(_ndarray_values)\n",
       "       13    0.000    0.000    0.001    0.000 base.py:2179(take)\n",
       "       48    0.000    0.000    0.000    0.000 generic.py:306(<dictcomp>)\n",
       "       17    0.000    0.000    0.000    0.000 sre_parse.py:903(fix_flags)\n",
       "       13    0.000    0.000    0.001    0.000 internals.py:4972(_simple_blockify)\n",
       "       26    0.000    0.000    0.000    0.000 {method 'nonzero' of 'numpy.ndarray' objects}\n",
       "       13    0.000    0.000    0.000    0.000 base.py:3389(_maybe_promote)\n",
       "        3    0.000    0.000    0.003    0.001 <frozen importlib._bootstrap>:663(_load_unlocked)\n",
       "        2    0.000    0.000    0.000    0.000 enum.py:851(_decompose)\n",
       "       10    0.000    0.000    0.001    0.000 api.py:91(_unique_indices)\n",
       "       96    0.000    0.000    0.000    0.000 function.py:38(__call__)\n",
       "      282    0.000    0.000    0.000    0.000 hebrewprober.py:282(language)\n",
       "       17    0.000    0.000    0.004    0.000 sre_compile.py:598(_code)\n",
       "       13    0.000    0.000    0.000    0.000 internals.py:4742(external_values)\n",
       "      102    0.000    0.000    0.000    0.000 {built-in method builtins.id}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method nt.open}\n",
       "        3    0.000    0.000    0.000    0.000 __init__.py:1272(_fixupParents)\n",
       "       52    0.000    0.000    0.000    0.000 base.py:922(<dictcomp>)\n",
       "       13    0.000    0.000    0.001    0.000 internals.py:4423(<listcomp>)\n",
       "       39    0.000    0.000    0.000    0.000 dammit.py:95(_substitute_xml_entity)\n",
       "       25    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:58(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 subprocess.py:485(list2cmdline)\n",
       "      159    0.000    0.000    0.000    0.000 gb2312prober.py:40(charset_name)\n",
       "        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:157(_get_module_lock)\n",
       "        2    0.000    0.000    0.000    0.000 shlex.py:300(split)\n",
       "       13    0.000    0.000    0.001    0.000 numeric.py:64(_shallow_copy)\n",
       "       25    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:56(_path_join)\n",
       "       13    0.000    0.000    0.000    0.000 {built-in method _operator.inv}\n",
       "       13    0.000    0.000    0.000    0.000 algorithms.py:224(_get_data_algo)\n",
       "      155    0.000    0.000    0.000    0.000 gb2312prober.py:44(language)\n",
       "        6    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:62(_path_split)\n",
       "       13    0.000    0.000    0.000    0.000 sorting.py:47(_int64_cut_off)\n",
       "       65    0.000    0.000    0.000    0.000 charsetgroupprober.py:57(language)\n",
       "      167    0.000    0.000    0.000    0.000 big5prober.py:41(charset_name)\n",
       "        1    0.000    0.000    1.653    1.653 webdriver.py:113(__init__)\n",
       "      102    0.000    0.000    0.000    0.000 sre_parse.py:81(groups)\n",
       "      147    0.000    0.000    0.000    0.000 eucjpprober.py:52(language)\n",
       "        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:504(_init_module_attrs)\n",
       "       60    0.000    0.000    0.000    0.000 sre_compile.py:65(_combine_flags)\n",
       "       13    0.000    0.000    0.000    0.000 fromnumeric.py:2478(prod)\n",
       "      169    0.000    0.000    0.000    0.000 numeric.py:110(is_all_dates)\n",
       "        1    0.000    0.000    4.707    4.707 webdriver.py:33(__init__)\n",
       "       44    0.000    0.000    0.000    0.000 chardistribution.py:139(get_order)\n",
       "      167    0.000    0.000    0.000    0.000 cp949prober.py:43(charset_name)\n",
       "       13    0.000    0.000    0.000    0.000 {method 'all' of 'numpy.ndarray' objects}\n",
       "       78    0.000    0.000    0.000    0.000 base.py:444(<genexpr>)\n",
       "       48    0.000    0.000    0.000    0.000 frame.py:457(<listcomp>)\n",
       "       48    0.000    0.000    0.000    0.000 internals.py:4085(consolidate)\n",
       "        1    0.000    0.000    0.000    0.000 utils.py:31(free_port)\n",
       "        1    0.000    0.000    0.000    0.000 subprocess.py:1007(_get_handles)\n",
       "        1    0.000    0.000    1.034    1.034 service.py:61(start)\n",
       "       61    0.000    0.000    0.000    0.000 cast.py:1232(construct_1d_ndarray_preserving_na)\n",
       "        8    0.000    0.000    0.000    0.000 sre_compile.py:413(<listcomp>)\n",
       "        3    0.000    0.000    0.006    0.002 <frozen importlib._bootstrap>:948(_find_and_load_unlocked)\n",
       "      2/1    0.000    0.000    0.000    0.000 copy.py:236(_deepcopy_dict)\n",
       "       65    0.000    0.000    0.000    0.000 internals.py:3311(ndim)\n",
       "       13    0.000    0.000    0.000    0.000 series.py:476(get_values)\n",
       "       13    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.copyto}\n",
       "       10    0.000    0.000    0.000    0.000 _parser.py:1042(_assign_hms)\n",
       "       47    0.000    0.000    0.000    0.000 sre_compile.py:453(_get_iscased)\n",
       "        4    0.000    0.000    0.000    0.000 _strptime.py:247(pattern)\n",
       "       13    0.000    0.000    0.000    0.000 frozen.py:38(__getitem__)\n",
       "        2    0.000    0.000    0.000    0.000 __init__.py:43(normalize_encoding)\n",
       "        2    0.000    0.000    0.001    0.000 linecache.py:15(getline)\n",
       "        2    0.000    0.000    0.000    0.000 remote_connection.py:73(get_remote_connection_headers)\n",
       "      147    0.000    0.000    0.000    0.000 euckrprober.py:45(language)\n",
       "       44    0.000    0.000    0.010    0.000 re.py:232(compile)\n",
       "       13    0.000    0.000    0.000    0.000 common.py:647(is_datetimelike)\n",
       "       26    0.000    0.000    0.000    0.000 element.py:625(next_elements)\n",
       "        5    0.000    0.000    0.000    0.000 element.py:1488(classes_match)\n",
       "       26    0.000    0.000    0.000    0.000 base.py:1938(<lambda>)\n",
       "        2    0.000    0.000    2.027    1.013 webdriver.py:301(execute)\n",
       "        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:574(spec_from_file_location)\n",
       "        3    0.000    0.000    0.003    0.001 <frozen importlib._bootstrap_external>:1240(_get_spec)\n",
       "        3    0.000    0.000    0.000    0.000 __init__.py:1341(__init__)\n",
       "       13    0.000    0.000    0.000    0.000 missing.py:376(array_equivalent)\n",
       "       26    0.000    0.000    0.000    0.000 base.py:766(size)\n",
       "       13    0.000    0.000    0.000    0.000 element.py:471(find_next)\n",
       "       10    0.000    0.000    0.000    0.000 api.py:129(_sanitize_and_check)\n",
       "        2    0.000    0.000    0.000    0.000 decoder.py:343(raw_decode)\n",
       "        2    0.000    0.000    0.000    0.000 utils.py:32(dump_json)\n",
       "       34    0.000    0.000    0.001    0.000 sre_parse.py:96(closegroup)\n",
       "        1    0.000    0.000    0.000    0.000 __init__.py:375(__init__)\n",
       "       13    0.000    0.000    0.000    0.000 base.py:2997(_get_unique_index)\n",
       "       13    0.000    0.000    0.000    0.000 internals.py:4752(get_values)\n",
       "        2    0.000    0.000    0.000    0.000 string.py:107(substitute)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'translate' of 'str' objects}\n",
       "        1    0.000    0.000    0.000    0.000 webdriver.py:63(_make_w3c_caps)\n",
       "        2    0.000    0.000    0.004    0.002 {built-in method builtins.__import__}\n",
       "        8    0.000    0.000    0.000    0.000 sre_compile.py:411(_mk_bitmap)\n",
       "       34    0.000    0.000    0.000    0.000 sre_compile.py:595(isstring)\n",
       "       13    0.000    0.000    0.000    0.000 generic.py:2543(_maybe_update_cacher)\n",
       "        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:78(acquire)\n",
       "       28    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:222(_verbose_message)\n",
       "       13    0.000    0.000    0.000    0.000 enum.py:834(_high_bit)\n",
       "     27/2    0.000    0.000    0.000    0.000 webdriver.py:284(_unwrap_value)\n",
       "        4    0.000    0.000    0.002    0.000 _strptime.py:270(compile)\n",
       "        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:523(_compile_bytecode)\n",
       "       17    0.000    0.000    0.000    0.000 sre_parse.py:76(__init__)\n",
       "       13    0.000    0.000    0.000    0.000 numeric.py:156(ones)\n",
       "       13    0.000    0.000    0.000    0.000 generic.py:2577(_clear_item_cache)\n",
       "      147    0.000    0.000    0.000    0.000 cp949prober.py:47(language)\n",
       "       40    0.000    0.000    0.000    0.000 {method 'isalnum' of 'str' objects}\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method _winapi.DuplicateHandle}\n",
       "       11    0.000    0.000    0.003    0.000 <frozen importlib._bootstrap_external>:74(_path_stat)\n",
       "        3    0.000    0.000    0.000    0.000 parse.py:165(port)\n",
       "       26    0.000    0.000    0.000    0.000 numeric.py:701(<genexpr>)\n",
       "       13    0.000    0.000    0.000    0.000 element.py:564(_find_one)\n",
       "      146    0.000    0.000    0.000    0.000 euctwprober.py:44(language)\n",
       "        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:438(_classify_pyc)\n",
       "        3    0.000    0.000    0.002    0.001 <frozen importlib._bootstrap_external>:722(exec_module)\n",
       "       39    0.000    0.000    0.000    0.000 sre_parse.py:168(__setitem__)\n",
       "       10    0.000    0.000    0.000    0.000 api.py:98(<listcomp>)\n",
       "        2    0.000    0.000    2.026    1.013 request.py:50(request)\n",
       "       30    0.000    0.000    0.000    0.000 _factories.py:9(__call__)\n",
       "       26    0.000    0.000    0.000    0.000 base.py:1964(inferred_type)\n",
       "        1    0.000    0.000    1.008    1.008 utils.py:43(find_connectable_ip)\n",
       "        1    0.000    0.000    0.000    0.000 charmap.py:11(<module>)\n",
       "        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:103(release)\n",
       "        4    0.000    0.000    0.000    0.000 shlex.py:294(__next__)\n",
       "       26    0.000    0.000    0.000    0.000 base.py:904(_coerce_to_ndarray)\n",
       "       13    0.000    0.000    0.000    0.000 internals.py:4745(internal_values)\n",
       "        2    0.000    0.000    0.001    0.000 warnings.py:35(_formatwarnmsg_impl)\n",
       "        2    0.000    0.000    0.000    0.000 encoder.py:182(encode)\n",
       "       39    0.000    0.000    0.000    0.000 internals.py:348(shape)\n",
       "       52    0.000    0.000    0.000    0.000 internals.py:352(dtype)\n",
       "      147    0.000    0.000    0.000    0.000 big5prober.py:45(language)\n",
       "        9    0.000    0.000    0.000    0.000 {built-in method _codecs.utf_8_decode}\n",
       "        4    0.000    0.000    0.000    0.000 shlex.py:97(get_token)\n",
       "        8    0.000    0.000    0.000    0.000 _collections_abc.py:72(_check_methods)\n",
       "       13    0.000    0.000    0.000    0.000 series.py:516(nonzero)\n",
       "       13    0.000    0.000    0.000    0.000 {method 'argsort' of 'numpy.ndarray' objects}\n",
       "        1    0.000    0.000    0.000    0.000 tokenize.py:443(open)\n",
       "       26    0.000    0.000    0.000    0.000 base.py:711(get_values)\n",
       "       94    0.000    0.000    0.000    0.000 charsetprober.py:58(get_confidence)\n",
       "        3    0.000    0.000    0.000    0.000 subprocess.py:1079(_make_inheritable)\n",
       "       48    0.000    0.000    0.000    0.000 frame.py:320(_constructor)\n",
       "       13    0.000    0.000    0.000    0.000 series.py:432(values)\n",
       "        4    0.000    0.000    0.000    0.000 copy.py:252(_keep_alive)\n",
       "       13    0.000    0.000    0.000    0.000 missing.py:596(clean_reindex_fill_method)\n",
       "        1    0.000    0.000    1.653    1.653 webdriver.py:231(start_session)\n",
       "       58    0.000    0.000    0.000    0.000 api.py:92(conv)\n",
       "       10    0.000    0.000    0.000    0.000 api.py:130(<setcomp>)\n",
       "       13    0.000    0.000    0.000    0.000 base.py:912(__iter__)\n",
       "        1    0.000    0.000    0.000    0.000 options.py:29(__init__)\n",
       "        5    0.000    0.000    0.000    0.000 {built-in method _winapi.CloseHandle}\n",
       "       13    0.000    0.000    0.000    0.000 base.py:1329(_get_names)\n",
       "       13    0.000    0.000    0.000    0.000 base.py:1578(is_boolean)\n",
       "        1    0.000    0.000    0.000    0.000 punycode.py:4(<module>)\n",
       "       39    0.000    0.000    0.000    0.000 {built-in method pandas._libs.algos.ensure_platform_int}\n",
       "       39    0.000    0.000    0.000    0.000 internals.py:4684(_block)\n",
       "        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:58(__init__)\n",
       "       13    0.000    0.000    0.000    0.000 _validators.py:221(validate_bool_kwarg)\n",
       "       21    0.000    0.000    0.000    0.000 chardistribution.py:199(get_order)\n",
       "        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:318(__exit__)\n",
       "        1    0.000    0.000    0.000    0.000 options.py:194(to_capabilities)\n",
       "        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:576(module_from_spec)\n",
       "        2    0.000    0.000    0.000    0.000 codecs.py:94(__new__)\n",
       "       18    0.000    0.000    0.000    0.000 enum.py:607(value)\n",
       "        2    0.000    0.000    0.000    0.000 utils.py:81(join_host_port)\n",
       "        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:369(_get_cached)\n",
       "       35    0.000    0.000    0.000    0.000 enum.py:602(name)\n",
       "        9    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:51(_r_long)\n",
       "       13    0.000    0.000    0.000    0.000 _methods.py:45(_all)\n",
       "       13    0.000    0.000    0.000    0.000 base.py:1307(_convert_can_do_setop)\n",
       "        1    0.000    0.000    0.000    0.000 service.py:37(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 platform.py:1059(system)\n",
       "       13    0.000    0.000    0.000    0.000 internals.py:213(get_values)\n",
       "       13    0.000    0.000    0.000    0.000 series.py:465(_values)\n",
       "        1    0.000    0.000    0.000    0.000 charmap.py:60(getregentry)\n",
       "        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1351(_get_spec)\n",
       "       13    0.000    0.000    0.000    0.000 inference.py:287(is_array_like)\n",
       "       13    0.000    0.000    0.000    0.000 internals.py:3473(__len__)\n",
       "        2    0.000    0.000    0.000    0.000 subprocess.py:859(_get_devnull)\n",
       "       26    0.000    0.000    0.000    0.000 base.py:662(dtype)\n",
       "        8    0.000    0.000    0.000    0.000 subprocess.py:202(Close)\n",
       "       13    0.000    0.000    0.000    0.000 base.py:1302(_assert_can_do_setop)\n",
       "       13    0.000    0.000    0.000    0.000 internals.py:222(to_dense)\n",
       "        9    0.000    0.000    0.000    0.000 codecs.py:319(decode)\n",
       "       13    0.000    0.000    0.000    0.000 function_base.py:241(iterable)\n",
       "        1    0.000    0.000    1.653    1.653 request.py:91(request_encode_body)\n",
       "        6    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:403(cached)\n",
       "        9    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:859(__exit__)\n",
       "        3    0.000    0.000    0.002    0.001 <frozen importlib._bootstrap_external>:84(_path_is_mode_type)\n",
       "       13    0.000    0.000    0.000    0.000 {method 'tolist' of 'numpy.ndarray' objects}\n",
       "       13    0.000    0.000    0.000    0.000 base.py:104(_reset_cache)\n",
       "        5    0.000    0.000    0.000    0.000 {method 'issubset' of 'set' objects}\n",
       "        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:147(__enter__)\n",
       "        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:176(cb)\n",
       "        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:471(_validate_timestamp_pyc)\n",
       "       14    0.000    0.000    0.000    0.000 enum.py:886(_power_of_two)\n",
       "        2    0.000    0.000    0.001    0.000 linecache.py:37(getlines)\n",
       "       13    0.000    0.000    0.000    0.000 algorithms.py:172(_ensure_arraylike)\n",
       "       56    0.000    0.000    0.000    0.000 latin1prober.py:108(charset_name)\n",
       "        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:884(__init__)\n",
       "       56    0.000    0.000    0.000    0.000 _collections_abc.py:392(__subclasshook__)\n",
       "       13    0.000    0.000    0.000    0.000 frame.py:844(__len__)\n",
       "       13    0.000    0.000    0.000    0.000 internals.py:5026(_shape_compat)\n",
       "        1    0.000    0.000    0.000    0.000 punycode.py:228(getregentry)\n",
       "        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:35(_new_module)\n",
       "        6    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1203(_path_importer_cache)\n",
       "        2    0.000    0.000    0.000    0.000 __init__.py:183(dumps)\n",
       "       13    0.000    0.000    0.000    0.000 base.py:803(empty)\n",
       "       13    0.000    0.000    0.000    0.000 {built-in method pandas._libs.algos.ensure_int64}\n",
       "        6    0.000    0.000    0.000    0.000 {method 'rsplit' of 'str' objects}\n",
       "        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:369(__init__)\n",
       "        3    0.000    0.000    0.001    0.000 <frozen importlib._bootstrap_external>:951(path_stats)\n",
       "        2    0.000    0.000    0.000    0.000 warnings.py:408(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 decoder.py:332(decode)\n",
       "       13    0.000    0.000    0.000    0.000 numeric.py:504(asanyarray)\n",
       "       16    0.000    0.000    0.000    0.000 element.py:1381(children)\n",
       "        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:401(_check_name_wrapper)\n",
       "        2    0.000    0.000    0.001    0.000 warnings.py:20(_showwarnmsg_impl)\n",
       "       10    0.000    0.000    0.000    0.000 _parser.py:1120(_parse_hms)\n",
       "       13    0.000    0.000    0.000    0.000 generic.py:2633(_check_setitem_copy)\n",
       "        9    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:855(__enter__)\n",
       "        3    0.000    0.000    0.003    0.001 <frozen importlib._bootstrap_external>:1272(find_spec)\n",
       "        6    0.000    0.000    0.000    0.000 webdriver.py:276(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 service.py:44(command_line_args)\n",
       "        1    0.000    0.000    0.000    0.000 punkt.py:210(_re_sent_end_chars)\n",
       "       15    0.000    0.000    0.000    0.000 {built-in method _imp.acquire_lock}\n",
       "        2    0.000    0.000    0.000    0.000 copy.py:210(_deepcopy_list)\n",
       "        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:151(__exit__)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1433(<setcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 enum.py:554(_missing_)\n",
       "        2    0.000    0.000    0.000    0.000 enum.py:772(_missing_)\n",
       "        1    0.000    0.000    2.019    2.019 remote_connection.py:23(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 service.py:26(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 netrc.py:1(<module>)\n",
       "        9    0.000    0.000    0.000    0.000 {built-in method from_bytes}\n",
       "        1    0.000    0.000    0.000    0.000 subprocess.py:130(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 tokenize.py:380(find_cookie)\n",
       "        5    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:36(_relax_case)\n",
       "        2    0.000    0.000    0.000    0.000 __init__.py:299(loads)\n",
       "        3    0.000    0.000    0.000    0.000 __init__.py:727(__init__)\n",
       "       15    0.000    0.000    0.000    0.000 {built-in method _imp.release_lock}\n",
       "        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:311(__enter__)\n",
       "        1    0.000    0.000    0.000    0.000 tokenize.py:350(detect_encoding)\n",
       "        1    0.000    0.000    0.000    0.000 <string>:1(<listcomp>)\n",
       "       13    0.000    0.000    0.000    0.000 internals.py:229(fill_value)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method msvcrt.open_osfhandle}\n",
       "        3    0.000    0.000    0.002    0.001 <frozen importlib._bootstrap_external>:93(_path_isfile)\n",
       "        4    0.000    0.000    0.000    0.000 _collections_abc.py:406(__subclasshook__)\n",
       "        9    0.000    0.000    0.000    0.000 copy.py:190(_deepcopy_atomic)\n",
       "        3    0.000    0.000    0.000    0.000 __init__.py:187(_checkLevel)\n",
       "       13    0.000    0.000    0.000    0.000 missing.py:74(clean_fill_method)\n",
       "       13    0.000    0.000    0.000    0.000 internals.py:199(external_values)\n",
       "       28    0.000    0.000    0.000    0.000 latin1prober.py:112(language)\n",
       "        2    0.000    0.000    0.000    0.000 utils.py:36(load_json)\n",
       "        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:143(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 copyreg.py:96(_slotnames)\n",
       "        1    0.000    0.000    0.000    0.000 calendar.py:113(weekday)\n",
       "        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:307(__init__)\n",
       "       13    0.000    0.000    0.000    0.000 internals.py:203(internal_values)\n",
       "        2    0.000    0.000    0.000    0.000 element.py:1600(<lambda>)\n",
       "        2    0.000    0.000    0.000    0.000 errorhandler.py:103(check_response)\n",
       "       12    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:321(<genexpr>)\n",
       "        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:792(find_spec)\n",
       "        2    0.000    0.000    0.000    0.000 url.py:38(request_uri)\n",
       "        8    0.000    0.000    0.000    0.000 {method 'translate' of 'bytearray' objects}\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method _imp.is_frozen}\n",
       "        1    0.000    0.000    0.000    0.000 sre_parse.py:1036(expand_template)\n",
       "        3    0.000    0.000    0.000    0.000 platform.py:921(uname)\n",
       "        1    0.000    0.000    0.002    0.002 <string>:1(<module>)\n",
       "       13    0.000    0.000    0.000    0.000 series.py:349(_constructor)\n",
       "        1    0.000    0.000    0.000    0.000 wait.py:111(_have_working_poll)\n",
       "       13    0.000    0.000    0.000    0.000 {built-in method pandas._libs.lib.is_float}\n",
       "        6    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}\n",
       "        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:719(find_spec)\n",
       "        1    0.000    0.000    0.000    0.000 re.py:252(escape)\n",
       "        1    0.000    0.000    0.000    0.000 subprocess.py:226(_cleanup)\n",
       "        1    0.000    0.000    0.374    0.374 request.py:74(request_encode_url)\n",
       "        1    0.000    0.000    0.000    0.000 webdriver.py:160(create_options)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method nt.close}\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method _imp._fix_co_filename}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method now}\n",
       "        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:211(_call_with_frames_removed)\n",
       "        4    0.000    0.000    0.000    0.000 _collections_abc.py:252(__subclasshook__)\n",
       "        2    0.000    0.000    0.001    0.000 warnings.py:85(_showwarnmsg)\n",
       "        2    0.000    0.000    0.001    0.000 warnings.py:106(_formatwarnmsg)\n",
       "        1    0.000    0.000    0.000    0.000 __init__.py:1169(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 __init__.py:1175(append)\n",
       "        2    0.000    0.000    0.000    0.000 string.py:90(__init__)\n",
       "        1    0.000    0.000    0.374    0.374 webdriver.py:681(close)\n",
       "        1    0.000    0.000    0.000    0.000 wait.py:124(wait_for_socket)\n",
       "        1    0.000    0.000    0.000    0.000 service.py:51(service_url)\n",
       "       13    0.000    0.000    0.000    0.000 {built-in method pandas._libs.lib.is_bool}\n",
       "        6    0.000    0.000    0.000    0.000 {built-in method _winapi.GetCurrentProcess}\n",
       "        1    0.000    0.000    0.000    0.000 subprocess.py:1198(_internal_poll)\n",
       "        1    0.000    0.000    1.001    1.001 service.py:114(is_connectable)\n",
       "        1    0.000    0.000    0.000    0.000 charmap.py:17(Codec)\n",
       "       13    0.000    0.000    0.000    0.000 {function FrozenList.__getitem__ at 0x00000000077D6F28}\n",
       "        8    0.000    0.000    0.000    0.000 {method 'index' of 'str' objects}\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method msvcrt.get_osfhandle}\n",
       "        1    0.000    0.000    0.000    0.000 re.py:313(filter)\n",
       "        1    0.000    0.000    0.000    0.000 subprocess.py:207(Detach)\n",
       "        4    0.000    0.000    0.000    0.000 chardistribution.py:224(get_order)\n",
       "        1    0.000    0.000    0.000    0.000 options.py:101(extensions)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _imp.is_builtin}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _winapi.WaitForSingleObject}\n",
       "        1    0.000    0.000    0.000    0.000 client.py:148(_encode)\n",
       "        1    0.000    0.000    0.000    0.000 calendar.py:120(monthrange)\n",
       "        4    0.000    0.000    0.000    0.000 jpcntx.py:170(got_enough_data)\n",
       "        2    0.000    0.000    0.000    0.000 remote_connection.py:420(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 retry.py:199(from_int)\n",
       "        2    0.000    0.000    0.000    0.000 response.py:211(get_redirect_location)\n",
       "        2    0.000    0.000    0.000    0.000 response.py:540(close)\n",
       "        1    0.000    0.000    0.000    0.000 charmap.py:24(IncrementalEncoder)\n",
       "        1    0.000    0.000    0.000    0.000 charmap.py:32(IncrementalDecoder)\n",
       "        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:719(create_module)\n",
       "        1    0.000    0.000    0.000    0.000 codecs.py:309(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 _collections_abc.py:680(values)\n",
       "        1    0.000    0.000    0.000    0.000 subprocess.py:957(poll)\n",
       "        1    0.000    0.000    0.000    0.000 tokenize.py:339(_get_normal_name)\n",
       "        2    0.000    0.000    0.000    0.000 shlex.py:291(__iter__)\n",
       "        1    0.000    0.000    0.000    0.000 charmap.py:40(StreamWriter)\n",
       "        1    0.000    0.000    0.000    0.000 punycode.py:198(Codec)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _codecs.lookup}\n",
       "       13    0.000    0.000    0.000    0.000 {method 'bit_length' of 'int' objects}\n",
       "        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:909(get_filename)\n",
       "        1    0.000    0.000    0.000    0.000 string.py:121(convert)\n",
       "        1    0.000    0.000    0.000    0.000 xmlerror.pxi:489(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 charmap.py:49(StreamReader)\n",
       "        1    0.000    0.000    0.000    0.000 webdriver.py:217(start_client)\n",
       "        1    0.000    0.000    0.000    0.000 webdriver.py:1188(file_detector)\n",
       "        1    0.000    0.000    0.000    0.000 switch_to.py:30(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 service.py:106(assert_process_still_running)\n",
       "        1    0.000    0.000    0.000    0.000 options.py:148(experimental_options)\n",
       "        1    0.000    0.000    0.000    0.000 netrc.py:22(netrc)\n",
       "        1    0.000    0.000    0.000    0.000 punycode.py:210(IncrementalEncoder)\n",
       "        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:424(has_location)\n",
       "        1    0.000    0.000    0.000    0.000 codecs.py:260(__init__)\n",
       "        4    0.000    0.000    0.000    0.000 enum.py:880(<lambda>)\n",
       "        1    0.000    0.000    0.000    0.000 tokenize.py:374(read_or_stop)\n",
       "        2    0.000    0.000    0.000    0.000 response.py:231(data)\n",
       "        1    0.000    0.000    0.000    0.000 mobile.py:45(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 options.py:38(binary_location)\n",
       "        1    0.000    0.000    0.000    0.000 options.py:63(debugger_address)\n",
       "        1    0.000    0.000    0.000    0.000 netrc.py:10(NetrcParseError)\n",
       "        1    0.000    0.000    0.000    0.000 punycode.py:220(StreamWriter)\n",
       "        1    0.000    0.000    0.000    0.000 punycode.py:223(StreamReader)\n",
       "        1    0.000    0.000    0.000    0.000 punycode.py:214(IncrementalDecoder)\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method _socket.getdefaulttimeout}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'seek' of '_io.BufferedReader' objects}\n",
       "        1    0.000    0.000    0.000    0.000 options.py:82(arguments)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'get' of 'mappingproxy' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'weekday' of 'datetime.date' objects}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%prun\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "scrape_specs = {}\n",
    "scraypahs = {}\n",
    "start_time = time.time()\n",
    "\n",
    "for site in list(scraper_dict.keys()):\n",
    "    temp_start_time = time.time()\n",
    "    print('\\n' + site)\n",
    "    scraypahs[site] = scraypah(scraper_dict[site])\n",
    "    if scraper_dict[site]['css_bool']:\n",
    "        scraypahs[site].css_scrape_em()\n",
    "    else:\n",
    "        scraypahs[site].get_urls()\n",
    "        scraypahs[site].scrape_em()\n",
    "    scrape_specs = print_results(scraypahs[site].source, scraypahs[site].scraped_count, scraypahs[site].skip_count,\n",
    "                                 scraypahs[site].too_old, scraypahs[site].relevant_df, round(time.time()-temp_start_time, 2),\n",
    "                                 scrape_specs)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for auto-categorizing articles from academic journals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_academic(news):\n",
    "    uncat_news = news.copy()\n",
    "    # Create a column called is_journal for journal_bool.  If True, associated article is a journal\n",
    "    uncat_news['is_journal']=uncat_news.source.apply(lambda x: scraper_dict[x]['journal_bool'])\n",
    "    # Filter news based on whether or not it's a journal\n",
    "    journal_news = uncat_news[uncat_news.is_journal].copy()\n",
    "    nonjournal_news = uncat_news[~uncat_news.is_journal].copy()\n",
    "    # If the source of the article corresponding with the row is a journal, set the category = 4\n",
    "    journal_news.category = 4\n",
    "    nonjournal_news.category = ''\n",
    "    categorized_news = pd.concat([journal_news, nonjournal_news])\n",
    "    # Drop the is_journal column\n",
    "    categorized_news.drop(columns = 'is_journal', inplace = True)\n",
    "    return categorized_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get summary of articles scraped and filter to today's newstype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T14:34:17.790747Z",
     "start_time": "2018-11-09T14:34:17.279486Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Errors</th>\n",
       "      <th>Pages Scraped</th>\n",
       "      <th>Relevant Articles</th>\n",
       "      <th>Time spent</th>\n",
       "      <th>Too old</th>\n",
       "      <th>Time per relevant article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MIT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.13</td>\n",
       "      <td>19.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Semiconductor Engineering</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.05</td>\n",
       "      <td>20.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quartz</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.86</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Recode</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.62</td>\n",
       "      <td>24.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GovTech</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.70</td>\n",
       "      <td>41.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Reuters</td>\n",
       "      <td>0.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>205.92</td>\n",
       "      <td>477.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CityLab</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.87</td>\n",
       "      <td>33.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Autoblog</td>\n",
       "      <td>1.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>257.88</td>\n",
       "      <td>355.0</td>\n",
       "      <td>85.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Electrek</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>123.75</td>\n",
       "      <td>185.0</td>\n",
       "      <td>30.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Verge</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.82</td>\n",
       "      <td>76.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Crunchbase</td>\n",
       "      <td>5.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.39</td>\n",
       "      <td>106.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Truck News</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>110.75</td>\n",
       "      <td>124.0</td>\n",
       "      <td>13.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Trucks.com</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.653846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Charged EVs</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ARS Technica</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.06</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.353333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Venture Beat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20.69</td>\n",
       "      <td>28.0</td>\n",
       "      <td>6.896667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>IEEE Spectrum</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.95</td>\n",
       "      <td>22.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Transport Topics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>34.06</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>GreenCarCongress</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>115.58</td>\n",
       "      <td>87.0</td>\n",
       "      <td>19.263333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Green Car Reports</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>44.66</td>\n",
       "      <td>110.0</td>\n",
       "      <td>14.886667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>The Fuse</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.76</td>\n",
       "      <td>5.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Business Wire</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>U.S. Department of Energy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.07</td>\n",
       "      <td>22.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Journal of Modern Transportation</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.40</td>\n",
       "      <td>2.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Jalopnik</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.37</td>\n",
       "      <td>4.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Bloomberg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Business Insider</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>CNET</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.74</td>\n",
       "      <td>2.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Electric VTOL News</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Detroit News</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Science</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>246.40</td>\n",
       "      <td>143.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Biomass Magazine</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Alternative Energy News</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.21</td>\n",
       "      <td>117.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>New Atlas</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.60</td>\n",
       "      <td>113.0</td>\n",
       "      <td>55.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>AZoM</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>272.68</td>\n",
       "      <td>286.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>CompositesWorld</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.14</td>\n",
       "      <td>71.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Lightweighting World</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.71</td>\n",
       "      <td>45.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>National Center for Manufacturing Sciences</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>105.42</td>\n",
       "      <td>221.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Composites Manufacturing</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.16</td>\n",
       "      <td>54.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Nanowerk</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.91</td>\n",
       "      <td>133.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Materials Science &amp; Engineering</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.27</td>\n",
       "      <td>10.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Kenworth</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.45</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4.225000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Peterbilt</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.32</td>\n",
       "      <td>4.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Volvo</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.06</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Cummins</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.98</td>\n",
       "      <td>6.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Eaton</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Allison Transmission</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Ford</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.10</td>\n",
       "      <td>8.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         index  Errors  Pages Scraped  \\\n",
       "0                                          MIT     0.0           13.0   \n",
       "1                    Semiconductor Engineering     0.0            0.0   \n",
       "2                                       Quartz     0.0            3.0   \n",
       "3                                       Recode     0.0           13.0   \n",
       "4                                      GovTech     2.0            3.0   \n",
       "5                                      Reuters     0.0          147.0   \n",
       "6                                      CityLab     0.0            7.0   \n",
       "7                                     Autoblog     1.0          126.0   \n",
       "8                                     Electrek     0.0           68.0   \n",
       "9                                    The Verge     2.0            0.0   \n",
       "10                                  Crunchbase     5.0           22.0   \n",
       "11                                  Truck News     0.0           26.0   \n",
       "12                                  Trucks.com     0.0           17.0   \n",
       "13                                 Charged EVs     0.0           19.0   \n",
       "14                                ARS Technica     0.0           14.0   \n",
       "15                                Venture Beat     0.0           12.0   \n",
       "16                               IEEE Spectrum     0.0            1.0   \n",
       "17                            Transport Topics     0.0           61.0   \n",
       "18                            GreenCarCongress     0.0           29.0   \n",
       "19                           Green Car Reports     0.0           34.0   \n",
       "20                                    The Fuse     0.0            1.0   \n",
       "21                               Business Wire     0.0           19.0   \n",
       "22                   U.S. Department of Energy     0.0            3.0   \n",
       "23            Journal of Modern Transportation     0.0            0.0   \n",
       "24                                    Jalopnik     0.0           16.0   \n",
       "25                                   Bloomberg     0.0            0.0   \n",
       "26                            Business Insider     1.0           13.0   \n",
       "27                                        CNET     0.0            8.0   \n",
       "28                          Electric VTOL News     0.0           10.0   \n",
       "29                                Detroit News     2.0            1.0   \n",
       "30                                     Science     0.0           32.0   \n",
       "31                            Biomass Magazine   118.0            0.0   \n",
       "32                     Alternative Energy News     0.0            0.0   \n",
       "33                                   New Atlas     0.0           17.0   \n",
       "34                                        AZoM     0.0           58.0   \n",
       "35                             CompositesWorld     0.0           19.0   \n",
       "36                        Lightweighting World     0.0            0.0   \n",
       "37  National Center for Manufacturing Sciences     3.0            0.0   \n",
       "38                    Composites Manufacturing     0.0            0.0   \n",
       "39                                    Nanowerk     0.0           43.0   \n",
       "40             Materials Science & Engineering     0.0            0.0   \n",
       "41                                    Kenworth     0.0            2.0   \n",
       "42                                   Peterbilt     0.0            0.0   \n",
       "43                                       Volvo     0.0            1.0   \n",
       "44                                     Cummins     0.0            4.0   \n",
       "45                                       Eaton    30.0            0.0   \n",
       "46                        Allison Transmission     0.0            0.0   \n",
       "47                                        Ford     0.0            2.0   \n",
       "\n",
       "    Relevant Articles  Time spent  Too old  Time per relevant article  \n",
       "0                 0.0       24.13     19.0                        inf  \n",
       "1                 0.0       40.05     20.0                        inf  \n",
       "2                 1.0        3.86      7.0                   3.860000  \n",
       "3                 0.0       15.62     24.0                        inf  \n",
       "4                 0.0       64.70     41.0                        inf  \n",
       "5                 0.0      205.92    477.0                        inf  \n",
       "6                 0.0       25.87     33.0                        inf  \n",
       "7                 3.0      257.88    355.0                  85.960000  \n",
       "8                 4.0      123.75    185.0                  30.937500  \n",
       "9                 0.0       46.82     76.0                        inf  \n",
       "10                0.0       57.39    106.0                        inf  \n",
       "11                8.0      110.75    124.0                  13.843750  \n",
       "12               13.0        8.50      0.0                   0.653846  \n",
       "13                0.0       21.94      0.0                        inf  \n",
       "14                3.0       16.06     16.0                   5.353333  \n",
       "15                3.0       20.69     28.0                   6.896667  \n",
       "16                0.0        5.95     22.0                        inf  \n",
       "17               13.0       34.06     33.0                   2.620000  \n",
       "18                6.0      115.58     87.0                  19.263333  \n",
       "19                3.0       44.66    110.0                  14.886667  \n",
       "20                0.0        3.76      5.0                        inf  \n",
       "21                0.0        4.81      0.0                        inf  \n",
       "22                0.0        6.07     22.0                        inf  \n",
       "23                0.0        1.40      2.0                        inf  \n",
       "24                0.0       11.37      4.0                        inf  \n",
       "25                0.0        0.65      0.0                        inf  \n",
       "26                0.0        9.54      1.0                        inf  \n",
       "27                0.0        7.74      2.0                        inf  \n",
       "28                0.0       23.19      1.0                        inf  \n",
       "29                0.0        1.20      0.0                        inf  \n",
       "30                0.0      246.40    143.0                        inf  \n",
       "31                0.0       58.07      0.0                        inf  \n",
       "32                0.0       84.21    117.0                        inf  \n",
       "33                1.0       55.60    113.0                  55.600000  \n",
       "34                0.0      272.68    286.0                        inf  \n",
       "35                0.0       32.14     71.0                        inf  \n",
       "36                0.0       28.71     45.0                        inf  \n",
       "37                0.0      105.42    221.0                        inf  \n",
       "38                0.0       65.16     54.0                        inf  \n",
       "39                0.0       85.91    133.0                        inf  \n",
       "40                0.0       11.27     10.0                        inf  \n",
       "41                2.0        8.45     18.0                   4.225000  \n",
       "42                0.0        1.32      4.0                        inf  \n",
       "43                1.0       10.06      9.0                  10.060000  \n",
       "44                0.0        5.98      6.0                        inf  \n",
       "45                0.0       29.37      0.0                        inf  \n",
       "46                0.0        0.56      0.0                        inf  \n",
       "47                0.0        4.10      8.0                        inf  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21CTP articles found: 61\n",
      "CAV articles found: 0\n",
      "AFV articles found: 0\n",
      "Hyperloop articles found: 0\n",
      "eVTOL articles found: 0\n",
      "INL articles found: 0\n"
     ]
    }
   ],
   "source": [
    "# Meta-data from the scrape session\n",
    "scrape_specs_df = pd.DataFrame.from_dict(scrape_specs).T.reset_index()\n",
    "scrape_specs_df['Time per relevant article'] = scrape_specs_df['Time spent'] / \\\n",
    "    scrape_specs_df['Relevant Articles']\n",
    "display(scrape_specs_df)\n",
    "\n",
    "# List all of the relevant news from each of the scrapers (each scraypah item has an attribute \"relevant_df\", which is a pandas\n",
    "# dataframe with all of the selected news items from that website)\n",
    "all_news_dfs = []\n",
    "for key, value in scraypahs.items():\n",
    "    all_news_dfs.append(value.relevant_df)\n",
    "\n",
    "# Stack all of the articles into a single dataframe and do some cleaning (drop duplicate articles)\n",
    "all_df = pd.concat(all_news_dfs)\n",
    "all_df = all_df[['title', 'date'] + all_scrapers + ['summary', 'source', 'link']].sort_values('date', ascending=False)\n",
    "all_df.drop_duplicates(subset='title', inplace=True)\n",
    "all_df = all_df.replace('\\$', '$', regex=True)\n",
    "\n",
    "for scraper in all_scrapers:\n",
    "    print(scraper + ' articles found: {}'.format(\n",
    "    all_df[scraper].sum().astype(int)))\n",
    "\n",
    "# Populate meta-data columns (helpful for searching all news items in the future if we want)\n",
    "all_df['reason_for_tag'] = all_df.apply(which_keyword_found, axis=1)\n",
    "all_df['keywords'] = all_df['title'].str.strip().apply(keyword_pull)\n",
    "\n",
    "# Drop rows if summary is 'NA'\n",
    "#all_df = all_df[~all_df.summary.isna()].copy()\n",
    "\n",
    "# Add a column for category\n",
    "all_df['category'] = ''\n",
    "\n",
    "# Auto-categorize articles from academic journals\n",
    "all_df = categorize_academic(all_df)\n",
    "\n",
    "# Drop all with summary == None.  This removes some Autoblog articles.\n",
    "all_df = all_df[all_df.summary!=''].copy()\n",
    "\n",
    "# Format for excel writing.  Split all_news into CAV, truck, AFV, etc. news dataframes\n",
    "for scraper in all_scrapers:\n",
    "    scraper_info[scraper]['news_df']= all_df[all_df[scraper] == 1].sort_values('date', ascending=False).drop(all_scrapers, axis=1).copy()\n",
    "    # Automatically labels academic articles category '4' for ease of categorization.\n",
    "    \n",
    "    \n",
    "    if not scraper_info[scraper]['auto_id_research']:\n",
    "        scraper_info[scraper]['news_df'].category = ''\n",
    "        \n",
    "        \n",
    "    # Drop vehicles older than a given date -- this is useful if two scrapers with different max ages are run on the same day.\n",
    "    # If 21CTP max age is 7 and INL max age is 31 and both are run on the same day, articles 31 days old or less will be scraped for\n",
    "    # 21CTP.  So we need a way of dropping the articles that are older than the max age (7 days) for 21CTP.\n",
    "    scraper_info[scraper]['news_df'] = scraper_info[scraper]['news_df'][(scraper_info[scraper]['news_df'].date - search_date).apply(lambda x: x.days) >= -scraper_info[scraper]['max_age']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete duplicate articles, delete irrelevant articles, and sample the remaining articles to get the desired number of total articles\n",
    "Filter out duplicate articles -- If two sets of keywords are similar, keep article from best source\n",
    "\n",
    "Delete irrelevant articles -- Based on bad_words in title\n",
    "\n",
    "Sample remaining articles -- Keeps all articles from best sources (with ratings of 1 or 2), randomly samples the remaining articles (from the worst sources), and combines the two to get the total desired number of articles (see ideal_no_articles dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions needed for article removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a news dataframe (i.e. AFV_news) and a list of keywords corresponding to that news (i.e. AFV_keywords), delete_duplicates()\n",
    "# uses Jaccard similarity to identify duplicate articles and drops the duplicate articles.\n",
    "# The function returns a dataframe with no duplicates (news) and a dataframe with the dropped duplicates for reference.\n",
    "def delete_duplicates(all_news, keywords):\n",
    "    def get_jaccard_sim(list1, list2): # https://towardsdatascience.com/overview-of-text-similarity-metrics-3397c4601f50\n",
    "        # For our purposes, we'll first subtract the keywords from each set so that they don't skew our similarity rating\n",
    "        # too high\n",
    "        a = set(list1)\n",
    "        b = set(list2)\n",
    "        c = a.intersection(b)\n",
    "        return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "    def get_duplicates(new_news, MAX_SIM = .5): # Increase MAX_SIM (must be between 0 and 1) if \"duplicates\" are not close enough.  \n",
    "                                                # Reduce if not picking up any duplicates.\n",
    "        jaccard_sim_df = pd.DataFrame(columns = [*range(len(news))])\n",
    "        news['kw_set'] = news.keywords.apply(lambda x: str(x).split(','))\n",
    "        news['kw_set'] = news.kw_set.apply(lambda x: [y.strip() for y in x])\n",
    "        for col in jaccard_sim_df:\n",
    "            jaccard_sim_df[col] = news.kw_set.apply(lambda x: get_jaccard_sim(x, news.kw_set.loc[col]))\n",
    "        duplicate_pairs = list(jaccard_sim_df[jaccard_sim_df > MAX_SIM].stack().index) # Create list of article pairs that exceed a \n",
    "                                                                                   # Jaccard similarity of MAX_SIM\n",
    "        duplicate_pairs = set([frozenset(pair) for pair in duplicate_pairs if pair[0] != pair[1]]) # Remove articles paired with \n",
    "                                                                                               # themselves as well as \n",
    "                                                                                               # entries like (2,4) if (4,2) \n",
    "                                                                                               # is already in the list\n",
    "        duplicate_pairs = [tuple(pair) for pair in duplicate_pairs] # List of duplicate pairs\n",
    "        return duplicate_pairs\n",
    "\n",
    "    # Finds all the duplicates removed as well as the titles of the articles that were kept instead.\n",
    "    def get_removed_duplicates(all_news, dropped_vs_kept_tuples):\n",
    "        duplicates_removed = all_news.copy()\n",
    "        column_mapping = dict(zip(all_news.columns, all_news.columns + ' of dropped article'))\n",
    "        duplicates_removed.rename(columns = column_mapping, inplace = True)\n",
    "        duplicates_removed.insert(1, 'title of kept article', duplicates_removed.index)\n",
    "        duplicates_removed['title of kept article'] = duplicates_removed['title of kept article'].map(dict(dropped_vs_kept_tuples))\n",
    "        title_mapping = all_news.title.to_dict()\n",
    "        duplicates_removed['title of kept article'] = duplicates_removed['title of kept article'].map(title_mapping)\n",
    "        return duplicates_removed[~duplicates_removed['title of kept article'].isna()]\n",
    "    \n",
    "    all_news.reset_index(inplace = True, drop = True)\n",
    "    news = all_news.copy()\n",
    "    \n",
    "    # Get a list of tuples with pairs of indices corresponding to duplicate articles.\n",
    "    duplicate_pairs = get_duplicates(news)\n",
    "    \n",
    "    # Initialize a list to store tuples of (dropped, kept) index pairs.  This is for tracking which articles\n",
    "    # were dropped vs. kept.  To be used with get_removed_duplicates()\n",
    "    dropped_vs_kept_tuples = []\n",
    "    \n",
    "    # Drop duplicates with higher source ratings. Article sources are rated from 1-Best to 3-Worst.\n",
    "    for pair in duplicate_pairs:\n",
    "        # Make sure that both indices are in the news dataframe.  If not, continue.\n",
    "        if (pair[0] not in news.index) or (pair[1] not in news.index):\n",
    "            continue\n",
    "        # Get the ratings of the article sources associated with each index in the pair.  \n",
    "        rating0 = scraper_dict[news.loc[pair[0]].source]['rating']\n",
    "        rating1 = scraper_dict[news.loc[pair[1]].source]['rating']\n",
    "        # Drop the article that has the source with the lowest rating.  If sources have the same rating, choose randomly.\n",
    "        if rating0 > rating1:\n",
    "            index_choice = 0\n",
    "        elif rating0 < rating1:\n",
    "            index_choice = 1\n",
    "        else:\n",
    "            index_choice = random.choice([0, 1])\n",
    "        \n",
    "        # Add a tuple to track which index was dropped vs. kept (dropped, kept).  \n",
    "        # This list is to be used with get_duplicates_removed().\n",
    "        dropped_vs_kept_tuples.append(tuple([pair[index_choice], pair[int(not index_choice)]]))\n",
    "        # Drop the article from the news dataframe.\n",
    "        news.drop(pair[index_choice], inplace = True)\n",
    "\n",
    "    duplicates_removed = get_removed_duplicates(all_news, dropped_vs_kept_tuples)\n",
    "    news.drop(columns = 'kw_set', inplace = True)\n",
    "    duplicates_removed['classification (d, x, s)'] = 'd' # Classified as 'd' for duplicate\n",
    "    return news, duplicates_removed\n",
    "\n",
    "# Deletes articles from news based on bad_words in title.  For example if we don't want 'picture' in the title of\n",
    "# AFV_news articles, we would add 'picture' to AFV_bad_words.\n",
    "def delete_irrelevant(news, bad_words):\n",
    "    bad_words = [bad_word.lower() for bad_word in bad_words] # Make bad words lowercase and later make titles lowercase to remove case sensitivity.\n",
    "    deleted_articles = news.copy()\n",
    "    column_mapping = dict(zip(news.columns, news.columns + ' of dropped article'))\n",
    "    deleted_articles.rename(columns = column_mapping, inplace = True)\n",
    "    deleted_articles['reason_for_deletion'] = deleted_articles['title of dropped article'].str.lower().apply(lambda x: list((set(x.split())&set(bad_words))))\n",
    "    deleted_articles.reason_for_deletion = deleted_articles.reason_for_deletion.apply(lambda x: ', '.join(x))\n",
    "    kept_articles = deleted_articles.copy()\n",
    "    kept_articles = kept_articles[kept_articles.reason_for_deletion==''].copy()\n",
    "    #print(kept_articles)\n",
    "    kept_articles.drop(columns = 'reason_for_deletion', inplace = True)\n",
    "    deleted_articles['classification (d, x, s)'] = 'x' # Classified as 'i' for irrelevant\n",
    "    deleted_articles = deleted_articles[deleted_articles.reason_for_deletion!=''].copy()\n",
    "    kept_articles.rename(columns = {v:k for k,v in column_mapping.items()}, inplace = True) # Map column names back to what they originally were in news.\n",
    "    return kept_articles, deleted_articles\n",
    "\n",
    "def sample_news(news, ideal_no):\n",
    "    def get_deleted_articles(news, kept_articles):\n",
    "        deleted_articles = news.copy()\n",
    "        deleted_articles['deleted'] = ~deleted_articles.title.isin(kept_articles.title)\n",
    "        deleted_articles = deleted_articles[deleted_articles.deleted].copy()\n",
    "        deleted_articles.drop(columns = 'deleted', inplace = True)\n",
    "        column_mapping = dict(zip(news.columns, news.columns + ' of dropped article'))\n",
    "        deleted_articles.rename(columns = column_mapping, inplace = True)\n",
    "        deleted_articles['classification (d, x, s)'] = 's'\n",
    "        return deleted_articles\n",
    "\n",
    "    news_with_source_ratings = news.copy()\n",
    "    # Create a column with the ratings of each news source.\n",
    "    news_with_source_ratings['rating']=news_with_source_ratings.source.apply(lambda x: scraper_dict[x]['rating'])\n",
    "    # Create dataframes with best and worst sources.\n",
    "    best_source_news = news_with_source_ratings[news_with_source_ratings.rating < 3].copy()\n",
    "    worst_source_news = news_with_source_ratings[news_with_source_ratings.rating == 3].copy()\n",
    "    \n",
    "    # If we have less than the idel_no of articles, just keep all news.\n",
    "    if len(news_with_source_ratings) <= ideal_no:\n",
    "        kept_articles = news_with_source_ratings\n",
    "    \n",
    "    # If the number of \"good\" articles is greater than the desired total number of articles sample only best_source_news\n",
    "    elif len(best_source_news) > ideal_no: \n",
    "        no_to_sample = ideal_no\n",
    "        kept_articles = best_source_news.sample(no_to_sample)\n",
    "    \n",
    "    # Otherwise, Sample worst_source_news and concatenate with best_source_news\n",
    "    else:\n",
    "        no_to_sample = ideal_no - len(best_source_news)\n",
    "        kept_articles = pd.concat([best_source_news, worst_source_news.sample(no_to_sample)])\n",
    "    kept_articles.drop(columns = 'rating', inplace = True)\n",
    "    return kept_articles, get_deleted_articles(news,kept_articles)\n",
    "\n",
    "# Sets aside articles to keep no matter what; i.e., keeps all articles with a certain rating.\n",
    "def keep_no_matter_what(news, rating_no):\n",
    "    all_articles = news.copy()\n",
    "    all_articles['rating']=all_articles.source.apply(lambda x: scraper_dict[x]['rating'])\n",
    "    kept_no_matter_what_articles = all_articles[all_articles.rating == rating_no].copy()\n",
    "    all_other_articles = all_articles[all_articles.rating != rating_no].copy()\n",
    "    kept_no_matter_what_articles.drop(columns = 'rating', inplace = True)\n",
    "    all_other_articles.drop(columns = 'rating', inplace = True)\n",
    "    return kept_no_matter_what_articles, all_other_articles\n",
    "\n",
    "# Executes delete_duplicates, delete_irrelevant, and sample_news functions to remove all unwanted articles from news df.\n",
    "def make_all_deletions(news, keywords, bad_words, ideal_no_articles):\n",
    "    all_articles = news.copy()\n",
    "    kept_no_matter_what_articles, all_other_articles = keep_no_matter_what(all_articles, 0) # Keep all articles with a rating of '0' no matter what.\n",
    "    kept_articles, duplicate_articles = delete_duplicates(all_other_articles, keywords) # Delete duplicates\n",
    "    kept_articles, irrelevant_articles = delete_irrelevant(kept_articles, bad_words) # Delete irrelevant\n",
    "    kept_articles, articles_not_sampled = sample_news(kept_articles, ideal_no_articles)\n",
    "    deleted_articles = pd.concat([duplicate_articles, irrelevant_articles, articles_not_sampled])#, sort = True) # Concatenate duplicate and irrelevant dataframes to get all deleted articles\n",
    "    # Reorder columns and fill NA\n",
    "    deleted_articles = deleted_articles[['title of dropped article', 'title of kept article', 'reason_for_deletion', 'date of dropped article', 'summary of dropped article', \n",
    "                                         'source of dropped article', 'link of dropped article', 'reason_for_tag of dropped article', 'keywords of dropped article', 'classification (d, x, s)']]\n",
    "    deleted_articles.fillna('NA', inplace = True)\n",
    "    # Add column for correct classification -- to be filled in by hand in Excel\n",
    "    deleted_articles['correct classification (d = duplicate, x = irrelevant, s = sample, n = neither)'] = ''\n",
    "    kept_articles = kept_articles[~kept_articles.summary.isna()].copy()\n",
    "    kept_articles = pd.concat([kept_articles, kept_no_matter_what_articles])\n",
    "    return kept_articles, deleted_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute duplicate and irrelevant article removal and write duplicates and irrelevant articles to spreadsheet (skip this cell if you don't want to remove articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some 21CTP stuff!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebarnard\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:146: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "for scraper in todays_scrapers:\n",
    "    # If we have at least 1 news item and we want to make deletions...\n",
    "    if (scraper_info[scraper]['news_df'].shape[0]>0) and (scraper_info[scraper]['make_deletions']):\n",
    "        # Execute duplicate and irrelevant article removal                                                             \n",
    "        scraper_info[scraper]['news_df'], scraper_info[scraper]['deletions_df'] = make_all_deletions(scraper_info[scraper]['news_df'], \n",
    "                                                                                                                                scraper_info[scraper]['general_keywords']+scraper_info[scraper]['specific_keywords'], \n",
    "                                                                                                                                scraper_info[scraper]['bad_words'], \n",
    "                                                                                                                                scraper_info[scraper]['ideal_no_articles'])\n",
    "        # Write duplicates and irrelevant articles to spreadsheet\n",
    "        if scraper_info[scraper]['deletions_df'].shape[0] > 0:\n",
    "            filename = scraper_info[scraper]['deletions_filename']\n",
    "            scraper_info[scraper]['deletions_df'].to_excel(filename)\n",
    "            print('Some ' + scraper + ' stuff!')\n",
    "            os.startfile(cwd + '/' + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write final dataframe to a spreadsheet\n",
    "CAVs on Monday, AFVs on Wednesday, 21CTP on Friday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T16:55:54.374736Z",
     "start_time": "2018-10-17T16:55:54.098473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some 21CTP stuff!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press enter to continue:  \n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "for scraper in todays_scrapers:\n",
    "    if scraper_info[scraper]['news_df'].shape[0] > 0:\n",
    "        filename = scraper_info[scraper]['news_download_filename']\n",
    "        scraper_info[scraper]['news_df'].to_excel(filename)\n",
    "        print('Some ' + scraper + ' stuff!')\n",
    "        os.startfile(cwd + '/' + filename)\n",
    "        \n",
    "_=input('Press enter to continue: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get AFV graph, table, and CA shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_bool = False\n",
    "if search_date.weekday() == scraper_info['AFV']['day']:\n",
    "    try:\n",
    "        # Go into EVSE directory.  This directory should be in the same directory as News_scraper.ipynb. This directory must contain EVSE Market Analysis.ipynb\n",
    "        %cd EVSE \n",
    "        cwd = os.getcwd()\n",
    "        print(cwd)\n",
    "        #!jupyter \"EVSE Market Analysis.ipynb\" --to script\n",
    "        %run \"EVSE Market Analysis.ipynb\"\n",
    "        # Get CA_shares from all_CA_shares.csv\n",
    "        CA_shares_data = pd.read_csv(cwd + '\\\\' + 'all_CA_shares.csv', ';')[['date', 'text']]\n",
    "        CA_shares_data.set_index('date', inplace = True)\n",
    "        \n",
    "        EVSE_file_dict = {\n",
    "            'EVSE_bar_chart' : cwd + '\\\\'+ f'EVSE_bar_chart_{search_date_str}.png', # Get bar chart\n",
    "            'CA_shares' : CA_shares_data.loc[search_date_str]['text'], # Get CA shares from today\n",
    "            \n",
    "        }\n",
    "        # Open deltstations.xlsx file for copying and pasting delt stations table into AFV news\n",
    "        os.startfile(cwd + '\\\\' + f'{search_date_str}_deltstation.xlsx')\n",
    "        graph_bool = True # Means all the necessary files are in the directory. If false, graphs will not be added to the docx\n",
    "    except:\n",
    "        print('There is a problem with one of the files.  The possibilities are:')\n",
    "        print('\\t1. ' + \"alt_fuel_stations ({}).csv\".format((dt.datetime.today() - dt.timedelta(days=14)).strftime(\"%B %d %Y\")) + 'is not in the EVSE folder.')\n",
    "        print('\\t2. ' + \"alt_fuel_stations ({}).csv\".format((dt.datetime.today() - dt.timedelta(days=7)).strftime(\"%B %d %Y\")) + 'is not in the EVSE folder.')\n",
    "        print('\\t3. all_CA_shares.csv is not in the EVSE folder.')\n",
    "        print('\\t4. ' + search_date_str + ' is not in the all_CA_shares.csv file.')\n",
    "    %cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Word file from the news update spreadsheets\n",
    "Automatically does CAV on Mondays, AFV on Wednesdays, and 21CTP on Fridays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:25:20.873985Z",
     "start_time": "2018-10-17T17:25:20.757295Z"
    }
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "for scraper in todays_scrapers:\n",
    "    if scraper_info[scraper]['gen_docx']:\n",
    "        print(scraper)\n",
    "        docx_filename = cwd + '/'+ gen_docx(scraper, graph_bool)\n",
    "        os.startfile(docx_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update news item tracking and news scraper meta-data databases\n",
    "Only run when **final** news item spreadsheet is saved in your working directory (i.e., after you have manually added other articles to the already-saved spreadsheet from the cell above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T11:30:00.039138Z",
     "start_time": "2018-09-12T11:30:00.031855Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is in case you go to upload this week's news items to the database, and realize you forgot to do last week's. Just replace\n",
    "# all instances of \"search_date_str\" in the next cell with \"last_week\"and run it. Make sure you switch them all back to \"search_date_str\"..\n",
    "last_week = str((pd.to_datetime(search_date_str) - dt.timedelta(days=7)).date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T21:47:24.974039Z",
     "start_time": "2018-11-19T21:47:24.165717Z"
    }
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('news_updates.db')\n",
    "if (search_date.weekday() == scraper_info['CAV']['day']) & (~db_update):\n",
    "    print('CAV')\n",
    "    pd.read_excel('cav_news_updates/{}_cav_news_download.xls'.format(search_date_str)\n",
    "                  ).to_sql('CAV', conn, if_exists='append', index=False)\n",
    "    db_update = True\n",
    "elif (search_date.weekday() == scraper_info['AFV']['day']) & (~db_update):\n",
    "    print('AFV')\n",
    "    pd.read_excel('afv_news_updates/{}_afv_news_download.xls'.format(search_date_str)\n",
    "                  ).to_sql('AFV', conn, if_exists='append', index=False)\n",
    "    db_update = True\n",
    "conn.close()\n",
    "\n",
    "# This saves the meta-data from all of the scraper runs every Wednesday (print out \"scrape_specs_df\" to see what the meta-data includes)\n",
    "if search_date.weekday() == 2:\n",
    "    conn = sqlite3.connect('news_updates_meta.db')\n",
    "    scrape_specs_df.drop(['Time spent', 'Time per relevant article'], axis=1).to_sql(\n",
    "        'news_updates_meta', conn, if_exists='append', index=False)\n",
    "    conn.close()\n",
    "    print('Uploaded metadata! So many datas!')\n",
    "    \n",
    "# Need to add 21CTP, evtol, and hyperloop metadata?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Halt run all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False #Will generate an error and stop notebook execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"test\"></a>\n",
    "<a href = \"#scraper_dict\">Jump to scraper_dict</a>\n",
    "# For scraper testing (no need to run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing a single website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing a single scraper - only needed when adding new sites (don't want to run all of them over and over...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T13:37:17.149891Z",
     "start_time": "2019-01-07T13:37:04.771546Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Unknown string format:', '2 days ago'): https://www.autoblog.com/photos/20-most-efficient-crossovers-and-suvs/ \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "('Unknown string format:', 'May 4th 20192019'): https://www.autoblog.com/2019/04/03/mercedes-glb-suv-crossover-interior-teaser-photo/ \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "117 Autoblog article(s) scraped\n",
      "2 Autoblog article(s) skipped due to error\n",
      "362 Autoblog article(s) skipped due to age\n",
      "13 relevant article(s) collected\n",
      " "
     ]
    },
    {
     "data": {
      "text/plain": [
       "         46852455 function calls (46825657 primitive calls) in 236.834 seconds\n",
       "\n",
       "   Ordered by: internal time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "     2431  110.141    0.045  110.141    0.045 {built-in method _openssl.SSL_read}\n",
       "      505   50.308    0.100   50.308    0.100 {built-in method time.sleep}\n",
       "      505   13.311    0.026   13.351    0.026 {built-in method _openssl.SSL_do_handshake}\n",
       "      517   11.485    0.022   11.485    0.022 {method 'connect' of '_socket.socket' objects}\n",
       "      505   10.265    0.020   10.265    0.020 {built-in method _openssl.SSL_CTX_load_verify_locations}\n",
       "   505064    2.950    0.000   17.969    0.000 _lxml.py:149(start)\n",
       " 1010/505    2.910    0.003   28.900    0.057 parser.pxi:1242(feed)\n",
       "  1828192    1.869    0.000    1.921    0.000 element.py:248(setup)\n",
       "   471918    1.771    0.000    8.189    0.000 element.py:873(__init__)\n",
       "  1526066    1.718    0.000    5.419    0.000 __init__.py:392(endData)\n",
       "        3    1.696    0.565    1.696    0.565 {method 'recv_into' of '_socket.socket' objects}\n",
       "   351131    1.610    0.000    5.501    0.000 __init__.py:152(_replace_cdata_list_attribute_values)\n",
       "      514    1.295    0.003    1.303    0.003 {built-in method _socket.getaddrinfo}\n",
       "   505064    1.062    0.000   13.397    0.000 __init__.py:502(handle_starttag)\n",
       "   505064    0.956    0.000    5.179    0.000 _lxml.py:198(end)\n",
       "   678137    0.909    0.000    1.989    0.000 __init__.py:422(object_was_parsed)\n",
       "   505064    0.871    0.000    1.393    0.000 __init__.py:479(_popToTag)\n",
       "  5357329    0.721    0.000    0.977    0.000 {built-in method builtins.isinstance}\n",
       "     2128    0.708    0.000    0.708    0.000 {built-in method nt.stat}\n",
       "   505064    0.698    0.000   19.600    0.000 saxparser.pxi:374(_handleSaxTargetStartNoNs)\n",
       "   678137    0.696    0.000    1.643    0.000 element.py:764(__new__)\n",
       "   471918    0.688    0.000    0.847    0.000 __init__.py:383(pushTag)\n",
       "2724652/2723612    0.651    0.000    0.743    0.000 {built-in method builtins.len}\n",
       "     1697    0.641    0.000    0.641    0.000 {method 'decompress' of 'zlib.Decompress' objects}\n",
       "      505    0.430    0.001    0.506    0.001 connectionpool.py:407(close)\n",
       "  2409602    0.422    0.000    0.422    0.000 apihelpers.pxi:1400(funicode)\n",
       "246264/227900    0.421    0.000    0.798    0.000 element.py:1792(_matches)\n",
       "   749162    0.417    0.000    0.783    0.000 _lxml.py:219(data)\n",
       "   505064    0.408    0.000    5.939    0.000 saxparser.pxi:452(_handleSaxEndNoNs)\n",
       "  1712150    0.395    0.000    0.395    0.000 _lxml.py:80(_getNsTag)\n",
       "   505064    0.387    0.000    1.864    0.000 __init__.py:529(handle_endtag)\n",
       "   471413    0.359    0.000    0.464    0.000 __init__.py:374(popTag)\n",
       "   527808    0.358    0.000    0.359    0.000 {method 'items' of 'dict' objects}\n",
       "   264260    0.323    0.000    0.323    0.000 {method 'split' of 're.Pattern' objects}\n",
       "   355485    0.322    0.000    0.323    0.000 {method 'keys' of 'dict' objects}\n",
       "  2526178    0.319    0.000    0.319    0.000 {method 'append' of 'list' objects}\n",
       "   704520    0.317    0.000    0.317    0.000 {built-in method __new__ of type object at 0x000007FEE6536BA0}\n",
       "   749162    0.298    0.000    1.339    0.000 saxparser.pxi:493(_handleSaxData)\n",
       "   402512    0.288    0.000    1.461    0.000 element.py:1766(search)\n",
       "   754094    0.272    0.000    0.370    0.000 __init__.py:534(handle_data)\n",
       "   749162    0.258    0.000    1.041    0.000 parsertarget.pxi:87(_handleSaxData)\n",
       "   505064    0.257    0.000   18.226    0.000 parsertarget.pxi:78(_handleSaxStart)\n",
       "   215838    0.242    0.000    1.268    0.000 element.py:1725(search_tag)\n",
       "   135208    0.219    0.000    0.504    0.000 os.py:673(__getitem__)\n",
       "   505064    0.212    0.000    5.391    0.000 parsertarget.pxi:84(_handleSaxEnd)\n",
       "   837694    0.196    0.000    0.196    0.000 {method 'get' of 'dict' objects}\n",
       "   141182    0.185    0.000    0.762    0.000 _collections_abc.py:742(__iter__)\n",
       "     2347    0.181    0.000    1.818    0.001 element.py:571(_find_all)\n",
       "   706954    0.176    0.000    0.309    0.000 apihelpers.pxi:1397(funicodeOrEmpty)\n",
       "   505064    0.166    0.000   18.392    0.000 saxparser.pxi:401(_callTargetSaxStart)\n",
       "   471918    0.164    0.000    0.164    0.000 __init__.py:108(can_be_empty_element)\n",
       "   471918    0.161    0.000    0.221    0.000 __init__.py:273(set_up_substitutions)\n",
       "  1487545    0.153    0.000    0.153    0.000 element.py:1084(__bool__)\n",
       "   607898    0.144    0.000    0.144    0.000 {method 'lower' of 'str' objects}\n",
       "   474036    0.143    0.000    0.152    0.000 {built-in method _abc._abc_instancecheck}\n",
       "   124627    0.143    0.000    0.186    0.000 _collections_abc.py:672(keys)\n",
       "   135208    0.136    0.000    0.285    0.000 os.py:743(encodekey)\n",
       "     1091    0.133    0.000    0.880    0.001 request.py:2456(getproxies_environment)\n",
       "      532    0.132    0.000    0.132    0.000 {built-in method nt._isdir}\n",
       "   899409    0.131    0.000    0.131    0.000 {built-in method builtins.hasattr}\n",
       " 1010/505    0.129    0.000    1.431    0.003 parser.pxi:1368(close)\n",
       "758716/758372    0.116    0.000    0.118    0.000 {method 'join' of 'str' objects}\n",
       "   124686    0.111    0.000    0.125    0.000 _collections_abc.py:719(__iter__)\n",
       "   473929    0.106    0.000    0.106    0.000 {method 'pop' of 'list' objects}\n",
       "     8585    0.102    0.000    0.138    0.000 decode_asn1.py:27(_obj2txt)\n",
       "   474036    0.102    0.000    0.254    0.000 abc.py:137(__instancecheck__)\n",
       "    15089    0.094    0.000    0.299    0.000 parse.py:361(urlparse)\n",
       "   131040    0.089    0.000    0.089    0.000 os.py:696(__iter__)\n",
       "      532    0.086    0.000    0.086    0.000 {built-in method _openssl.SSL_write}\n",
       "  1010128    0.086    0.000    0.086    0.000 etree.pyx:109(__len__)\n",
       "    18759    0.085    0.000    0.151    0.000 parse.py:394(urlsplit)\n",
       "   505064    0.085    0.000    0.085    0.000 _lxml.py:189(_prefix_for_namespace)\n",
       "   135208    0.083    0.000    0.107    0.000 os.py:737(check_str)\n",
       "      505    0.082    0.000    0.082    0.000 {built-in method _openssl.SSL_CTX_set_cipher_list}\n",
       "      505    0.078    0.000    0.078    0.000 {built-in method _openssl.SSL_CTX_new}\n",
       "      505    0.076    0.000    0.751    0.001 decode_asn1.py:192(parse)\n",
       "    36059    0.075    0.000    0.084    0.000 parse.py:109(_coerce_args)\n",
       "        1    0.072    0.072  222.463  222.463 <ipython-input-10-19c3509cb48d>:88(scrape_em)\n",
       "      505    0.072    0.000    0.080    0.000 extensions.py:1377(__init__)\n",
       "406517/405903    0.072    0.000    0.173    0.000 {built-in method builtins.next}\n",
       "    19596    0.071    0.000    0.071    0.000 {method 'search' of 're.Pattern' objects}\n",
       "   242706    0.064    0.000    0.082    0.000 element.py:1689(_normalize_search_value)\n",
       "   414016    0.062    0.000    0.066    0.000 element.py:1386(descendants)\n",
       "     1037    0.059    0.000    0.243    0.000 cookiejar.py:452(parse_ns_headers)\n",
       "     1010    0.058    0.000    0.065    0.000 extensions.py:489(__init__)\n",
       "    44858    0.056    0.000    0.091    0.000 xmlerror.pxi:197(_receive)\n",
       "   505064    0.055    0.000    0.055    0.000 saxparser.pxi:481(_pushSaxEndEvent)\n",
       "     7575    0.055    0.000    0.202    0.000 core.py:234(check_label)\n",
       "     1091    0.054    0.000    0.054    0.000 {built-in method winreg.OpenKey}\n",
       "     8585    0.053    0.000    0.067    0.000 _oid.py:11(__init__)\n",
       "     1650    0.050    0.000    0.050    0.000 {built-in method winreg.QueryValueEx}\n",
       "     3112    0.048    0.000    0.118    0.000 _collections_abc.py:824(update)\n",
       "   128857    0.047    0.000    0.047    0.000 _collections_abc.py:698(__init__)\n",
       "   141969    0.045    0.000    0.045    0.000 {method 'upper' of 'str' objects}\n",
       "    69716    0.045    0.000    0.063    0.000 element.py:1049(get)\n",
       "    32825    0.044    0.000    0.087    0.000 intranges.py:38(intranges_contain)\n",
       "   104208    0.044    0.000    0.044    0.000 {built-in method builtins.getattr}\n",
       "     3111    0.043    0.000    0.442    0.000 cookiejar.py:933(set_ok)\n",
       "     2431    0.041    0.000  110.223    0.045 SSL.py:1787(recv_into)\n",
       "      535    0.040    0.000    0.108    0.000 feedparser.py:471(_parse_headers)\n",
       "    14883    0.039    0.000    0.088    0.000 _collections_abc.py:657(get)\n",
       "     3111    0.039    0.000    0.256    0.000 cookiejar.py:1461(_cookie_from_cookie_tuple)\n",
       "     5585    0.038    0.000    0.095    0.000 queue.py:121(put)\n",
       "      739    0.037    0.000    1.863    0.003 {built-in method builtins.eval}\n",
       "     6781    0.036    0.000    0.060    0.000 parse.py:810(quote_from_bytes)\n",
       "    45760    0.036    0.000    0.036    0.000 {method 'encode' of 'str' objects}\n",
       "     2202    0.035    0.000    0.676    0.000 response.py:71(decompress)\n",
       "     3468    0.033    0.000    0.047    0.000 SSL.py:1600(_raise_ssl_error)\n",
       "    17047    0.033    0.000    0.033    0.000 {method 'sub' of 're.Pattern' objects}\n",
       "     3111    0.033    0.000    0.072    0.000 cookiejar.py:142(_str2time)\n",
       "      535    0.032    0.000    0.382    0.001 client.py:193(parse_headers)\n",
       "     6411    0.032    0.000    0.200    0.000 cookiejar.py:606(request_host)\n",
       "     1070    0.031    0.000    0.236    0.000 feedparser.py:218(_parsegen)\n",
       "      518    0.031    0.000    0.031    0.000 {function socket.close at 0x00000000036516A8}\n",
       "    19044    0.030    0.000    0.037    0.000 structures.py:51(__getitem__)\n",
       "     5050    0.030    0.000    0.030    0.000 {built-in method _openssl.X509V3_EXT_d2i}\n",
       "     1118    0.029    0.000    1.142    0.001 cookiejar.py:1654(extract_cookies)\n",
       "    27082    0.029    0.000    0.029    0.000 {method 'split' of 'str' objects}\n",
       "      518    0.028    0.000    0.028    0.000 socket.py:139(__init__)\n",
       "     2074    0.028    0.000    0.031    0.000 cookiejar.py:1364(_normalized_cookie_tuples)\n",
       "     3111    0.028    0.000    0.039    0.000 cookiejar.py:747(__init__)\n",
       "    15740    0.027    0.000    0.027    0.000 {method 'match' of 're.Pattern' objects}\n",
       "    36084    0.027    0.000    0.027    0.000 {method 'startswith' of 'str' objects}\n",
       "     3088    0.026    0.000    0.026    0.000 threading.py:75(RLock)\n",
       "     8362    0.026    0.000    0.046    0.000 _policybase.py:293(header_source_parse)\n",
       "     2236    0.026    0.000    0.051    0.000 message.py:497(get_all)\n",
       "  532/505    0.026    0.000  146.600    0.290 sessions.py:617(send)\n",
       "      505    0.025    0.000   32.090    0.064 connection.py:299(connect)\n",
       "     2201    0.024    0.000    1.081    0.000 response.py:629(read_chunked)\n",
       "      481    0.024    0.000    0.129    0.000 {built-in method pandas._libs.tslib.array_to_datetime}\n",
       "     3111    0.024    0.000    0.150    0.000 cookiejar.py:220(http2time)\n",
       "     6781    0.024    0.000    0.092    0.000 parse.py:746(quote)\n",
       "      534    0.024    0.000  144.780    0.271 connectionpool.py:446(urlopen)\n",
       "    11169    0.024    0.000    0.038    0.000 threading.py:335(notify)\n",
       "     4814    0.023    0.000    0.050    0.000 message.py:462(get)\n",
       "    14047    0.023    0.000    0.027    0.000 structures.py:46(__setitem__)\n",
       "    11351    0.023    0.000  111.916    0.010 {method 'readline' of '_io.BufferedReader' objects}\n",
       "      534    0.022    0.000  144.545    0.271 connectionpool.py:319(_make_request)\n",
       "    39667    0.022    0.000    0.022    0.000 {method 'decode' of 'bytes' objects}\n",
       "     3535    0.022    0.000    0.121    0.000 sessions.py:49(merge_setting)\n",
       "     9483    0.022    0.000    0.028    0.000 _collections.py:151(__getitem__)\n",
       "    15077    0.022    0.000    0.046    0.000 _policybase.py:281(_sanitize_header)\n",
       "9439/6833    0.021    0.000    1.119    0.000 {method 'join' of 'bytes' objects}\n",
       "    32825    0.021    0.000    0.021    0.000 {built-in method _bisect.bisect_left}\n",
       "     1118    0.021    0.000    0.623    0.001 cookiejar.py:1574(make_cookies)\n",
       "     7575    0.021    0.000    0.028    0.000 core.py:67(check_bidi)\n",
       "      535    0.021    0.000  111.760    0.209 client.py:256(_read_status)\n",
       "     9549    0.020    0.000    0.065    0.000 cookies.py:51(get_full_url)\n",
       "     1569    0.020    0.000    0.058    0.000 ntpath.py:287(expanduser)\n",
       "      505    0.019    0.000   30.465    0.060 __init__.py:88(__init__)\n",
       "     6089    0.019    0.000    0.049    0.000 queue.py:153(get)\n",
       "      505    0.019    0.000    0.050    0.000 inspect.py:2115(_signature_from_function)\n",
       "    24333    0.019    0.000    0.019    0.000 {method 'find' of 'str' objects}\n",
       "     3535    0.019    0.000    0.054    0.000 utils.py:284(to_key_val_list)\n",
       "      505    0.019    0.000    0.821    0.002 sessions.py:426(prepare_request)\n",
       "      532    0.019    0.000  144.735    0.272 adapters.py:394(send)\n",
       "      534    0.019    0.000    0.023    0.000 poolmanager.py:58(_default_key_normalizer)\n",
       "      505    0.018    0.000  148.488    0.294 sessions.py:466(request)\n",
       "     2607    0.018    0.000    0.130    0.000 structures.py:40(__init__)\n",
       "     8585    0.018    0.000    0.018    0.000 {built-in method _openssl.OBJ_obj2txt}\n",
       "    15655    0.018    0.000    0.036    0.000 _oid.py:43(__eq__)\n",
       "     3192    0.018    0.000    0.041    0.000 cookiejar.py:536(domain_match)\n",
       "     9967    0.018    0.000    0.021    0.000 feedparser.py:78(readline)\n",
       "      505    0.018    0.000    0.107    0.000 SSL.py:705(__init__)\n",
       "     6249    0.018    0.000    0.106    0.000 cookiejar.py:663(escape_path)\n",
       "     3192    0.017    0.000    0.042    0.000 cookies.py:343(set_cookie)\n",
       "     7575    0.017    0.000    0.229    0.000 core.py:291(ulabel)\n",
       "      506    0.017    0.000    0.134    0.000 connectionpool.py:159(__init__)\n",
       "    51005    0.017    0.000    0.027    0.000 utils.py:34(<lambda>)\n",
       "     3192    0.016    0.000    0.021    0.000 cookiejar.py:1641(set_cookie)\n",
       "      505    0.016    0.000    0.086    0.000 inspect.py:1087(getfullargspec)\n",
       "     4313    0.016    0.000    0.039    0.000 parse.py:154(hostname)\n",
       "        1    0.016    0.016    0.016    0.016 {built-in method _winapi.CreateProcess}\n",
       "     3111    0.016    0.000    0.018    0.000 calendar.py:655(timegm)\n",
       "      535    0.016    0.000  112.181    0.210 client.py:289(begin)\n",
       "     9118    0.016    0.000    0.030    0.000 enum.py:283(__call__)\n",
       "     1515    0.016    0.000    0.040    0.000 SSL.py:306(wrapper)\n",
       "     2704    0.016    0.000    0.029    0.000 client.py:1185(putheader)\n",
       "      506    0.016    0.000    0.147    0.000 sessions.py:365(__init__)\n",
       "     2608    0.015    0.000    0.094    0.000 client.py:594(_safe_read)\n",
       "     2525    0.015    0.000    0.276    0.000 core.py:340(encode)\n",
       "      532    0.015    0.000    0.400    0.001 utils.py:168(get_netrc_auth)\n",
       "     3138    0.015    0.000    0.108    0.000 cookiejar.py:633(request_path)\n",
       "     2434    0.015    0.000  111.953    0.046 socket.py:575(readinto)\n",
       "     3192    0.015    0.000    0.237    0.000 cookiejar.py:712(is_third_party)\n",
       "      507    0.015    0.000    6.845    0.014 connection.py:33(create_connection)\n",
       "      505    0.015    0.000    0.303    0.001 ssl_.py:229(create_urllib3_context)\n",
       "     8359    0.015    0.000    0.028    0.000 _collections.py:209(add)\n",
       "   124626    0.015    0.000    0.015    0.000 etree.pyx:112(__iter__)\n",
       "     9118    0.015    0.000    0.015    0.000 enum.py:525(__new__)\n",
       "     4545    0.014    0.000    0.029    0.000 decode_asn1.py:92(_decode_general_name)\n",
       "     1518    0.014    0.000    0.014    0.000 threading.py:216(__init__)\n",
       "    44858    0.014    0.000    0.105    0.000 parser.pxi:612(_forwardParserError)\n",
       "     9576    0.014    0.000    0.027    0.000 cookiejar.py:521(is_HDN)\n",
       "     4316    0.014    0.000    0.019    0.000 parse.py:190(_hostinfo)\n",
       "     2075    0.014    0.000    0.042    0.000 cookiejar.py:1236(__init__)\n",
       "    20200    0.014    0.000    0.018    0.000 backend.py:114(openssl_assert)\n",
       "      534    0.014    0.000    0.137    0.000 response.py:499(from_httplib)\n",
       "      505    0.013    0.000    0.049    0.000 decode_asn1.py:260(_decode_certificate_policies)\n",
       "    15077    0.013    0.000    0.059    0.000 _policybase.py:311(header_fetch_parse)\n",
       "     1012    0.013    0.000    0.053    0.000 adapters.py:113(__init__)\n",
       "      483    0.013    0.000    0.068    0.000 _parser.py:668(_parse)\n",
       "     1543    0.013    0.000    0.073    0.000 cookies.py:508(cookiejar_from_dict)\n",
       "     3192    0.013    0.000    0.030    0.000 cookiejar.py:677(reach)\n",
       "    15611    0.013    0.000    0.022    0.000 utils.py:51(_has_surrogates)\n",
       "    15150    0.013    0.000    0.025    0.000 _oid.py:58(__hash__)\n",
       "     1070    0.013    0.000    0.013    0.000 {method 'readlines' of '_io._IOBase' objects}\n",
       "      532    0.013    0.000    0.054    0.000 models.py:596(__init__)\n",
       "      535    0.013    0.000    1.188    0.002 client.py:1231(_send_request)\n",
       "     4932    0.013    0.000    0.095    0.000 _lxml.py:227(comment)\n",
       "      505    0.012    0.000    0.056    0.000 models.py:441(prepare_headers)\n",
       "     2371    0.012    0.000    0.024    0.000 element.py:1662(__init__)\n",
       "      534    0.012    0.000    0.027    0.000 response.py:160(__init__)\n",
       "     1064    0.012    0.000    0.070    0.000 utils.py:767(select_proxy)\n",
       "     1039    0.012    0.000    0.038    0.000 url.py:132(parse_url)\n",
       "     2608    0.012    0.000    0.072    0.000 {method 'read' of '_io.BufferedReader' objects}\n",
       "      505    0.012    0.000    0.083    0.000 models.py:355(prepare_url)\n",
       "     2074    0.012    0.000    0.300    0.000 cookiejar.py:1553(_cookies_from_attrs_set)\n",
       "      532    0.012    0.000    0.016    0.000 makefile.py:14(backport_makefile)\n",
       "     3247    0.012    0.000    0.020    0.000 cookiejar.py:1197(vals_sorted_by_key)\n",
       "     3111    0.012    0.000    0.029    0.000 cookiejar.py:72(_timegm)\n",
       "      505    0.012    0.000    0.123    0.000 pyopenssl.py:389(__init__)\n",
       "    37379    0.012    0.000    0.012    0.000 {method 'strip' of 'str' objects}\n",
       "      505    0.012    0.000    1.144    0.002 pyopenssl.py:345(getpeercert)\n",
       "      532    0.012    0.000    0.763    0.001 adapters.py:255(build_response)\n",
       "      505    0.012    0.000    0.012    0.000 {built-in method today}\n",
       "     9967    0.012    0.000    0.032    0.000 feedparser.py:128(__next__)\n",
       "     2020    0.012    0.000    0.018    0.000 inspect.py:2465(__init__)\n",
       "     2020    0.011    0.000    0.027    0.000 enum.py:809(__or__)\n",
       "      534    0.011    0.000    0.043    0.000 _collections.py:225(extend)\n",
       "    36360    0.011    0.000    0.011    0.000 intranges.py:34(_decode_range)\n",
       "     2020    0.011    0.000    0.017    0.000 crypto.py:1090(_from_raw_x509_ptr)\n",
       "     1010    0.011    0.000    0.043    0.000 SSL.py:303(__init__)\n",
       "     6781    0.011    0.000    0.011    0.000 {method 'rstrip' of 'bytes' objects}\n",
       "     2431    0.011    0.000  110.234    0.045 pyopenssl.py:292(recv_into)\n",
       "     1515    0.011    0.000    0.042    0.000 decode_asn1.py:81(_decode_general_names)\n",
       "     1010    0.011    0.000    0.019    0.000 functools.py:37(update_wrapper)\n",
       "     2140    0.011    0.000    0.045    0.000 message.py:564(get_content_type)\n",
       "     3859    0.011    0.000    0.038    0.000 _collections_abc.py:664(__contains__)\n",
       " 1010/505    0.011    0.000    0.063    0.000 inspect.py:2196(_signature_from_callable)\n",
       "      559    0.011    0.000    0.611    0.001 utils.py:694(should_bypass_proxies)\n",
       "    24310    0.010    0.000    0.010    0.000 {method 'partition' of 'str' objects}\n",
       "     2128    0.010    0.000    0.719    0.000 genericpath.py:16(exists)\n",
       "    36059    0.010    0.000    0.010    0.000 parse.py:98(_noop)\n",
       "     1932    0.010    0.000    0.025    0.000 _strptime.py:318(_strptime)\n",
       "     7575    0.010    0.000    0.248    0.000 core.py:266(alabel)\n",
       "      505    0.010    0.000   13.416    0.027 pyopenssl.py:438(wrap_socket)\n",
       "     2202    0.010    0.000    0.183    0.000 response.py:593(_update_chunk_length)\n",
       "    44858    0.010    0.000    0.010    0.000 xmlerror.pxi:585(_getThreadErrorLog)\n",
       "     8359    0.010    0.000    0.010    0.000 {method 'setdefault' of 'collections.OrderedDict' objects}\n",
       "      505    0.010    0.000    0.048    0.000 decode_asn1.py:380(_decode_authority_information_access)\n",
       "     1650    0.010    0.000    0.075    0.000 cookies.py:37(__init__)\n",
       "    32825    0.010    0.000    0.010    0.000 intranges.py:31(_encode_range)\n",
       "     1040    0.010    0.000    0.023    0.000 client.py:403(close)\n",
       "      532    0.010    0.000    0.041    0.000 cookiejar.py:1331(add_cookie_header)\n",
       "     1118    0.010    0.000    1.188    0.001 cookies.py:118(extract_cookies_to_jar)\n",
       "     3468    0.010    0.000    0.010    0.000 {built-in method _openssl.SSL_get_error}\n",
       "      505    0.010    0.000    0.031    0.000 SSL.py:1534(__init__)\n",
       "     1010    0.010    0.000    0.056    0.000 SSL.py:1083(set_verify)\n",
       "    11674    0.009    0.000    0.014    0.000 threading.py:240(__enter__)\n",
       "      514    0.009    0.000    1.332    0.003 socket.py:731(getaddrinfo)\n",
       "     2106    0.009    0.000    0.013    0.000 timeout.py:93(__init__)\n",
       "      505    0.009    0.000    0.040    0.000 connectionpool.py:807(_new_conn)\n",
       "      559    0.009    0.000    0.550    0.001 utils.py:86(proxy_bypass)\n",
       "     1013    0.009    0.000    0.023    0.000 poolmanager.py:152(__init__)\n",
       "     8362    0.009    0.000    0.011    0.000 message.py:479(set_raw)\n",
       "    44858    0.009    0.000    0.009    0.000 xmlerror.pxi:69(_setError)\n",
       "      505    0.009    0.000    1.041    0.002 sessions.py:690(merge_environment_settings)\n",
       "     3379    0.009    0.000    0.015    0.000 _parser.py:83(get_token)\n",
       "      535    0.009    0.000    0.328    0.001 parser.py:60(parsestr)\n",
       "      508    0.009    0.000    0.009    0.000 {method 'setsockopt' of '_socket.socket' objects}\n",
       "      756    0.009    0.000    0.027    0.000 punkt.py:1325(_slices_from_text)\n",
       "     1697    0.009    0.000    0.103    0.000 response.py:607(_handle_chunk)\n",
       "     3111    0.009    0.000    0.013    0.000 cookiejar.py:999(set_ok_domain)\n",
       "     2525    0.009    0.000    0.287    0.000 pyopenssl.py:170(idna_encode)\n",
       "     1010    0.009    0.000    0.009    0.000 {method 'callback' of 'CompiledFFI' objects}\n",
       "    44858    0.009    0.000    0.009    0.000 xmlerror.pxi:493(receive)\n",
       "      535    0.009    0.000    0.016    0.000 feedparser.py:139(__init__)\n",
       "     2075    0.008    0.000    0.008    0.000 cookiejar.py:870(__init__)\n",
       "      505    0.008    0.000    0.023    0.000 crypto.py:585(__getattr__)\n",
       "      505    0.008    0.000    0.095    0.000 decode_asn1.py:614(_decode_precert_signed_certificate_timestamps)\n",
       "     3111    0.008    0.000    0.246    0.000 cookiejar.py:967(set_ok_verifiability)\n",
       "      479    0.008    0.000    0.009    0.000 datetimes.py:606(<lambda>)\n",
       "    11169    0.008    0.000    0.015    0.000 threading.py:255(_is_owned)\n",
       "     1093    0.008    0.000    0.009    0.000 contextlib.py:81(__init__)\n",
       "      534    0.008    0.000    0.035    0.000 message.py:459(<listcomp>)\n",
       "     4229    0.008    0.000    0.012    0.000 _collections_abc.py:676(items)\n",
       "     5053    0.008    0.000    0.037    0.000 {built-in method builtins.all}\n",
       "      505    0.008    0.000    0.012    0.000 ssl_.py:360(is_ipaddress)\n",
       "     3192    0.008    0.000    0.013    0.000 copy.py:66(copy)\n",
       "    12949    0.008    0.000    0.008    0.000 structures.py:58(<genexpr>)\n",
       "    11674    0.008    0.000    0.011    0.000 threading.py:243(__exit__)\n",
       "      532    0.008    0.000    0.538    0.001 adapters.py:203(cert_verify)\n",
       "     9090    0.008    0.000    0.008    0.000 {method 'new' of 'CompiledFFI' objects}\n",
       "     2579    0.008    0.000    0.009    0.000 structures.py:57(__iter__)\n",
       "     8080    0.008    0.000    0.008    0.000 {method 'gc' of 'CompiledFFI' objects}\n",
       "     3192    0.008    0.000    0.052    0.000 cookies.py:45(get_host)\n",
       "     3219    0.007    0.000    0.119    0.000 cookiejar.py:622(eff_request_host)\n",
       "     1047    0.007    0.000    0.007    0.000 {method 'settimeout' of '_socket.socket' objects}\n",
       "     1039    0.007    0.000    0.012    0.000 url.py:22(__new__)\n",
       "      535    0.007    0.000    0.319    0.001 parser.py:42(parse)\n",
       "      505    0.007    0.000    0.007    0.000 {built-in method _openssl.SSL_new}\n",
       "3733/3301    0.007    0.000    0.027    0.000 cookiejar.py:1201(deepvalues)\n",
       "      535    0.007    0.000    0.019    0.000 client.py:1061(putrequest)\n",
       "      505    0.007    0.000    0.009    0.000 __init__.py:42(lookup)\n",
       "      505    0.007    0.000    0.090    0.000 decode_asn1.py:502(_decode_dist_points)\n",
       "     1012    0.007    0.000    0.007    0.000 retry.py:159(__init__)\n",
       "     1012    0.007    0.000    0.030    0.000 adapters.py:146(init_poolmanager)\n",
       "      534    0.007    0.000    0.053    0.000 _collections.py:136(__init__)\n",
       "      559    0.007    0.000    0.076    0.000 utils.py:47(proxy_bypass_registry)\n",
       "    11674    0.007    0.000    0.010    0.000 queue.py:14(_qsize)\n",
       "     5050    0.007    0.000    0.008    0.000 extensions.py:1127(__init__)\n",
       "    44858    0.007    0.000    0.007    0.000 xmlerror.pxi:473(receive)\n",
       "     2347    0.007    0.000    1.825    0.001 element.py:1361(find_all)\n",
       "     3111    0.007    0.000    0.114    0.000 cookiejar.py:988(set_ok_path)\n",
       "      505    0.007    0.000    0.092    0.000 parsertarget.pxi:29(__cinit__)\n",
       "     1608    0.007    0.000    0.318    0.000 element.py:1103(__getattr__)\n",
       "     1039    0.007    0.000    0.010    0.000 url.py:99(split_first)\n",
       "    12095    0.007    0.000    0.007    0.000 {method 'lstrip' of 'str' objects}\n",
       "     2206    0.007    0.000    1.772    0.001 element.py:1350(find)\n",
       "     1699    0.007    0.000    0.681    0.000 response.py:318(_decode)\n",
       "    32825    0.007    0.000    0.007    0.000 {built-in method unicodedata.bidirectional}\n",
       "      481    0.007    0.000    0.189    0.000 datetimes.py:106(to_datetime)\n",
       "      506    0.007    0.000    0.027    0.000 queue.py:33(__init__)\n",
       "     1012    0.007    0.000    0.518    0.001 _collections.py:87(clear)\n",
       "    11184    0.007    0.000    0.007    0.000 {method 'acquire' of '_thread.lock' objects}\n",
       "     2431    0.007    0.000    0.007    0.000 {built-in method allocator}\n",
       "      505    0.007    0.000    0.010    0.000 inspect.py:2748(__init__)\n",
       "     1092    0.007    0.000    0.011    0.000 parse.py:460(urlunsplit)\n",
       "      505    0.007    0.000    0.270    0.001 models.py:307(prepare)\n",
       "      506    0.007    0.000    0.039    0.000 utils.py:802(default_headers)\n",
       "      506    0.006    0.000    0.014    0.000 connection.py:103(__init__)\n",
       "     5585    0.006    0.000    0.009    0.000 queue.py:17(_put)\n",
       "      535    0.006    0.000    0.025    0.000 feedparser.py:101(push)\n",
       "     3247    0.006    0.000    0.006    0.000 {built-in method builtins.sorted}\n",
       "      534    0.006    0.000    0.160    0.000 poolmanager.py:243(connection_from_pool_key)\n",
       "      505    0.006    0.000    1.088    0.002 pyopenssl.py:195(get_subj_alt_name)\n",
       "     5374    0.006    0.000    0.006    0.000 {method 'acquire' of '_thread.RLock' objects}\n",
       "      505    0.006    0.000    0.029    0.000 decode_asn1.py:483(_decode_extended_key_usage)\n",
       "     3058    0.006    0.000    0.018    0.000 socket.py:97(_intenum_converter)\n",
       "      505    0.006    0.000    0.030    0.000 dammit.py:299(find_declared_encoding)\n",
       "     1010    0.006    0.000    0.053    0.000 _lxml.py:88(prepare_markup)\n",
       "     1092    0.006    0.000    0.024    0.000 parse.py:449(urlunparse)\n",
       "      507    0.006    0.000    0.007    0.000 client.py:827(__init__)\n",
       "     5050    0.006    0.000    0.006    0.000 {built-in method _openssl.X509_get_ext}\n",
       "     7875    0.006    0.000    0.006    0.000 _collections.py:181(__iter__)\n",
       "      532    0.006    0.000    0.273    0.001 adapters.py:292(get_connection)\n",
       "      505    0.006    0.000    0.006    0.000 parser.pxi:924(_newPushParserCtxt)\n",
       "     2032    0.006    0.000    0.008    0.000 message.py:29(_splitparam)\n",
       "      505    0.006    0.000   23.706    0.047 ssl_.py:291(ssl_wrap_socket)\n",
       "     3111    0.006    0.000    0.010    0.000 cookiejar.py:445(strip_quotes)\n",
       "     2228    0.006    0.000    1.092    0.000 response.py:473(stream)\n",
       "     6565    0.006    0.000    0.006    0.000 {built-in method _abc._abc_subclasscheck}\n",
       "     1064    0.006    0.000    1.122    0.001 models.py:815(content)\n",
       "      505    0.006    0.000    0.006    0.000 {built-in method zlib.decompressobj}\n",
       "      505    0.006    0.000    0.021    0.000 _lxml.py:61(parser_for)\n",
       "     2228    0.006    0.000    1.099    0.000 models.py:746(generate)\n",
       "     4932    0.006    0.000    0.109    0.000 saxparser.pxi:580(_handleSaxTargetComment)\n",
       "      505    0.006    0.000    0.008    0.000 parser.pxi:798(__init__)\n",
       "     4545    0.006    0.000    0.006    0.000 decode_asn1.py:724(_asn1_string_to_bytes)\n",
       "     7575    0.006    0.000    0.009    0.000 core.py:127(check_initial_combiner)\n",
       "     9544    0.006    0.000    0.006    0.000 {method 'rstrip' of 'str' objects}\n",
       "      505    0.006    0.000    0.051    0.000 _lxml.py:222(doctype)\n",
       "      505    0.006    0.000    0.009    0.000 parser.pxi:521(__cinit__)\n",
       "     3618    0.005    0.000    0.005    0.000 {method 'groups' of 're.Match' objects}\n",
       "     7575    0.005    0.000    0.009    0.000 core.py:143(check_nfc)\n",
       "     1010    0.005    0.000    0.014    0.000 extensions.py:1182(get_values_for_type)\n",
       "      535    0.005    0.000  112.208    0.210 client.py:1277(getresponse)\n",
       "     7575    0.005    0.000    0.005    0.000 core.py:134(check_hyphen_ok)\n",
       "      505    0.005    0.000    0.023    0.000 utils.py:906(get_auth_from_url)\n",
       "      534    0.005    0.000    0.020    0.000 response.py:38(assert_header_parsing)\n",
       "      505    0.005    0.000    0.008    0.000 decode_asn1.py:355(_decode_authority_key_identifier)\n",
       "      505    0.005    0.000    0.005    0.000 parser.pxi:1409(_htmlCtxtResetPush)\n",
       "      514    0.005    0.000    0.008    0.000 idna.py:147(encode)\n",
       "      535    0.005    0.000    0.005    0.000 feedparser.py:53(__init__)\n",
       "      505    0.005    0.000    0.014    0.000 models.py:452(prepare_body)\n",
       "     2347    0.005    0.000    0.010    0.000 element.py:1878(__init__)\n",
       "     2020    0.005    0.000    0.014    0.000 utils.py:927(check_header_validity)\n",
       "      532    0.005    0.000    0.058    0.000 adapters.py:329(request_url)\n",
       "      505    0.005    0.000   30.357    0.060 _lxml.py:250(feed)\n",
       "      505    0.005    0.000    0.008    0.000 utils.py:450(_parse_content_type_header)\n",
       "      532    0.005    0.000    0.005    0.000 {method 'Close' of 'PyHKEY' objects}\n",
       "  586/532    0.005    0.000    3.881    0.007 sessions.py:143(resolve_redirects)\n",
       "     2525    0.005    0.000    0.293    0.000 pyopenssl.py:157(_dnsname_to_stdlib)\n",
       "      507    0.005    0.000    0.011    0.000 response.py:336(_flush_decoder)\n",
       "      507    0.005    0.000    6.850    0.014 connection.py:145(_new_conn)\n",
       "     1543    0.005    0.000    0.021    0.000 cookies.py:521(<listcomp>)\n",
       "     1010    0.005    0.000    0.038    0.000 dammit.py:240(encodings)\n",
       "    11674    0.005    0.000    0.005    0.000 {method '__enter__' of '_thread.lock' objects}\n",
       "     1093    0.005    0.000    0.034    0.000 contextlib.py:116(__exit__)\n",
       "     5555    0.005    0.000    0.014    0.000 {method 'add' of 'set' objects}\n",
       "     3111    0.005    0.000    0.005    0.000 cookiejar.py:952(set_ok_version)\n",
       "      505    0.005    0.000    0.007    0.000 models.py:226(__init__)\n",
       "     5050    0.005    0.000    0.011    0.000 decode_asn1.py:835(<lambda>)\n",
       "      535    0.005    0.000    0.021    0.000 client.py:226(__init__)\n",
       "      481    0.005    0.000  142.661    0.297 sessions.py:537(get)\n",
       "      505    0.005    0.000    0.013    0.000 models.py:82(_encode_params)\n",
       "     1012    0.005    0.000    0.008    0.000 sessions.py:738(mount)\n",
       "      532    0.005    0.000    0.097    0.000 SSL.py:1705(send)\n",
       "     1070    0.005    0.000    0.241    0.000 feedparser.py:178(_call_parse)\n",
       "     1037    0.005    0.000    0.021    0.000 cookies.py:529(merge_cookies)\n",
       "     1013    0.005    0.000    0.011    0.000 _collections.py:44(__init__)\n",
       "      506    0.005    0.000    0.143    0.000 poolmanager.py:171(_new_pool)\n",
       "      505    0.005    0.000    0.016    0.000 __init__.py:349(reset)\n",
       "    18863    0.005    0.000    0.005    0.000 {built-in method builtins.setattr}\n",
       "     3139    0.005    0.000    0.005    0.000 cookiejar.py:1729(__iter__)\n",
       "      505    0.005    0.000   13.364    0.026 SSL.py:1898(do_handshake)\n",
       "     1449    0.004    0.000    0.006    0.000 _parser.py:433(append)\n",
       "      532    0.004    0.000    0.044    0.000 request.py:2617(getproxies_registry)\n",
       "    44332    0.004    0.000    0.004    0.000 xmlerror.pxi:63(__dealloc__)\n",
       "      532    0.004    0.000   32.095    0.060 connectionpool.py:831(_validate_conn)\n",
       "      505    0.004    0.000    0.019    0.000 connection.py:228(__init__)\n",
       "     1515    0.004    0.000    0.004    0.000 {built-in method _socket.inet_pton}\n",
       "     8088    0.004    0.000    0.004    0.000 cookiejar.py:43(_debug)\n",
       "      534    0.004    0.000    0.195    0.000 poolmanager.py:207(connection_from_host)\n",
       "     3468    0.004    0.000    0.004    0.000 SSL.py:284(raise_if_problem)\n",
       "      532    0.004    0.000    0.007    0.000 models.py:290(__init__)\n",
       "     3147    0.004    0.000    0.004    0.000 {built-in method builtins.min}\n",
       "     1932    0.004    0.000    0.004    0.000 {built-in method _locale.setlocale}\n",
       "      505    0.004    0.000    0.010    0.000 crypto.py:1098(to_cryptography)\n",
       "      532    0.004    0.000    0.087    0.000 cookies.py:135(get_cookie_header)\n",
       "    32825    0.004    0.000    0.004    0.000 {built-in method builtins.ord}\n",
       "      505    0.004    0.000    0.008    0.000 dammit.py:218(__init__)\n",
       "     5050    0.004    0.000    0.004    0.000 {built-in method _openssl.X509_EXTENSION_get_critical}\n",
       "     1543    0.004    0.000    0.007    0.000 hooks.py:17(default_hooks)\n",
       "      505    0.004    0.000    0.004    0.000 parser.pxi:556(_resetParserContext)\n",
       "     7698    0.004    0.000    0.004    0.000 {method 'read' of '_io.StringIO' objects}\n",
       "      539    0.004    0.000    0.004    0.000 {built-in method numpy.core.multiarray.array}\n",
       "     7575    0.004    0.000    0.005    0.000 core.py:53(valid_label_length)\n",
       "     4932    0.004    0.000    0.099    0.000 parsertarget.pxi:96(_handleSaxComment)\n",
       "     6096    0.004    0.000    0.004    0.000 {method 'endswith' of 'str' objects}\n",
       "      534    0.004    0.000    0.187    0.000 poolmanager.py:230(connection_from_context)\n",
       "      506    0.004    0.000    0.004    0.000 utils.py:793(default_user_agent)\n",
       "     3192    0.004    0.000    0.005    0.000 cookies.py:84(unverifiable)\n",
       "      532    0.004    0.000    0.215    0.000 poolmanager.py:267(connection_from_url)\n",
       "      507    0.004    0.000    0.004    0.000 {built-in method _thread.allocate_lock}\n",
       "     3111    0.004    0.000    0.004    0.000 {method 'index' of 'list' objects}\n",
       "     3194    0.004    0.000    0.004    0.000 {method 'rfind' of 'str' objects}\n",
       "      505    0.004    0.000    0.016    0.000 ssl.py:276(match_hostname)\n",
       "      505    0.004    0.000    0.007    0.000 ssl.py:240(_inet_paton)\n",
       "     1070    0.004    0.000    0.004    0.000 message.py:120(__init__)\n",
       "      534    0.004    0.000    0.012    0.000 message.py:213(get_payload)\n",
       "      479    0.004    0.000    0.015    0.000 datetimes.py:329(__new__)\n",
       "    20200    0.004    0.000    0.004    0.000 binding.py:54(_openssl_assert)\n",
       "    15162    0.004    0.000    0.004    0.000 {built-in method builtins.hash}\n",
       "      505    0.004    0.000    0.088    0.000 SSL.py:1175(set_cipher_list)\n",
       "     1577    0.004    0.000    0.004    0.000 {method 'format' of 'str' objects}\n",
       "      505    0.004    0.000    0.009    0.000 decode_asn1.py:396(_decode_key_usage)\n",
       "     2704    0.004    0.000    0.004    0.000 {method 'fullmatch' of 're.Pattern' objects}\n",
       "      539    0.004    0.000    0.006    0.000 parse.py:386(_splitnetloc)\n",
       "     1064    0.004    0.000    0.014    0.000 cookies.py:348(update)\n",
       "     1010    0.004    0.000    0.005    0.000 crypto.py:549(__setattr__)\n",
       "        4    0.004    0.001    0.004    0.001 {method 'sendall' of '_socket.socket' objects}\n",
       "     2434    0.004    0.000    0.006    0.000 {method '_checkReadable' of '_io._IOBase' objects}\n",
       "     5196    0.004    0.000    0.004    0.000 {method 'copy' of 'dict' objects}\n",
       "      481    0.004    0.000    0.161    0.000 datetimes.py:276(_convert_listlike)\n",
       "      535    0.004    0.000    0.004    0.000 socket.py:563(__init__)\n",
       "      505    0.004    0.000    0.094    0.000 pyopenssl.py:418(set_ciphers)\n",
       "     3192    0.004    0.000    0.056    0.000 cookies.py:48(get_origin_req_host)\n",
       "     6318    0.004    0.000    0.004    0.000 timeout.py:103(_validate_timeout)\n",
       "      497    0.004    0.000    0.004    0.000 {built-in method pandas._libs.algos.ensure_object}\n",
       "      505    0.004    0.000    0.096    0.000 decode_asn1.py:597(_decode_crl_distribution_points)\n",
       "     1037    0.004    0.000    0.004    0.000 cookiejar.py:1562(_process_rfc2109_cookies)\n",
       "     3192    0.004    0.000    0.059    0.000 cookies.py:88(origin_req_host)\n",
       "      964    0.004    0.000    0.014    0.000 _parser.py:877(_parse_numeric_token)\n",
       "     5050    0.004    0.000    0.004    0.000 {built-in method _openssl.X509_EXTENSION_get_object}\n",
       "     7575    0.004    0.000    0.004    0.000 {method 'startswith' of 'bytes' objects}\n",
       "     5050    0.004    0.000    0.004    0.000 {method 'cast' of 'CompiledFFI' objects}\n",
       "      532    0.004    0.000    0.010    0.000 cookiejar.py:1710(clear_expired_cookies)\n",
       "      505    0.004    0.000    0.010    0.000 crypto.py:1419(_get_name)\n",
       "      505    0.004    0.000    0.137    0.000 connectionpool.py:757(__init__)\n",
       "     4173    0.004    0.000    0.004    0.000 {method 'count' of 'str' objects}\n",
       "     1605    0.004    0.000    0.033    0.000 message.py:588(get_content_maintype)\n",
       "      117    0.004    0.000    0.004    0.000 <ipython-input-9-81aa55e67a15>:83(<listcomp>)\n",
       "      505    0.003    0.000    0.007    0.000 weakref.py:165(__setitem__)\n",
       "      506    0.003    0.000    0.004    0.000 _collections.py:58(__setitem__)\n",
       "     3111    0.003    0.000    0.003    0.000 cookiejar.py:128(offset_from_tz_string)\n",
       "     7575    0.003    0.000    0.003    0.000 {built-in method unicodedata.normalize}\n",
       "      532    0.003    0.000    0.005    0.000 utils.py:565(unquote_unreserved)\n",
       "     2611    0.003    0.000    0.004    0.000 {method 'extend' of 'list' objects}\n",
       "      535    0.003    0.000    0.007    0.000 feedparser.py:70(close)\n",
       "     1093    0.003    0.000    0.013    0.000 contextlib.py:237(helper)\n",
       "      505    0.003    0.000    0.011    0.000 parser.pxi:1669(__init__)\n",
       "      532    0.003    0.000    0.013    0.000 utils.py:475(get_encoding_from_headers)\n",
       "     1928    0.003    0.000    0.003    0.000 {method 'split' of 'bytes' objects}\n",
       "     1010    0.003    0.000    0.005    0.000 ssl_.py:190(resolve_cert_reqs)\n",
       "     1093    0.003    0.000    0.006    0.000 contextlib.py:107(__enter__)\n",
       "      505    0.003    0.000    0.051    0.000 decode_asn1.py:429(_decode_subject_alt_name)\n",
       "     1068    0.003    0.000    0.008    0.000 timeout.py:156(clone)\n",
       "      534    0.003    0.000    0.018    0.000 response.py:303(_init_decoder)\n",
       "     6273    0.003    0.000    0.003    0.000 {method 'pop' of 'dict' objects}\n",
       "      505    0.003    0.000    0.010    0.000 connectionpool.py:782(_prepare_conn)\n",
       "      505    0.003    0.000   10.269    0.020 SSL.py:751(load_verify_locations)\n",
       "      559    0.003    0.000    0.015    0.000 utils.py:702(<lambda>)\n",
       "      505    0.003    0.000    0.110    0.000 parser.pxi:868(_createContext)\n",
       "     5374    0.003    0.000    0.003    0.000 {method 'release' of '_thread.RLock' objects}\n",
       "     6565    0.003    0.000    0.009    0.000 abc.py:141(__subclasscheck__)\n",
       "     3535    0.003    0.000    0.007    0.000 extensions.py:1188(<genexpr>)\n",
       "      532    0.003    0.000    1.049    0.002 utils.py:755(get_environ_proxies)\n",
       "     1010    0.003    0.000    0.003    0.000 functools.py:67(wraps)\n",
       "     3535    0.003    0.000    0.003    0.000 {built-in method _openssl.sk_GENERAL_NAME_value}\n",
       "     2969    0.003    0.000    0.003    0.000 socket.py:614(readable)\n",
       "      505    0.003    0.000    0.003    0.000 {method 'flush' of 'zlib.Decompress' objects}\n",
       "      505    0.003    0.000    0.009    0.000 response.py:64(__init__)\n",
       "      535    0.003    0.000    0.005    0.000 feedparser.py:197(_new_message)\n",
       "      505    0.003    0.000    0.005    0.000 extensions.py:265(__init__)\n",
       "    11674    0.003    0.000    0.003    0.000 {method '__exit__' of '_thread.lock' objects}\n",
       "      509    0.003    0.000    0.040    0.000 client.py:934(close)\n",
       "     1010    0.003    0.000    0.007    0.000 extensions.py:301(__init__)\n",
       "      506    0.003    0.000    0.003    0.000 parse.py:599(unquote)\n",
       "      535    0.003    0.000    0.006    0.000 socket.py:652(close)\n",
       "     1045    0.003    0.000    0.005    0.000 __init__.py:1361(debug)\n",
       "      535    0.003    0.000    0.003    0.000 {method 'write' of '_io.StringIO' objects}\n",
       "      535    0.003    0.000    0.035    0.000 feedparser.py:184(close)\n",
       "     7575    0.003    0.000    0.003    0.000 {built-in method unicodedata.category}\n",
       "      505    0.003    0.000    0.008    0.000 models.py:175(register_hook)\n",
       "     1039    0.003    0.000    0.004    0.000 connection.py:116(host)\n",
       "     1010    0.003    0.000    0.059    0.000 pyopenssl.py:408(verify_mode)\n",
       "      534    0.003    0.000    0.059    0.000 connectionpool.py:212(_get_conn)\n",
       "      505    0.003    0.000    0.096    0.000 parsertarget.pxi:107(_setTarget)\n",
       "      534    0.003    0.000    0.007    0.000 response.py:255(_init_length)\n",
       "     5584    0.003    0.000    0.004    0.000 queue.py:20(_get)\n",
       "      505    0.003    0.000    0.509    0.001 poolmanager.py:156(<lambda>)\n",
       "      505    0.003    0.000    0.035    0.000 pyopenssl.py:335(close)\n",
       "     1515    0.003    0.000    0.013    0.000 sessions.py:73(<listcomp>)\n",
       "     1012    0.003    0.000    0.004    0.000 sessions.py:744(<listcomp>)\n",
       "      461    0.003    0.000    0.005    0.000 element.py:599(<genexpr>)\n",
       "     1010    0.003    0.000    0.005    0.000 utils.py:126(__getattr__)\n",
       "      505    0.003    0.000    0.003    0.000 dammit.py:273(strip_byte_order_mark)\n",
       "     4545    0.003    0.000    0.003    0.000 {built-in method _openssl.ASN1_BIT_STRING_get_bit}\n",
       "      532    0.003    0.000    0.090    0.000 models.py:556(prepare_cookies)\n",
       "      506    0.003    0.000    0.005    0.000 connectionpool.py:64(__init__)\n",
       "     1068    0.003    0.000    0.011    0.000 connectionpool.py:290(_get_timeout)\n",
       "      505    0.003    0.000    0.003    0.000 {built-in method _openssl.SSLv23_method}\n",
       "      505    0.003    0.000    0.003    0.000 _lxml.py:70(__init__)\n",
       "     2773    0.003    0.000    0.003    0.000 element.py:341(_last_descendant)\n",
       "     1068    0.003    0.000    0.026    0.000 response.py:347(_error_catcher)\n",
       "1971/1970    0.003    0.000    0.004    0.000 common.py:1835(_get_dtype_type)\n",
       "      483    0.003    0.000    0.003    0.000 _parser.py:62(__init__)\n",
       "     1623    0.003    0.000    0.003    0.000 {method 'update' of 'dict' objects}\n",
       "     4835    0.003    0.000    0.003    0.000 {method 'rpartition' of 'str' objects}\n",
       "      479    0.003    0.000    0.007    0.000 datetimes.py:627(_simple_new)\n",
       "     2525    0.003    0.000    0.004    0.000 inspect.py:2797(<genexpr>)\n",
       "      532    0.003    0.000    0.007    0.000 models.py:61(path_url)\n",
       "      532    0.003    0.000    0.004    0.000 sessions.py:719(get_adapter)\n",
       "     1575    0.003    0.000    0.006    0.000 {function SocketIO.close at 0x00000000036530D0}\n",
       "     1012    0.003    0.000    0.003    0.000 adapters.py:58(__init__)\n",
       "     2525    0.003    0.000    0.003    0.000 general_name.py:138(_init_without_validation)\n",
       "      505    0.003    0.000    0.004    0.000 decode_asn1.py:345(_decode_subject_key_identifier)\n",
       "      505    0.003    0.000    0.295    0.001 pyopenssl.py:236(<listcomp>)\n",
       "     1515    0.003    0.000    0.003    0.000 weakref.py:134(__getitem__)\n",
       "      534    0.003    0.000    0.004    0.000 _collections.py:51(__getitem__)\n",
       "     8897    0.003    0.000    0.003    0.000 {method 'popleft' of 'collections.deque' objects}\n",
       "      505    0.003    0.000    0.003    0.000 parser.pxi:1193(_initSaxDocument)\n",
       "     1010    0.003    0.000    0.006    0.000 pyopenssl.py:404(verify_mode)\n",
       "      534    0.003    0.000    0.021    0.000 connectionpool.py:250(_put_conn)\n",
       "     3111    0.003    0.000    0.003    0.000 cookiejar.py:979(set_ok_name)\n",
       "     2579    0.003    0.000    0.003    0.000 _internal_utils.py:14(to_native_string)\n",
       "      505    0.003    0.000    0.006    0.000 connection.py:274(set_cert)\n",
       "     2701    0.003    0.000    0.005    0.000 client.py:1233(<genexpr>)\n",
       "      505    0.003    0.000    0.004    0.000 SSL.py:1679(set_tlsext_host_name)\n",
       "        1    0.003    0.003    9.655    9.655 <ipython-input-10-19c3509cb48d>:49(get_urls)\n",
       "      505    0.003    0.000    0.753    0.001 x509.py:133(extensions)\n",
       "     1012    0.003    0.000    0.523    0.001 adapters.py:319(close)\n",
       "      505    0.003    0.000    0.005    0.000 decode_asn1.py:329(_decode_basic_constraints)\n",
       "      964    0.003    0.000    0.003    0.000 _parser.py:1242(_to_decimal)\n",
       "      505    0.003    0.000   10.272    0.020 pyopenssl.py:423(load_verify_locations)\n",
       "     1010    0.003    0.000    0.124    0.000 parser.pxi:852(_getPushParserContext)\n",
       "     1010    0.003    0.000    0.003    0.000 {built-in method _openssl.SSL_CTX_set_verify}\n",
       "      532    0.003    0.000    0.019    0.000 utils.py:589(requote_uri)\n",
       "      505    0.003    0.000    0.017    0.000 extensions.py:1166(__init__)\n",
       "     3138    0.003    0.000    0.003    0.000 cookiejar.py:910(is_blocked)\n",
       "     1519    0.002    0.000    0.002    0.000 request.py:41(__init__)\n",
       "      505    0.002    0.000    0.009    0.000 SSL.py:2191(get_peer_certificate)\n",
       "      532    0.002    0.000    0.003    0.000 models.py:729(iter_content)\n",
       "     2607    0.002    0.000    0.010    0.000 <frozen importlib._bootstrap>:1009(_handle_fromlist)\n",
       "      483    0.002    0.000    0.022    0.000 _parser.py:205(split)\n",
       "      532    0.002    0.000    0.101    0.000 pyopenssl.py:325(sendall)\n",
       "      479    0.002    0.000    0.012    0.000 datetimelike.py:389(__getitem__)\n",
       "      535    0.002    0.000    0.258    0.000 feedparser.py:173(feed)\n",
       "     1543    0.002    0.000    0.002    0.000 hooks.py:18(<dictcomp>)\n",
       "     2184    0.002    0.000    0.002    0.000 {built-in method time.time}\n",
       "      532    0.002    0.000    0.004    0.000 cookiejar.py:1272(_cookie_attrs)\n",
       "      505    0.002    0.000    0.004    0.000 utils.py:107(super_len)\n",
       "     4656    0.002    0.000    0.002    0.000 {method 'values' of 'collections.OrderedDict' objects}\n",
       "      505    0.002    0.000   30.362    0.060 __init__.py:339(_feed)\n",
       "     1515    0.002    0.000    0.004    0.000 ssl.py:191(_dnsname_match)\n",
       "      535    0.002    0.000    0.008    0.000 {method 'close' of '_io.BufferedReader' objects}\n",
       "     5600    0.002    0.000    0.002    0.000 {method 'append' of 'collections.deque' objects}\n",
       "      479    0.002    0.000    0.005    0.000 _parser.py:461(_resolve_from_stridxs)\n",
       "     2020    0.002    0.000    0.003    0.000 dammit.py:230(_usable)\n",
       "      507    0.002    0.000    0.011    0.000 connection.py:85(_set_socket_options)\n",
       "     3239    0.002    0.000    0.003    0.000 client.py:986(_output)\n",
       "     4040    0.002    0.000    0.002    0.000 crypto.py:205(__init__)\n",
       "     2020    0.002    0.000    0.003    0.000 general_name.py:183(_init_without_validation)\n",
       "      505    0.002    0.000    0.003    0.000 models.py:339(prepare_method)\n",
       "     7482    0.002    0.000    0.002    0.000 {built-in method builtins.callable}\n",
       "     1010    0.002    0.000    0.003    0.000 structures.py:60(__len__)\n",
       "      505    0.002    0.000    0.003    0.000 parse.py:832(urlencode)\n",
       "      561    0.002    0.000    0.023    0.000 response.py:224(release_conn)\n",
       "      505    0.002    0.000    0.004    0.000 extensions.py:83(get_extension_for_class)\n",
       "      506    0.002    0.000    0.006    0.000 timeout.py:140(from_float)\n",
       "     7575    0.002    0.000    0.002    0.000 {method 'lower' of 'bytes' objects}\n",
       "     1037    0.002    0.000    0.003    0.000 cookiejar.py:334(split_header_words)\n",
       "      534    0.002    0.000    0.037    0.000 message.py:451(items)\n",
       "     1515    0.002    0.000    0.002    0.000 {built-in method _openssl.X509_STORE_CTX_get_current_cert}\n",
       "      506    0.002    0.000    0.525    0.001 sessions.py:733(close)\n",
       "      532    0.002    0.000    0.461    0.001 request.py:2662(getproxies)\n",
       "      505    0.002    0.000    0.025    0.000 models.py:534(prepare_auth)\n",
       "     1010    0.002    0.000    0.004    0.000 extensions.py:695(__init__)\n",
       "     1045    0.002    0.000    0.002    0.000 __init__.py:1619(isEnabledFor)\n",
       "      505    0.002    0.000    0.010    0.000 models.py:576(prepare_hooks)\n",
       "      505    0.002    0.000    0.002    0.000 {built-in method _openssl.SSL_set_fd}\n",
       "     3535    0.002    0.000    0.003    0.000 extensions.py:1186(<genexpr>)\n",
       "      535    0.002    0.000    1.117    0.002 client.py:1007(_send_output)\n",
       "      505    0.002    0.000    0.011    0.000 response.py:114(_get_decoder)\n",
       "      535    0.002    0.000    0.003    0.000 client.py:415(flush)\n",
       "      535    0.002    0.000    0.007    0.000 client.py:369(_check_close)\n",
       "     1070    0.002    0.000    0.003    0.000 feedparser.py:122(pushlines)\n",
       "      505    0.002    0.000    0.003    0.000 parser.pxi:564(prepare)\n",
       "     2434    0.002    0.000    0.002    0.000 {method '_checkClosed' of '_io._IOBase' objects}\n",
       "     3535    0.002    0.000    0.002    0.000 _util.py:62(openssl_assert)\n",
       "      505    0.002    0.000    0.004    0.000 SSL.py:1327(set_options)\n",
       "      506    0.002    0.000    0.002    0.000 queue.py:11(_init)\n",
       "      483    0.002    0.000    0.077    0.000 _parser.py:577(parse)\n",
       "     1569    0.002    0.000    0.002    0.000 {built-in method nt.fspath}\n",
       "      117    0.002    0.000    0.007    0.000 <ipython-input-9-81aa55e67a15>:69(page_scan)\n",
       "     1273    0.002    0.000    0.004    0.000 punkt.py:396(__init__)\n",
       "     1932    0.002    0.000    0.003    0.000 locale.py:384(normalize)\n",
       "      532    0.002    0.000    0.219    0.000 utils.py:227(extract_zipped_paths)\n",
       "      505    0.002    0.000    0.055    0.000 saxparser.pxi:508(_handleSaxTargetDoctype)\n",
       "      505    0.002    0.000    0.002    0.000 {built-in method _openssl.OBJ_txt2nid}\n",
       "      536    0.002    0.000    1.114    0.002 client.py:948(send)\n",
       "      536    0.002    0.000    0.003    0.000 response.py:547(closed)\n",
       "      497    0.002    0.000    0.002    0.000 parser.pxi:530(__dealloc__)\n",
       "      505    0.002    0.000    0.006    0.000 pyopenssl.py:399(options)\n",
       "      535    0.002    0.000    1.190    0.002 client.py:1226(request)\n",
       "      505    0.002    0.000    0.002    0.000 {built-in method _openssl.SSL_CTX_set_options}\n",
       "      505    0.002    0.000    0.002    0.000 pyopenssl.py:254(__init__)\n",
       "      559    0.002    0.000    0.005    0.000 models.py:707(is_redirect)\n",
       "      534    0.002    0.000    0.002    0.000 retry.py:295(_is_method_retryable)\n",
       "      505    0.002    0.000    0.002    0.000 {built-in method _openssl.SSL_CTX_set_mode}\n",
       "      505    0.002    0.000    0.004    0.000 SSL.py:1340(set_mode)\n",
       "     3138    0.002    0.000    0.002    0.000 cookiejar.py:925(is_not_allowed)\n",
       "     1515    0.002    0.000    0.002    0.000 {built-in method _openssl.X509_STORE_CTX_get_ex_data}\n",
       "     1846    0.002    0.000    0.008    0.000 punkt.py:549(_tokenize_words)\n",
       "      491    0.002    0.000    0.003    0.000 common.py:1784(_get_dtype)\n",
       "     2020    0.002    0.000    0.002    0.000 x509.py:503(__init__)\n",
       "      962    0.002    0.000    0.003    0.000 _parser.py:1008(_find_hms_idx)\n",
       "      505    0.002    0.000    0.004    0.000 xmlerror.pxi:430(__init__)\n",
       "      505    0.002    0.000    0.003    0.000 SSL.py:608(_asFileDescriptor)\n",
       "     1932    0.002    0.000    0.010    0.000 locale.py:571(getlocale)\n",
       "     2140    0.002    0.000    0.002    0.000 {method 'seek' of '_io.StringIO' objects}\n",
       "      505    0.002    0.000    0.002    0.000 {built-in method _openssl.SSL_CTX_set_ecdh_auto}\n",
       "      534    0.002    0.000    0.002    0.000 {method 'pop' of 'collections.OrderedDict' objects}\n",
       "      508    0.002    0.000    0.002    0.000 parse.py:178(_userinfo)\n",
       "     1932    0.002    0.000    0.012    0.000 _strptime.py:26(_getlang)\n",
       "      534    0.002    0.000    0.004    0.000 retry.py:304(is_retry)\n",
       "     3111    0.002    0.000    0.002    0.000 {method 'toordinal' of 'datetime.date' objects}\n",
       "     1010    0.002    0.000    0.016    0.000 decode_asn1.py:568(_decode_distpoint)\n",
       "      483    0.002    0.000    0.003    0.000 _parser.py:227(__init__)\n",
       "     2020    0.002    0.000    0.002    0.000 {built-in method _openssl.sk_SCT_value}\n",
       "      505    0.002    0.000    0.003    0.000 extensions.py:821(__init__)\n",
       "     1010    0.002    0.000    0.002    0.000 {built-in method builtins.max}\n",
       "      505    0.002    0.000    0.008    0.000 element.py:853(for_name_and_ids)\n",
       "     1012    0.002    0.000    0.520    0.001 poolmanager.py:198(clear)\n",
       "     3111    0.002    0.000    0.002    0.000 cookiejar.py:1058(set_ok_port)\n",
       "      573    0.002    0.000    0.002    0.000 {method 'findall' of 're.Pattern' objects}\n",
       "     1010    0.002    0.000    0.015    0.000 extensions.py:1223(get_values_for_type)\n",
       "     3070    0.002    0.000    0.002    0.000 {method 'replace' of 'str' objects}\n",
       "     1010    0.002    0.000    0.002    0.000 {built-in method _openssl.SSL_CTX_get_verify_mode}\n",
       "      505    0.002    0.000    0.002    0.000 xmlerror.pxi:274(__init__)\n",
       "      505    0.002    0.000    0.002    0.000 extensions.py:953(__init__)\n",
       "      505    0.002    0.000    0.030    0.000 decode_asn1.py:422(_decode_general_names_extension)\n",
       "     3149    0.002    0.000    0.002    0.000 generic.py:7(_check)\n",
       "     2633    0.002    0.000    0.002    0.000 {method 'setdefault' of 'dict' objects}\n",
       "      505    0.002    0.000    0.002    0.000 weakref.py:339(__init__)\n",
       "      535    0.002    0.000    0.002    0.000 parser.py:18(__init__)\n",
       "     1010    0.002    0.000    0.003    0.000 SSL.py:1125(get_verify_mode)\n",
       "     1068    0.002    0.000    0.002    0.000 message.py:181(is_multipart)\n",
       "      532    0.002    0.000    0.005    0.000 pyopenssl.py:311(settimeout)\n",
       "     1443    0.002    0.000    0.005    0.000 common.py:332(is_datetime64_dtype)\n",
       "     1273    0.002    0.000    0.003    0.000 punkt.py:595(_first_pass_annotation)\n",
       "      505    0.002    0.000    0.003    0.000 crypto.py:86(_get_backend)\n",
       "     3030    0.002    0.000    0.013    0.000 extensions.py:1168(<genexpr>)\n",
       "      505    0.002    0.000    0.012    0.000 crypto.py:1464(get_subject)\n",
       "      481    0.002    0.000    0.007    0.000 _parser.py:479(resolve_ymd)\n",
       "      532    0.002    0.000    0.014    0.000 cookiejar.py:1265(_cookies_for_request)\n",
       "     3379    0.002    0.000    0.017    0.000 _parser.py:195(__next__)\n",
       "      505    0.002    0.000    0.002    0.000 {built-in method _openssl.ASN1_STRING_to_UTF8}\n",
       "      532    0.002    0.000    0.098    0.000 pyopenssl.py:314(_send_until_done)\n",
       "      505    0.002    0.000    3.883    0.008 sessions.py:668(<listcomp>)\n",
       "      505    0.002    0.000    0.003    0.000 _internal_utils.py:30(unicode_is_ascii)\n",
       "      559    0.002    0.000    0.007    0.000 sessions.py:97(get_redirect_target)\n",
       "      505    0.002    0.000    0.002    0.000 response.py:75(is_response_to_head)\n",
       "      505    0.001    0.000    0.005    0.000 parser.pxi:605(_initParserContext)\n",
       "     4040    0.001    0.000    0.001    0.000 inspect.py:2515(name)\n",
       "      505    0.001    0.000    0.002    0.000 saxparser.pxi:89(__cinit__)\n",
       "     1010    0.001    0.000    0.001    0.000 SSL.py:281(__init__)\n",
       "      535    0.001    0.000    0.002    0.000 feedparser.py:210(_pop_message)\n",
       "      506    0.001    0.000    0.002    0.000 connectionpool.py:878(_ipv6_host)\n",
       "     2525    0.001    0.000    0.002    0.000 core.py:60(valid_string_length)\n",
       "      505    0.001    0.000    0.755    0.001 utils.py:158(inner)\n",
       "      505    0.001    0.000    0.002    0.000 docloader.pxi:172(_initResolverContext)\n",
       "     1445    0.001    0.000    0.002    0.000 _parser.py:328(weekday)\n",
       "      505    0.001    0.000    0.001    0.000 {built-in method _openssl.X509_NAME_get_index_by_NID}\n",
       "     5309    0.001    0.000    0.002    0.000 _parser.py:209(isword)\n",
       "      715    0.001    0.000    1.740    0.002 <ipython-input-9-81aa55e67a15>:21(try_locs)\n",
       "      942    0.001    0.000    0.005    0.000 element.py:1005(get_text)\n",
       "      479    0.001    0.000    0.003    0.000 _parser.py:1209(_build_naive)\n",
       "     1515    0.001    0.000    0.001    0.000 {built-in method _openssl.sk_GENERAL_NAME_num}\n",
       "      505    0.001    0.000    0.001    0.000 extensions.py:146(__init__)\n",
       "      534    0.001    0.000    0.002    0.000 poolmanager.py:282(_merge_pool_kwargs)\n",
       "      505    0.001    0.000    0.001    0.000 {built-in method _openssl.SSL_set_tlsext_host_name}\n",
       "      505    0.001    0.000    0.017    0.000 connection.py:372(_match_hostname)\n",
       "      505    0.001    0.000    0.003    0.000 extensions.py:659(__init__)\n",
       "     1992    0.001    0.000    0.003    0.000 element.py:980(_all_strings)\n",
       "      508    0.001    0.000    0.004    0.000 parse.py:146(username)\n",
       "      479    0.001    0.000    0.001    0.000 {method 'replace' of 'datetime.datetime' objects}\n",
       "     2525    0.001    0.000    0.007    0.000 extensions.py:1380(<genexpr>)\n",
       "      532    0.001    0.000    0.001    0.000 pyopenssl.py:265(_decref_socketios)\n",
       "     1010    0.001    0.000    0.002    0.000 response.py:584(supports_chunked_reads)\n",
       "     1037    0.001    0.000    0.002    0.000 _util.py:127(text_to_bytes_and_warn)\n",
       "     1010    0.001    0.000    0.001    0.000 {built-in method _openssl.sk_POLICYINFO_value}\n",
       "     3192    0.001    0.000    0.001    0.000 cookies.py:65(is_unverifiable)\n",
       "     1064    0.001    0.000    0.002    0.000 ntpath.py:34(_get_bothseps)\n",
       "      117    0.001    0.000    0.001    0.000 <ipython-input-9-81aa55e67a15>:82(<listcomp>)\n",
       "     6227    0.001    0.000    0.002    0.000 _parser.py:240(<genexpr>)\n",
       "      483    0.001    0.000    0.079    0.000 _parser.py:1258(parse)\n",
       "      505    0.001    0.000    0.052    0.000 parsertarget.pxi:90(_handleSaxDoctype)\n",
       "     1932    0.001    0.000    0.004    0.000 locale.py:467(_parse_localename)\n",
       "     1064    0.001    0.000    0.001    0.000 {built-in method time.perf_counter}\n",
       "      534    0.001    0.000    0.002    0.000 <string>:1(__new__)\n",
       "     1070    0.001    0.000    0.001    0.000 {method 'extend' of 'collections.deque' objects}\n",
       "     1515    0.001    0.000    0.001    0.000 {built-in method _openssl.X509_STORE_CTX_set_error}\n",
       "     1043    0.001    0.000    0.002    0.000 {built-in method builtins.any}\n",
       "      505    0.001    0.000    0.001    0.000 extensions.py:340(__init__)\n",
       "     1010    0.001    0.000    0.001    0.000 {built-in method _openssl.sk_ACCESS_DESCRIPTION_value}\n",
       "      535    0.001    0.000    0.002    0.000 message.py:303(set_payload)\n",
       "      534    0.001    0.000    0.002    0.000 timeout.py:171(start_connect)\n",
       "     1010    0.001    0.000    0.001    0.000 {built-in method _openssl.sk_DIST_POINT_value}\n",
       "      532    0.001    0.000    0.001    0.000 {method 'sort' of 'list' objects}\n",
       "     1515    0.001    0.000    0.001    0.000 {built-in method _openssl.X509_STORE_CTX_get_error}\n",
       "      505    0.001    0.000    0.002    0.000 weakref.py:109(remove)\n",
       "     1010    0.001    0.000    0.001    0.000 inspect.py:158(isfunction)\n",
       "     1515    0.001    0.000    0.001    0.000 {built-in method _openssl.SSL_get_ex_data_X509_STORE_CTX_idx}\n",
       "      599    0.001    0.000    0.003    0.000 {built-in method builtins.sum}\n",
       "      505    0.001    0.000    0.001    0.000 {built-in method _openssl.SSL_get_peer_certificate}\n",
       "     1924    0.001    0.000    0.001    0.000 _parser.py:342(hms)\n",
       "      525    0.001    0.000    0.002    0.000 common.py:369(is_datetime64tz_dtype)\n",
       "     1012    0.001    0.000    0.002    0.000 six.py:577(itervalues)\n",
       "     1068    0.001    0.000    0.001    0.000 response.py:200(<genexpr>)\n",
       "     2236    0.001    0.000    0.001    0.000 cookies.py:111(info)\n",
       "     1932    0.001    0.000    0.026    0.000 _strptime.py:574(_strptime_datetime)\n",
       "      521    0.001    0.000    0.033    0.000 socket.py:416(close)\n",
       "      538    0.001    0.000    0.004    0.000 response.py:532(getheader)\n",
       "     2020    0.001    0.000    0.001    0.000 {method 'isidentifier' of 'str' objects}\n",
       "      535    0.001    0.000    1.118    0.002 client.py:1213(endheaders)\n",
       "     1574    0.001    0.000    0.001    0.000 timeout.py:195(connect_timeout)\n",
       "     5584    0.001    0.000    0.001    0.000 {method 'pop' of 'collections.deque' objects}\n",
       "     1010    0.001    0.000    0.001    0.000 {built-in method _openssl.sk_ASN1_OBJECT_value}\n",
       "      505    0.001    0.000    0.001    0.000 sessions.py:80(merge_hooks)\n",
       "      505    0.001    0.000    0.001    0.000 {built-in method _openssl.SSL_set_mode}\n",
       "      505    0.001    0.000    0.018    0.000 extensions.py:1214(__init__)\n",
       "      670    0.001    0.000    0.028    0.000 punkt.py:1340(_realign_boundaries)\n",
       "     1118    0.001    0.000    0.001    0.000 cookies.py:104(__init__)\n",
       "     1010    0.001    0.000    0.001    0.000 extensions.py:704(<genexpr>)\n",
       "     3799    0.001    0.000    0.001    0.000 {built-in method builtins.iter}\n",
       "     1070    0.001    0.000    0.001    0.000 parse.py:432(<genexpr>)\n",
       "      505    0.001    0.000    0.002    0.000 parser.pxi:117(initThreadDictRef)\n",
       "      532    0.001    0.000    0.001    0.000 hooks.py:23(dispatch_hook)\n",
       "     1926    0.001    0.000    0.001    0.000 _parser.py:335(month)\n",
       " 2499/756    0.001    0.000    0.027    0.000 punkt.py:310(_pair_iter)\n",
       "      505    0.001    0.000    0.001    0.000 _lxml.py:247(default_parser)\n",
       "      535    0.001    0.000    0.009    0.000 client.py:398(_close_conn)\n",
       "       27    0.001    0.000    0.001    0.000 wait.py:41(_retry_on_intr)\n",
       "      483    0.001    0.000    0.001    0.000 _parser.py:400(__init__)\n",
       "     1515    0.001    0.000    0.001    0.000 {built-in method _openssl.X509_up_ref}\n",
       "      505    0.001    0.000    0.001    0.000 saxparser.pxi:98(_initParserContext)\n",
       "      505    0.001    0.000    0.002    0.000 weakref.py:334(__new__)\n",
       "     4213    0.001    0.000    0.001    0.000 _parser.py:214(isnum)\n",
       "      505    0.001    0.000    0.002    0.000 extensions.py:410(__init__)\n",
       "     1118    0.001    0.000    0.001    0.000 utils.py:672(set_environ)\n",
       "     2047    0.001    0.000    0.001    0.000 {method 'items' of 'collections.OrderedDict' objects}\n",
       "     1070    0.001    0.000    0.001    0.000 {method 'truncate' of '_io.StringIO' objects}\n",
       "     1010    0.001    0.000    0.001    0.000 {method 'update' of 'collections.OrderedDict' objects}\n",
       "     1515    0.001    0.000    0.001    0.000 {built-in method _openssl.X509_STORE_CTX_get_error_depth}\n",
       "       81    0.001    0.000    0.010    0.000 cookiejar.py:1079(return_ok)\n",
       "      479    0.001    0.000    0.001    0.000 _parser.py:468(<listcomp>)\n",
       "      505    0.001    0.000    0.001    0.000 apihelpers.pxi:1420(_utf8)\n",
       "      534    0.001    0.000    0.002    0.000 request.py:77(set_file_position)\n",
       "     1170    0.001    0.000    0.002    0.000 punkt.py:1515(_second_pass_annotation)\n",
       "      505    0.001    0.000    0.001    0.000 {built-in method _openssl.X509_get_ext_count}\n",
       "      481    0.001    0.000    0.002    0.000 element.py:57(__new__)\n",
       "     2020    0.001    0.000    0.005    0.000 extensions.py:498(<genexpr>)\n",
       "      534    0.001    0.000    0.001    0.000 {built-in method time.monotonic}\n",
       "      518    0.001    0.000    0.032    0.000 socket.py:412(_real_close)\n",
       "      507    0.001    0.000    0.001    0.000 connection.py:93(allowed_gai_family)\n",
       "      506    0.001    0.000    0.526    0.001 sessions.py:423(__exit__)\n",
       "     1743    0.001    0.000    0.016    0.000 punkt.py:1505(_annotate_second_pass)\n",
       "     1447    0.001    0.000    0.001    0.000 _parser.py:348(ampm)\n",
       "      505    0.001    0.000    0.002    0.000 SSL.py:2248(set_connect_state)\n",
       "      573    0.001    0.000    0.017    0.000 punkt.py:1370(text_contains_sentbreak)\n",
       "      505    0.001    0.000    0.001    0.000 {method 'values' of 'mappingproxy' objects}\n",
       "      481    0.001    0.000    0.005    0.000 common.py:1083(is_datetime64_ns_dtype)\n",
       "      505    0.001    0.000    0.001    0.000 models.py:521(prepare_content_length)\n",
       "      505    0.001    0.000    0.001    0.000 {built-in method _openssl.X509_get_subject_name}\n",
       "     1447    0.001    0.000    0.001    0.000 _parser.py:325(jump)\n",
       "      505    0.001    0.000    0.004    0.000 _collections_abc.py:701(__len__)\n",
       "      505    0.001    0.000    0.001    0.000 xmlerror.pxi:457(clear)\n",
       "      535    0.001    0.000    0.001    0.000 client.py:795(_get_content_length)\n",
       "     1846    0.001    0.000    0.012    0.000 punkt.py:574(_annotate_first_pass)\n",
       "      505    0.001    0.000    0.001    0.000 {built-in method _weakref._remove_dead_weakref}\n",
       "     1151    0.001    0.000    0.001    0.000 client.py:426(isclosed)\n",
       "      505    0.001    0.000    0.001    0.000 response.py:68(__getattr__)\n",
       "      559    0.001    0.000    0.011    0.000 parse.py:325(geturl)\n",
       "      505    0.001    0.000    0.001    0.000 x509.py:28(__init__)\n",
       "      505    0.001    0.000    0.001    0.000 _util.py:93(path_string)\n",
       "     1010    0.001    0.000    0.001    0.000 extensions.py:1176(__iter__)\n",
       "      505    0.001    0.000    0.001    0.000 crypto.py:208(add)\n",
       "     2525    0.001    0.000    0.001    0.000 inspect.py:2519(default)\n",
       "      117    0.001    0.000    0.034    0.000 <ipython-input-9-81aa55e67a15>:37(clean_summary)\n",
       "      505    0.001    0.000    0.002    0.000 decode_asn1.py:834(<lambda>)\n",
       "      505    0.001    0.000    0.006    0.000 parser.pxi:579(cleanup)\n",
       "      506    0.001    0.000    0.001    0.000 {method 'flush' of '_io.BufferedReader' objects}\n",
       "      505    0.001    0.000    0.001    0.000 {built-in method _openssl.OPENSSL_free}\n",
       "       61    0.001    0.000    0.001    0.000 {method 'clear' of 'dict' objects}\n",
       "     2020    0.001    0.000    0.001    0.000 inspect.py:2527(kind)\n",
       "      505    0.001    0.000    0.001    0.000 parser.pxi:70(_findThreadParserContext)\n",
       "      505    0.001    0.000    0.001    0.000 ssl_.py:213(resolve_ssl_version)\n",
       "      505    0.001    0.000    0.001    0.000 {built-in method _openssl.X509_NAME_ENTRY_get_data}\n",
       "      505    0.001    0.000    0.001    0.000 {built-in method _openssl.SSL_set_connect_state}\n",
       "     1515    0.001    0.000    0.001    0.000 extensions.py:823(<genexpr>)\n",
       "      479    0.001    0.000    0.001    0.000 _parser.py:386(validate)\n",
       "     1515    0.001    0.000    0.001    0.000 extensions.py:661(<genexpr>)\n",
       "      505    0.001    0.000    0.005    0.000 _util.py:111(byte_string)\n",
       "       24    0.001    0.000    5.892    0.245 api.py:63(get)\n",
       "      505    0.001    0.000    0.001    0.000 docloader.pxi:116(__cinit__)\n",
       "     1515    0.001    0.000    0.001    0.000 apihelpers.pxi:1394(funicodeOrNone)\n",
       "      505    0.001    0.000    0.001    0.000 {built-in method _openssl.sk_POLICYQUALINFO_value}\n",
       "     2020    0.001    0.000    0.001    0.000 inspect.py:2523(annotation)\n",
       "      505    0.001    0.000    0.001    0.000 parsertarget.pxi:129(_handleParseResult)\n",
       "      481    0.001    0.000    0.001    0.000 element.py:1088(__setitem__)\n",
       "      505    0.001    0.000    0.001    0.000 {built-in method _openssl.X509_NAME_get_entry}\n",
       "     1012    0.001    0.000    0.001    0.000 {method 'clear' of 'collections.OrderedDict' objects}\n",
       "      986    0.001    0.000    0.001    0.000 element.py:1068(__getitem__)\n",
       "     1515    0.001    0.000    0.001    0.000 extensions.py:267(<genexpr>)\n",
       "      479    0.001    0.000    0.001    0.000 {built-in method pandas._libs.tslibs.timezones.tz_standardize}\n",
       "      479    0.001    0.000    0.001    0.000 _parser.py:476(<dictcomp>)\n",
       "     5309    0.001    0.000    0.001    0.000 {method 'isalpha' of 'str' objects}\n",
       "      505    0.001    0.000    0.001    0.000 parser.pxi:101(_getThreadDict)\n",
       "      573    0.001    0.000    0.001    0.000 base.py:61(is_dtype)\n",
       "      986    0.001    0.000    0.001    0.000 __init__.py:162(deprecated_argument)\n",
       "      505    0.001    0.000    0.001    0.000 {built-in method _openssl.sk_SCT_num}\n",
       "     1515    0.001    0.000    0.001    0.000 extensions.py:413(<genexpr>)\n",
       "     1010    0.001    0.000    0.001    0.000 decode_asn1.py:717(_asn1_integer_to_int_or_none)\n",
       "      534    0.001    0.000    0.001    0.000 timeout.py:213(read_timeout)\n",
       "     1273    0.001    0.000    0.001    0.000 punkt.py:419(_get_type)\n",
       "      942    0.001    0.000    0.004    0.000 element.py:1010(<listcomp>)\n",
       "      505    0.001    0.000    0.001    0.000 {built-in method _openssl.sk_DIST_POINT_num}\n",
       "      505    0.001    0.000    0.003    0.000 parser.pxi:127(initParserDict)\n",
       "       27    0.001    0.000    0.001    0.000 client.py:468(readinto)\n",
       "      505    0.001    0.000    0.001    0.000 {built-in method _openssl.sk_POLICYINFO_num}\n",
       "      505    0.001    0.000    0.001    0.000 SSL.py:1588(__getattr__)\n",
       "      479    0.001    0.000    0.003    0.000 _parser.py:239(__len__)\n",
       "      507    0.001    0.000    0.001    0.000 client.py:871(_get_hostport)\n",
       "      505    0.001    0.000    0.001    0.000 {built-in method _openssl.sk_POLICYQUALINFO_num}\n",
       "      505    0.001    0.000    0.001    0.000 {built-in method _openssl.sk_ASN1_OBJECT_num}\n",
       "      506    0.001    0.000    0.001    0.000 inference.py:251(is_list_like)\n",
       "       27    0.001    0.000    0.002    0.000 parse.py:479(urljoin)\n",
       "       27    0.001    0.000    0.013    0.000 cookiejar.py:1247(_cookies_for_domain)\n",
       "     1070    0.001    0.000    0.001    0.000 feedparser.py:125(__iter__)\n",
       "      505    0.001    0.000    0.001    0.000 {built-in method _openssl.sk_ACCESS_DESCRIPTION_num}\n",
       "     1515    0.001    0.000    0.001    0.000 pyopenssl.py:465(_verify_callback)\n",
       "       29    0.000    0.000    0.005    0.000 response.py:404(read)\n",
       "      479    0.000    0.000    0.001    0.000 _parser.py:1169(_build_tzaware)\n",
       "     2593    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}\n",
       "     1010    0.000    0.000    0.000    0.000 __init__.py:105(reset)\n",
       "       27    0.000    0.000    0.071    0.003 sessions.py:276(rebuild_proxies)\n",
       "      486    0.000    0.000    0.000    0.000 base.py:635(_reset_identity)\n",
       "      505    0.000    0.000    0.000    0.000 pyopenssl.py:395(options)\n",
       "      117    0.000    0.000    0.001    0.000 <ipython-input-9-81aa55e67a15>:12(replace_em)\n",
       "      506    0.000    0.000    0.000    0.000 connection.py:135(host)\n",
       "      505    0.000    0.000    0.001    0.000 docloader.pxi:166(clear)\n",
       "     4221    0.000    0.000    0.000    0.000 {method 'isdigit' of 'str' objects}\n",
       "      505    0.000    0.000    0.000    0.000 _lxml.py:146(close)\n",
       "      505    0.000    0.000    0.000    0.000 dammit.py:222(<listcomp>)\n",
       "       81    0.000    0.000    0.001    0.000 copy.py:268(_reconstruct)\n",
       "       15    0.000    0.000    0.000    0.000 socket.py:334(send)\n",
       "       24    0.000    0.000    5.891    0.245 api.py:16(request)\n",
       "      532    0.000    0.000    0.000    0.000 utils.py:514(iter_slices)\n",
       "     1695    0.000    0.000    0.000    0.000 {method 'group' of 're.Match' objects}\n",
       "      505    0.000    0.000    0.001    0.000 extensions.py:99(__iter__)\n",
       "      532    0.000    0.000    0.000    0.000 cookies.py:81(get_new_headers)\n",
       "      505    0.000    0.000    0.000    0.000 extensions.py:236(__init__)\n",
       "     2671    0.000    0.000    0.000    0.000 {built-in method builtins.issubclass}\n",
       "      532    0.000    0.000    0.000    0.000 {method 'fileno' of '_socket.socket' objects}\n",
       "      481    0.000    0.000    0.000    0.000 {built-in method pandas._libs.lib.is_integer}\n",
       "      514    0.000    0.000    0.001    0.000 common.py:89(is_object_dtype)\n",
       "      505    0.000    0.000    0.000    0.000 extensions.py:73(__init__)\n",
       "     1260    0.000    0.000    0.001    0.000 punkt.py:460(is_ellipsis)\n",
       "      505    0.000    0.000    0.000    0.000 apihelpers.pxi:1363(_is_valid_xml_utf8)\n",
       "      481    0.000    0.000    0.000    0.000 _parser.py:487(<dictcomp>)\n",
       "      670    0.000    0.000    0.029    0.000 punkt.py:1305(span_tokenize)\n",
       "      505    0.000    0.000    0.000    0.000 models.py:184(<genexpr>)\n",
       "      966    0.000    0.000    0.001    0.000 _parser.py:219(isspace)\n",
       "      505    0.000    0.000    0.000    0.000 {method 'reverse' of 'list' objects}\n",
       "      479    0.000    0.000    0.000    0.000 {built-in method pandas._libs.tslibs.timezones.maybe_get_tz}\n",
       "       29    0.000    0.000    0.002    0.000 client.py:436(read)\n",
       "      573    0.000    0.000    0.002    0.000 punkt.py:263(word_tokenize)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method io.open}\n",
       "      117    0.000    0.000    0.029    0.000 punkt.py:1323(<listcomp>)\n",
       "      505    0.000    0.000    0.000    0.000 xmlerror.pxi:427(__cinit__)\n",
       "       81    0.000    0.000    0.003    0.000 cookiejar.py:1144(return_ok_domain)\n",
       "      505    0.000    0.000    0.000    0.000 inspect.py:2831(return_annotation)\n",
       "      505    0.000    0.000    0.000    0.000 etree.pyx:273(__init__)\n",
       "      576    0.000    0.000    0.000    0.000 punkt.py:423(type_no_period)\n",
       "      532    0.000    0.000    0.000    0.000 adapters.py:358(add_headers)\n",
       "       81    0.000    0.000    0.000    0.000 {method '__reduce_ex__' of 'object' objects}\n",
       "        6    0.000    0.000    6.030    1.005 socket.py:691(create_connection)\n",
       "      505    0.000    0.000    0.000    0.000 parser.pxi:893(_registerHtmlErrorHandler)\n",
       "       27    0.000    0.000    0.001    0.000 cookies.py:414(copy)\n",
       "      505    0.000    0.000    0.000    0.000 pyopenssl.py:240(<genexpr>)\n",
       "      573    0.000    0.000    0.000    0.000 punkt.py:1406(_annotate_tokens)\n",
       "      505    0.000    0.000    0.000    0.000 saxparser.pxi:95(_setSaxParserTarget)\n",
       "      535    0.000    0.000    0.000    0.000 {function HTTPResponse.flush at 0x000000000410B400}\n",
       "      534    0.000    0.000    0.000    0.000 connectionpool.py:280(_validate_conn)\n",
       "      505    0.000    0.000    0.000    0.000 saxparser.pxi:229(__cinit__)\n",
       "       27    0.000    0.000    0.003    0.000 models.py:328(copy)\n",
       "      479    0.000    0.000    0.000    0.000 {method 'date' of 'datetime.datetime' objects}\n",
       "      505    0.000    0.000    0.000    0.000 etree.pyx:280(clear)\n",
       "     4101    0.000    0.000    0.000    0.000 element.py:803(name)\n",
       "      964    0.000    0.000    0.000    0.000 {method 'is_finite' of 'decimal.Decimal' objects}\n",
       "      506    0.000    0.000    0.000    0.000 sessions.py:420(__enter__)\n",
       "      505    0.000    0.000    0.000    0.000 xmlerror.pxi:183(__init__)\n",
       "      479    0.000    0.000    0.000    0.000 _parser.py:469(<listcomp>)\n",
       "      479    0.000    0.000    0.000    0.000 datetimes.py:604(_box_func)\n",
       "      517    0.000    0.000    0.000    0.000 {built-in method pandas._libs.lib.is_scalar}\n",
       "      505    0.000    0.000    0.000    0.000 inspect.py:2827(parameters)\n",
       "      117    0.000    0.000    0.029    0.000 punkt.py:1265(tokenize)\n",
       "      533    0.000    0.000    0.000    0.000 base.py:641(__len__)\n",
       "      505    0.000    0.000    0.000    0.000 parser.pxi:884(_configureSaxContext)\n",
       "       81    0.000    0.000    0.000    0.000 _collections_abc.py:790(pop)\n",
       "      117    0.000    0.000    0.000    0.000 {method 'finditer' of 're.Pattern' objects}\n",
       "      162    0.000    0.000    0.000    0.000 cookiejar.py:796(is_expired)\n",
       "      479    0.000    0.000    0.000    0.000 datetimes.py:684(tz)\n",
       "      964    0.000    0.000    0.000    0.000 _parser.py:1058(_could_be_tzname)\n",
       "      568    0.000    0.000    0.000    0.000 punkt.py:470(is_initial)\n",
       "      505    0.000    0.000    0.000    0.000 saxparser.pxi:43(__cinit__)\n",
       "       81    0.000    0.000    0.005    0.000 cookiejar.py:1106(return_ok_verifiability)\n",
       "      461    0.000    0.000    0.001    0.000 <ipython-input-9-81aa55e67a15>:40(<genexpr>)\n",
       "      505    0.000    0.000    0.000    0.000 saxparser.pxi:105(_connectTarget)\n",
       "      481    0.000    0.000    0.000    0.000 _parser.py:411(has_month)\n",
       "      505    0.000    0.000    0.000    0.000 parser.pxi:552(_initParserContext)\n",
       "      117    0.000    0.000    0.029    0.000 punkt.py:1316(sentences_from_text)\n",
       "       27    0.000    0.000    0.001    0.000 wait.py:68(select_wait_for_socket)\n",
       "       80    0.000    0.000    0.000    0.000 punkt.py:1598(_ortho_heuristic)\n",
       "       27    0.000    0.000    0.001    0.000 cookiejar.py:1166(domain_return_ok)\n",
       "      966    0.000    0.000    0.000    0.000 {method 'isspace' of 'str' objects}\n",
       "       13    0.000    0.000    0.000    0.000 {pandas._libs.lib.fast_multiget}\n",
       "        1    0.000    0.000    0.003    0.003 <string>:1(<listcomp>)\n",
       "      479    0.000    0.000    0.000    0.000 frequencies.py:74(to_offset)\n",
       "      117    0.000    0.000    0.000    0.000 <ipython-input-9-81aa55e67a15>:76(<dictcomp>)\n",
       "      479    0.000    0.000    0.000    0.000 _parser.py:366(convertyear)\n",
       "      479    0.000    0.000    0.000    0.000 datetimelike.py:232(freq)\n",
       "       81    0.000    0.000    0.000    0.000 cookiejar.py:1124(return_ok_expires)\n",
       "        1    0.000    0.000    2.014    2.014 remote_connection.py:106(__init__)\n",
       "       27    0.000    0.000    0.016    0.001 sessions.py:256(rebuild_auth)\n",
       "       81    0.000    0.000    0.000    0.000 copyreg.py:87(__newobj__)\n",
       "       27    0.000    0.000    0.000    0.000 sessions.py:317(rebuild_method)\n",
       "      505    0.000    0.000    0.000    0.000 saxparser.pxi:204(flushEvents)\n",
       "      479    0.000    0.000    0.000    0.000 base.py:547(_deepcopy_if_needed)\n",
       "        1    0.000    0.000  236.863  236.863 {built-in method builtins.exec}\n",
       "      117    0.000    0.000    0.000    0.000 punkt.py:279(period_context_re)\n",
       "        1    0.000    0.000  236.863  236.863 <string>:2(<module>)\n",
       "       27    0.000    0.000    0.000    0.000 pyopenssl.py:261(fileno)\n",
       "      573    0.000    0.000    0.000    0.000 punkt.py:247(_word_tokenizer_re)\n",
       "       54    0.000    0.000    0.000    0.000 response.py:7(is_fp_closed)\n",
       "      648    0.000    0.000    0.000    0.000 punkt.py:432(type_no_sentperiod)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'listen' of '_socket.socket' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _winapi.TerminateProcess}\n",
       "      579    0.000    0.000    0.000    0.000 {method 'end' of 're.Match' objects}\n",
       "     1010    0.000    0.000    0.000    0.000 etree.pyx:305(_has_raised)\n",
       "      481    0.000    0.000    0.000    0.000 _parser.py:407(has_year)\n",
       "      483    0.000    0.000    0.000    0.000 _parser.py:192(__iter__)\n",
       "       28    0.000    0.000    0.002    0.000 connection.py:7(is_connection_dropped)\n",
       "        3    0.000    0.000    0.000    0.000 socket.py:221(makefile)\n",
       "       81    0.000    0.000    0.000    0.000 cookiejar.py:1097(return_ok_version)\n",
       "       27    0.000    0.000    0.001    0.000 structures.py:80(copy)\n",
       "       30    0.000    0.000    0.001    0.000 parse.py:83(clear_cache)\n",
       "       27    0.000    0.000    0.001    0.000 wait.py:139(wait_for_read)\n",
       "      505    0.000    0.000    0.000    0.000 etree.pyx:289(clear)\n",
       "       27    0.000    0.000    0.000    0.000 {method 'readinto' of '_io.BufferedReader' objects}\n",
       "        1    0.000    0.000    3.011    3.011 service.py:137(stop)\n",
       "       27    0.000    0.000    0.000    0.000 cookies.py:68(has_header)\n",
       "       27    0.000    0.000    0.000    0.000 models.py:942(close)\n",
       "       27    0.000    0.000    0.001    0.000 cookiejar.py:1188(path_return_ok)\n",
       "        2    0.000    0.000    1.702    0.851 remote_connection.py:376(_request)\n",
       "      7/4    0.000    0.000    0.000    0.000 base.py:255(__new__)\n",
       "      495    0.000    0.000    0.000    0.000 {method 'start' of 're.Match' objects}\n",
       "       81    0.000    0.000    0.000    0.000 cookiejar.py:1282(<lambda>)\n",
       "     19/2    0.000    0.000    0.000    0.000 webdriver.py:267(_wrap_value)\n",
       "     13/1    0.000    0.000    0.000    0.000 copy.py:132(deepcopy)\n",
       "       27    0.000    0.000    0.001    0.000 cookies.py:426(_copy_cookie_jar)\n",
       "       27    0.000    0.000    0.000    0.000 structures.py:54(__delitem__)\n",
       "       29    0.000    0.000    0.000    0.000 response.py:279(<listcomp>)\n",
       "        5    0.000    0.000    5.025    1.005 utils.py:97(is_connectable)\n",
       "        2    0.000    0.000    0.000    0.000 encoder.py:204(iterencode)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'bind' of '_socket.socket' objects}\n",
       "      133    0.000    0.000    0.000    0.000 punkt.py:442(first_upper)\n",
       "       15    0.000    0.000    0.001    0.000 iostream.py:195(schedule)\n",
       "      108    0.000    0.000    0.000    0.000 message.py:606(get_default_type)\n",
       "       12    0.000    0.000    0.001    0.000 iostream.py:382(write)\n",
       "       29    0.000    0.000    0.000    0.000 <ipython-input-9-81aa55e67a15>:47(<listcomp>)\n",
       "       77    0.000    0.000    0.000    0.000 punkt.py:447(first_lower)\n",
       "        1    0.000    0.000    0.001    0.001 frame.py:7644(_homogenize)\n",
       "       15    0.000    0.000    0.000    0.000 threading.py:1080(is_alive)\n",
       "       27    0.000    0.000    0.000    0.000 {method 'insert' of 'list' objects}\n",
       "        6    0.000    0.000    0.000    0.000 {built-in method _winapi.CloseHandle}\n",
       "       17    0.000    0.000    0.000    0.000 internals.py:3148(get_block_type)\n",
       "       37    0.000    0.000    0.000    0.000 generic.py:4378(__setattr__)\n",
       "       81    0.000    0.000    0.000    0.000 cookiejar.py:1130(return_ok_port)\n",
       "       14    0.000    0.000    0.000    0.000 cast.py:867(maybe_infer_to_datetimelike)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method nt.open}\n",
       "        2    0.000    0.000    1.702    0.851 remote_connection.py:355(execute)\n",
       "       29    0.000    0.000    0.000    0.000 {method 'pop' of 'set' objects}\n",
       "        1    0.000    0.000    0.000    0.000 frame.py:7367(extract_index)\n",
       "       81    0.000    0.000    0.000    0.000 cookiejar.py:1118(return_ok_secure)\n",
       "       27    0.000    0.000    0.000    0.000 cookiejar.py:1244(set_policy)\n",
       "       13    0.000    0.000    0.000    0.000 cast.py:971(maybe_cast_to_datetime)\n",
       "        9    0.000    0.000    0.000    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
       "        1    0.000    0.000    3.011    3.011 service.py:117(send_remote_shutdown_command)\n",
       "       88    0.000    0.000    0.000    0.000 <ipython-input-9-81aa55e67a15>:50(<listcomp>)\n",
       "        6    0.000    0.000    0.001    0.000 {built-in method builtins.print}\n",
       "       15    0.000    0.000    0.000    0.000 series.py:4019(_sanitize_array)\n",
       "        3    0.000    0.000    0.000    0.000 internals.py:3363(_rebuild_blknos_and_blklocs)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:4880(form_blocks)\n",
       "        7    0.000    0.000    0.000    0.000 {pandas._libs.lib.infer_dtype}\n",
       "      2/1    0.000    0.000    0.000    0.000 copy.py:236(_deepcopy_dict)\n",
       "        1    0.000    0.000    1.657    1.657 webdriver.py:113(__init__)\n",
       "        1    0.000    0.000    0.001    0.001 frame.py:4340(duplicated)\n",
       "       11    0.000    0.000    0.000    0.000 internals.py:4137(iget)\n",
       "       27    0.000    0.000    0.000    0.000 {method 'copy' of 'collections.OrderedDict' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'close' of '_io.BufferedWriter' objects}\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:5017(_stack_arrays)\n",
       "        1    0.000    0.000    4.700    4.700 webdriver.py:33(__init__)\n",
       "       27    0.000    0.000    0.000    0.000 cookies.py:78(add_unredirected_header)\n",
       "       16    0.000    0.000    0.000    0.000 internals.py:116(__init__)\n",
       "       12    0.000    0.000    0.000    0.000 iostream.py:307(_is_master_process)\n",
       "        2    0.000    0.000    1.701    0.851 poolmanager.py:302(urlopen)\n",
       "       25    0.000    0.000    0.000    0.000 common.py:1688(is_extension_array_dtype)\n",
       "       13    0.000    0.000    0.000    0.000 series.py:166(__init__)\n",
       "       13    0.000    0.000    0.000    0.000 series.py:365(_set_axis)\n",
       "       11    0.000    0.000    0.000    0.000 generic.py:2484(_get_item_cache)\n",
       "       27    0.000    0.000    0.000    0.000 {method 'tobytes' of 'memoryview' objects}\n",
       "        2    0.000    0.000    0.000    0.000 {pandas._libs.tslibs.conversion.datetime_to_datetime64}\n",
       "       15    0.000    0.000    0.000    0.000 series.py:4036(_try_cast)\n",
       "        1    0.000    0.000    0.016    0.016 subprocess.py:1101(_execute_child)\n",
       "        2    0.000    0.000    0.000    0.000 subprocess.py:1302(terminate)\n",
       "        1    0.000    0.000    1.008    1.008 request.py:1275(do_open)\n",
       "       11    0.000    0.000    0.000    0.000 internals.py:4108(get)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'getsockname' of '_socket.socket' objects}\n",
       "       12    0.000    0.000    0.000    0.000 iostream.py:320(_schedule_flush)\n",
       "       13    0.000    0.000    0.000    0.000 internals.py:4639(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 utils.py:81(join_host_port)\n",
       "        1    0.000    0.000    0.000    0.000 <ipython-input-9-81aa55e67a15>:58(print_results)\n",
       "       11    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.empty}\n",
       "       60    0.000    0.000    0.000    0.000 _collections_abc.py:760(__iter__)\n",
       "        1    0.000    0.000    1.003    1.003 utils.py:43(find_connectable_ip)\n",
       "        2    0.000    0.000    0.000    0.000 decoder.py:343(raw_decode)\n",
       "        1    0.000    0.000    0.017    0.017 subprocess.py:650(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 parse.py:165(port)\n",
       "     27/2    0.000    0.000    0.000    0.000 webdriver.py:284(_unwrap_value)\n",
       "        2    0.000    0.000    1.702    0.851 webdriver.py:301(execute)\n",
       "        2    0.000    0.000    0.000    0.000 remote_connection.py:73(get_remote_connection_headers)\n",
       "        1    0.000    0.000    0.000    0.000 algorithms.py:576(factorize)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:850(_try_convert_to_int_index)\n",
       "        1    0.000    0.000    0.000    0.000 webdriver.py:63(_make_w3c_caps)\n",
       "       14    0.000    0.000    0.000    0.000 internals.py:2298(__init__)\n",
       "       16    0.000    0.000    0.000    0.000 internals.py:3191(make_block)\n",
       "       27    0.000    0.000    0.000    0.000 cookies.py:421(get_policy)\n",
       "        1    0.000    0.000    0.000    0.000 arraysetops.py:438(in1d)\n",
       "       24    0.000    0.000    0.000    0.000 base.py:2067(__getitem__)\n",
       "       15    0.000    0.000    0.000    0.000 generic.py:124(__init__)\n",
       "        1    0.000    0.000    1.028    1.028 service.py:61(start)\n",
       "      133    0.000    0.000    0.000    0.000 {method 'isupper' of 'str' objects}\n",
       "        1    0.000    0.000    1.005    1.005 client.py:925(connect)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:2886(difference)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _winapi.CreatePipe}\n",
       "       22    0.000    0.000    0.000    0.000 {method 'get_loc' of 'pandas._libs.index.IndexEngine' objects}\n",
       "        2    0.000    0.000    0.000    0.000 string.py:107(substitute)\n",
       "       47    0.000    0.000    0.000    0.000 common.py:513(is_categorical_dtype)\n",
       "        6    0.000    0.000    0.000    0.000 common.py:301(_asarray_tuplesafe)\n",
       "       11    0.000    0.000    0.000    0.000 frame.py:3100(_box_col_values)\n",
       "       35    0.000    0.000    0.000    0.000 common.py:227(is_datetimetz)\n",
       "        2    0.000    0.000    0.000    0.000 enum.py:554(_missing_)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:4972(_simple_blockify)\n",
       "        1    0.000    0.000    0.000    0.000 api.py:80(_union_indexes)\n",
       "       34    0.000    0.000    0.000    0.000 base.py:672(values)\n",
       "       11    0.000    0.000    0.000    0.000 frame.py:3093(_box_item_values)\n",
       "        2    0.000    0.000    1.701    0.851 request.py:50(request)\n",
       "        1    0.000    0.000    0.000    0.000 connectionpool.py:199(_new_conn)\n",
       "        2    0.000    0.000    3.013    1.507 service.py:114(is_connectable)\n",
       "        7    0.000    0.000    0.000    0.000 base.py:473(_simple_new)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:1569(is_unique)\n",
       "       16    0.000    0.000    0.000    0.000 internals.py:237(mgr_locs)\n",
       "        1    0.000    0.000    1.008    1.008 request.py:507(open)\n",
       "       42    0.000    0.000    0.000    0.000 common.py:122(is_sparse)\n",
       "       10    0.000    0.000    0.000    0.000 internals.py:3307(shape)\n",
       "        1    0.000    0.000    0.000    0.000 utils.py:31(free_port)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'get_labels' of 'pandas._libs.hashtable.StringHashTable' objects}\n",
       "        1    0.000    0.000    0.000    0.000 algorithms.py:172(_ensure_arraylike)\n",
       "        4    0.000    0.000    0.000    0.000 copy.py:252(_keep_alive)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:669(transpose)\n",
       "        2    0.000    0.000    0.000    0.000 encoder.py:182(encode)\n",
       "        2    0.000    0.000    0.000    0.000 copy.py:210(_deepcopy_list)\n",
       "       14    0.000    0.000    0.000    0.000 series.py:401(name)\n",
       "        1    0.000    0.000    0.000    0.000 subprocess.py:1007(_get_handles)\n",
       "        1    0.000    0.000    0.000    0.000 sorting.py:20(get_group_index)\n",
       "        1    0.000    0.000    0.000    0.000 algorithms.py:449(_factorize_array)\n",
       "       13    0.000    0.000    0.000    0.000 common.py:1629(is_extension_type)\n",
       "       22    0.000    0.000    0.000    0.000 base.py:3071(get_loc)\n",
       "        1    0.000    0.000    1.657    1.657 webdriver.py:231(start_session)\n",
       "        5    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.arange}\n",
       "        1    0.000    0.000    0.000    0.000 subprocess.py:485(list2cmdline)\n",
       "       15    0.000    0.000    0.000    0.000 cast.py:853(maybe_castable)\n",
       "       13    0.000    0.000    0.000    0.000 <ipython-input-9-81aa55e67a15>:91(<dictcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 algorithms.py:1548(take_nd)\n",
       "       15    0.000    0.000    0.000    0.000 generic.py:4362(__getattr__)\n",
       "       15    0.000    0.000    0.000    0.000 iostream.py:93(_event_pipe)\n",
       "       35    0.000    0.000    0.000    0.000 {method 'view' of 'numpy.ndarray' objects}\n",
       "       28    0.000    0.000    0.000    0.000 common.py:195(is_categorical)\n",
       "        3    0.000    0.000    0.000    0.000 internals.py:3488(_verify_integrity)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:4518(take)\n",
       "        1    0.000    0.000    0.001    0.001 frame.py:4309(drop_duplicates)\n",
       "        2    0.000    0.000    0.000    0.000 service.py:51(service_url)\n",
       "       15    0.000    0.000    0.000    0.000 threading.py:1038(_wait_for_tstate_lock)\n",
       "       10    0.000    0.000    0.000    0.000 dtypes.py:707(is_dtype)\n",
       "       12    0.000    0.000    0.000    0.000 frame.py:713(iteritems)\n",
       "        2    0.000    0.000    0.001    0.001 connection.py:180(connect)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'get_indexer' of 'pandas._libs.index.IndexEngine' objects}\n",
       "        2    0.000    0.000    0.002    0.001 frame.py:334(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 internals.py:3265(__init__)\n",
       "        9    0.000    0.000    0.000    0.000 subprocess.py:202(Close)\n",
       "        2    0.000    0.000    0.000    0.000 cast.py:1207(construct_1d_object_array_from_listlike)\n",
       "       11    0.000    0.000    0.000    0.000 missing.py:112(_isna_new)\n",
       "        1    0.000    0.000    0.001    0.001 frame.py:426(_init_dict)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:3219(get_indexer)\n",
       "        1    0.000    0.000    0.000    0.000 indexing.py:2441(maybe_convert_indices)\n",
       "        1    0.000    0.000    1.657    1.657 request.py:91(request_encode_body)\n",
       "       77    0.000    0.000    0.000    0.000 {method 'islower' of 'str' objects}\n",
       "       26    0.000    0.000    0.000    0.000 base.py:4914(_ensure_index)\n",
       "       14    0.000    0.000    0.000    0.000 series.py:405(name)\n",
       "        4    0.000    0.000    0.000    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {pandas._libs.algos.take_2d_axis0_object_object}\n",
       "        3    0.000    0.000    0.000    0.000 base.py:510(_shallow_copy)\n",
       "       13    0.000    0.000    0.000    0.000 internals.py:5020(_asarray_compat)\n",
       "        2    0.000    0.000    0.000    0.000 utils.py:32(dump_json)\n",
       "        1    0.000    0.000    0.000    0.000 {pandas._libs.hashtable.duplicated_int64}\n",
       "        2    0.000    0.000    0.000    0.000 decoder.py:332(decode)\n",
       "        3    0.000    0.000    0.000    0.000 platform.py:1059(system)\n",
       "        1    0.000    0.000    2.014    2.014 remote_connection.py:23(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 request.py:325(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 numeric.py:35(__new__)\n",
       "        1    0.000    0.000    0.000    0.000 frame.py:461(_init_ndarray)\n",
       "       11    0.000    0.000    0.000    0.000 generic.py:2498(_set_as_cached)\n",
       "       12    0.000    0.000    0.000    0.000 internals.py:269(make_block_same_class)\n",
       "       10    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:416(parent)\n",
       "       11    0.000    0.000    0.000    0.000 internals.py:372(iget)\n",
       "        1    0.000    0.000    0.000    0.000 request.py:1237(do_request_)\n",
       "        2    0.000    0.000    0.000    0.000 algorithms.py:48(_ensure_data)\n",
       "       17    0.000    0.000    0.000    0.000 common.py:477(is_interval_dtype)\n",
       "        2    0.000    0.000    0.000    0.000 connection.py:172(_prepare_conn)\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method _winapi.DuplicateHandle}\n",
       "        1    0.000    0.000    1.008    1.008 request.py:1344(http_open)\n",
       "        7    0.000    0.000    0.000    0.000 __init__.py:205(iteritems)\n",
       "        9    0.000    0.000    0.000    0.000 common.py:1578(is_bool_dtype)\n",
       "        6    0.000    0.000    0.000    0.000 webdriver.py:276(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _winapi.GetExitCodeProcess}\n",
       "        3    0.000    0.000    0.000    0.000 subprocess.py:1079(_make_inheritable)\n",
       "        1    0.000    0.000    0.000    0.000 numeric.py:630(require)\n",
       "       11    0.000    0.000    0.000    0.000 concat.py:105(_get_sliced_frame_result_type)\n",
       "        1    0.000    0.000    0.000    0.000 service.py:37(__init__)\n",
       "       14    0.000    0.000    0.000    0.000 {built-in method pandas._libs.lib.infer_datetimelike_array}\n",
       "       22    0.000    0.000    0.000    0.000 {built-in method builtins.id}\n",
       "        2    0.000    0.000    0.000    0.000 __init__.py:299(loads)\n",
       "       28    0.000    0.000    0.000    0.000 numeric.py:433(asarray)\n",
       "        2    0.000    0.000    0.001    0.000 frame.py:4383(<genexpr>)\n",
       "        4    0.000    0.000    0.000    0.000 {method 'take' of 'numpy.ndarray' objects}\n",
       "        2    0.000    0.000    0.000    0.000 __init__.py:183(dumps)\n",
       "        3    0.000    0.000    0.001    0.000 socket.py:406(_decref_socketios)\n",
       "       12    0.000    0.000    0.000    0.000 inference.py:415(is_hashable)\n",
       "        8    0.000    0.000    0.000    0.000 common.py:858(is_signed_integer_dtype)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:2445(equals)\n",
       "        1    0.000    0.000    0.000    0.000 sorting.py:55(maybe_lift)\n",
       "        1    0.000    0.000    0.000    0.000 frame.py:2371(transpose)\n",
       "        6    0.000    0.000    0.000    0.000 generic.py:377(_get_axis_name)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:4869(create_block_manager_from_arrays)\n",
       "        1    0.000    0.000    0.000    0.000 options.py:29(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 {pandas._libs.lib.fast_unique_multiple_list}\n",
       "       11    0.000    0.000    0.000    0.000 {built-in method pandas._libs.missing.checknull}\n",
       "        1    0.000    0.000    0.000    0.000 subprocess.py:844(__del__)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:1283(astype)\n",
       "        1    0.000    0.000    0.001    0.001 frame.py:7349(_arrays_to_mgr)\n",
       "       30    0.000    0.000    0.000    0.000 internals.py:3309(<genexpr>)\n",
       "        2    0.000    0.000    0.000    0.000 utils.py:36(load_json)\n",
       "       12    0.000    0.000    0.000    0.000 {built-in method nt.getpid}\n",
       "        1    0.000    0.000    0.000    0.000 subprocess.py:979(wait)\n",
       "        1    0.000    0.000    0.000    0.000 cast.py:257(maybe_promote)\n",
       "       13    0.000    0.000    0.000    0.000 cast.py:1232(construct_1d_ndarray_preserving_na)\n",
       "        3    0.000    0.000    0.000    0.000 dtypes.py:675(construct_from_string)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:1117(__neg__)\n",
       "        1    0.000    0.000    0.000    0.000 <ipython-input-10-19c3509cb48d>:11(__init__)\n",
       "        7    0.000    0.000    0.000    0.000 {method 'any' of 'numpy.ndarray' objects}\n",
       "        2    0.000    0.000    0.000    0.000 subprocess.py:859(_get_devnull)\n",
       "        1    0.000    0.000    1.008    1.008 request.py:139(urlopen)\n",
       "        2    0.000    0.000    0.000    0.000 request.py:411(add_unredirected_header)\n",
       "        1    0.000    0.000    0.000    0.000 algorithms.py:141(_reconstruct_data)\n",
       "       11    0.000    0.000    0.000    0.000 missing.py:32(isna)\n",
       "        3    0.000    0.000    0.000    0.000 dtypes.py:401(__new__)\n",
       "        4    0.000    0.000    0.000    0.000 dtypes.py:584(is_dtype)\n",
       "       10    0.000    0.000    0.000    0.000 base.py:677(_values)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:1364(set_names)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:1935(_engine)\n",
       "        1    0.000    0.000    0.000    0.000 sorting.py:389(safe_sort)\n",
       "       25    0.000    0.000    0.000    0.000 internals.py:233(mgr_locs)\n",
       "       26    0.000    0.000    0.000    0.000 internals.py:3384(_get_items)\n",
       "       13    0.000    0.000    0.000    0.000 series.py:391(_set_subtyp)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.round}\n",
       "        2    0.000    0.000    0.000    0.000 subprocess.py:1198(_internal_poll)\n",
       "        1    0.000    0.000    0.000    0.000 request.py:349(full_url)\n",
       "        3    0.000    0.000    0.000    0.000 dtypes.py:459(construct_from_string)\n",
       "        1    0.000    0.000    0.000    0.000 api.py:91(_unique_indices)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:973(copy)\n",
       "        1    0.000    0.000    0.000    0.000 frozen.py:38(__getitem__)\n",
       "        1    0.000    0.000    0.000    0.000 frame.py:4364(f)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:3171(_update_inplace)\n",
       "       16    0.000    0.000    0.000    0.000 internals.py:127(_check_ndim)\n",
       "        3    0.000    0.000    0.000    0.000 internals.py:356(ftype)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:1237(take_nd)\n",
       "        3    0.000    0.000    0.000    0.000 internals.py:3784(_consolidate_check)\n",
       "        2    0.000    0.000    0.000    0.000 url.py:38(request_uri)\n",
       "        1    0.000    0.000    0.000    0.000 options.py:194(to_capabilities)\n",
       "        4    0.000    0.000    0.000    0.000 {method 'ravel' of 'numpy.ndarray' objects}\n",
       "       15    0.000    0.000    0.000    0.000 threading.py:507(is_set)\n",
       "        2    0.000    0.000    0.000    0.000 string.py:90(__init__)\n",
       "        2    0.000    0.000    1.008    0.504 request.py:496(_call_chain)\n",
       "        1    0.000    0.000    0.000    0.000 request.py:634(http_response)\n",
       "        5    0.000    0.000    0.000    0.000 dtypes.py:266(construct_from_string)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:920(_get_attributes_dict)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:317(_construct_axes_from_arguments)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:4388(reindex_indexer)\n",
       "        1    0.000    0.000    0.045    0.045 request.py:74(request_encode_url)\n",
       "        1    0.000    0.000    0.000    0.000 parse.py:925(unwrap)\n",
       "        1    0.000    0.000    1.008    1.008 request.py:535(_open)\n",
       "        1    0.000    0.000    0.000    0.000 fromnumeric.py:64(_wrapreduction)\n",
       "        7    0.000    0.000    0.000    0.000 _methods.py:42(_any)\n",
       "        7    0.000    0.000    0.000    0.000 common.py:1527(is_float_dtype)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:893(tolist)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:520(_shallow_copy_with_infer)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:3922(as_array)\n",
       "        2    0.000    0.000    0.000    0.000 errorhandler.py:103(check_response)\n",
       "        1    0.000    0.000    3.011    3.011 service.py:171(__del__)\n",
       "        9    0.000    0.000    0.000    0.000 copy.py:190(_deepcopy_atomic)\n",
       "        1    0.000    0.000    0.000    0.000 parse.py:934(splittype)\n",
       "        1    0.000    0.000    0.000    0.000 request.py:307(request_host)\n",
       "        1    0.000    0.000    0.000    0.000 request.py:380(_parse)\n",
       "        1    0.000    0.000    0.000    0.000 request.py:393(get_full_url)\n",
       "        1    0.000    0.000    0.002    0.002 <string>:1(<module>)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:789(_ndarray_values)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:2179(take)\n",
       "        6    0.000    0.000    0.000    0.000 internals.py:3490(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 series.py:643(__array_wrap__)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method nt.close}\n",
       "        3    0.000    0.000    0.000    0.000 platform.py:921(uname)\n",
       "        1    0.000    0.000    0.000    0.000 arraysetops.py:675(setdiff1d)\n",
       "        1    0.000    0.000    0.000    0.000 algorithms.py:224(_get_data_algo)\n",
       "        8    0.000    0.000    0.000    0.000 common.py:907(is_unsigned_integer_dtype)\n",
       "        1    0.000    0.000    0.000    0.000 api.py:98(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:996(_validate_names)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:1332(_set_names)\n",
       "        2    0.000    0.000    0.000    0.000 frame.py:555(shape)\n",
       "        1    0.000    0.000    0.001    0.001 frame.py:905(from_dict)\n",
       "        4    0.000    0.000    0.000    0.000 generic.py:390(_get_axis)\n",
       "        2    0.000    0.000    0.000    0.000 generic.py:4345(__finalize__)\n",
       "        3    0.000    0.000    0.000    0.000 internals.py:3266(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:3351(_is_single_block)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:4846(create_block_manager_from_blocks)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'to_array' of 'pandas._libs.hashtable.ObjectVector' objects}\n",
       "        1    0.000    0.000    0.000    0.000 subprocess.py:130(__init__)\n",
       "        4    0.000    0.000    0.000    0.000 common.py:444(is_period_dtype)\n",
       "        5    0.000    0.000    0.000    0.000 common.py:692(is_dtype_equal)\n",
       "        5    0.000    0.000    0.000    0.000 common.py:1043(is_datetime64_any_dtype)\n",
       "       13    0.000    0.000    0.000    0.000 api.py:92(conv)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:3389(_maybe_promote)\n",
       "       13    0.000    0.000    0.000    0.000 numeric.py:110(is_all_dates)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:4423(<listcomp>)\n",
       "        1    0.000    0.000    0.045    0.045 webdriver.py:681(close)\n",
       "        2    0.000    0.000    0.000    0.000 response.py:540(close)\n",
       "        1    0.000    0.000    0.000    0.000 service.py:26(__init__)\n",
       "        6    0.000    0.000    0.000    0.000 {method 'fill' of 'numpy.ndarray' objects}\n",
       "        1    0.000    0.000    0.000    0.000 subprocess.py:1215(_wait)\n",
       "        3    0.000    0.000    0.000    0.000 request.py:343(full_url)\n",
       "        1    0.000    0.000    0.000    0.000 request.py:1302(<dictcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 numeric.py:156(ones)\n",
       "        1    0.000    0.000    0.000    0.000 fromnumeric.py:2478(prod)\n",
       "        1    0.000    0.000    0.000    0.000 _methods.py:45(_all)\n",
       "        6    0.000    0.000    0.000    0.000 common.py:407(is_timedelta64_dtype)\n",
       "        1    0.000    0.000    0.000    0.000 _validators.py:221(validate_bool_kwarg)\n",
       "        1    0.000    0.000    0.000    0.000 numeric.py:64(_shallow_copy)\n",
       "        1    0.000    0.000    0.000    0.000 frame.py:457(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:303(_construct_axes_dict_from)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:4563(values)\n",
       "        4    0.000    0.000    0.000    0.000 internals.py:4101(_consolidate_inplace)\n",
       "        2    0.000    0.000    0.000    0.000 retry.py:199(from_int)\n",
       "        1    0.000    0.000    0.000    0.000 webdriver.py:160(create_options)\n",
       "        2    0.000    0.000    0.000    0.000 {method 'nonzero' of 'numpy.ndarray' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.copyto}\n",
       "        3    0.000    0.000    0.000    0.000 {pandas._libs.lib.values_from_object}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _winapi.WaitForSingleObject}\n",
       "        1    0.000    0.000    0.000    0.000 string.py:121(convert)\n",
       "        1    0.000    0.000    0.000    0.000 client.py:148(_encode)\n",
       "        1    0.000    0.000    0.000    0.000 algorithms.py:1421(_get_take_nd_function)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:766(size)\n",
       "        1    0.000    0.000    0.000    0.000 api.py:130(<setcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 api.py:129(_sanitize_and_check)\n",
       "        6    0.000    0.000    0.000    0.000 base.py:444(<genexpr>)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:922(<dictcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:1938(<lambda>)\n",
       "        1    0.000    0.000    0.000    0.000 sorting.py:47(_int64_cut_off)\n",
       "        1    0.000    0.000    0.000    0.000 frame.py:7419(_prep_ndarray)\n",
       "        2    0.000    0.000    0.000    0.000 generic.py:364(_get_axis_number)\n",
       "        3    0.000    0.000    0.000    0.000 generic.py:675(<genexpr>)\n",
       "        3    0.000    0.000    0.000    0.000 generic.py:1571(<genexpr>)\n",
       "        5    0.000    0.000    0.000    0.000 internals.py:3311(ndim)\n",
       "        5    0.000    0.000    0.000    0.000 internals.py:3776(is_consolidated)\n",
       "        3    0.000    0.000    0.000    0.000 internals.py:3785(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 service.py:44(command_line_args)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _operator.inv}\n",
       "        1    0.000    0.000    0.000    0.000 common.py:647(is_datetimelike)\n",
       "        1    0.000    0.000    0.000    0.000 common.py:191(_dict_keys_to_ordered_list)\n",
       "        1    0.000    0.000    0.000    0.000 _decorators.py:136(wrapper)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:711(get_values)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:1578(is_boolean)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:1964(inferred_type)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:2997(_get_unique_index)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:1524(empty)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:2543(_maybe_update_cacher)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:4433(_consolidate_inplace)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:4742(external_values)\n",
       "        1    0.000    0.000    0.000    0.000 series.py:516(nonzero)\n",
       "        2    0.000    0.000    0.000    0.000 response.py:211(get_redirect_location)\n",
       "        1    0.000    0.000    0.000    0.000 service.py:106(assert_process_still_running)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'all' of 'numpy.ndarray' objects}\n",
       "        2    0.000    0.000    0.000    0.000 {method 'transpose' of 'numpy.ndarray' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method msvcrt.open_osfhandle}\n",
       "        1    0.000    0.000    0.000    0.000 subprocess.py:957(poll)\n",
       "        1    0.000    0.000    0.000    0.000 missing.py:376(array_equivalent)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:912(__iter__)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:615(is_)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:904(_coerce_to_ndarray)\n",
       "        1    0.000    0.000    0.000    0.000 frame.py:478(_get_axes)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:4436(f)\n",
       "        3    0.000    0.000    0.000    0.000 internals.py:4684(_block)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:4745(internal_values)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:4752(get_values)\n",
       "        1    0.000    0.000    0.000    0.000 series.py:476(get_values)\n",
       "        2    0.000    0.000    0.000    0.000 remote_connection.py:420(<listcomp>)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'capitalize' of 'str' objects}\n",
       "        3    0.000    0.000    0.000    0.000 {method 'title' of 'str' objects}\n",
       "        6    0.000    0.000    0.000    0.000 {built-in method _winapi.GetCurrentProcess}\n",
       "        1    0.000    0.000    0.000    0.000 _collections_abc.py:680(values)\n",
       "        1    0.000    0.000    0.000    0.000 subprocess.py:207(Detach)\n",
       "        1    0.000    0.000    0.000    0.000 subprocess.py:226(_cleanup)\n",
       "        1    0.000    0.000    0.000    0.000 parse.py:1009(splittag)\n",
       "        1    0.000    0.000    0.000    0.000 request.py:388(get_method)\n",
       "        3    0.000    0.000    0.000    0.000 request.py:415(has_header)\n",
       "        1    0.000    0.000    0.000    0.000 client.py:737(info)\n",
       "        3    0.000    0.000    0.000    0.000 inference.py:119(is_iterator)\n",
       "        1    0.000    0.000    0.000    0.000 function.py:267(validate_transpose_for_generic)\n",
       "        1    0.000    0.000    0.000    0.000 missing.py:596(clean_reindex_fill_method)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:662(dtype)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:1302(_assert_can_do_setop)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:1307(_convert_can_do_setop)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:349(<dictcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:684(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:2577(_clear_item_cache)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:4423(_protect_consolidate)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:213(get_values)\n",
       "        3    0.000    0.000    0.000    0.000 internals.py:348(shape)\n",
       "        1    0.000    0.000    0.000    0.000 series.py:432(values)\n",
       "        1    0.000    0.000    0.000    0.000 webdriver.py:217(start_client)\n",
       "        1    0.000    0.000    0.000    0.000 options.py:101(extensions)\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method _socket.getdefaulttimeout}\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method msvcrt.get_osfhandle}\n",
       "        1    0.000    0.000    0.000    0.000 parse.py:947(splithost)\n",
       "        2    0.000    0.000    0.000    0.000 numeric.py:701(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 function_base.py:241(iterable)\n",
       "        1    0.000    0.000    0.000    0.000 inference.py:287(is_array_like)\n",
       "        2    0.000    0.000    0.000    0.000 function.py:38(__call__)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:104(_reset_cache)\n",
       "        1    0.000    0.000    0.000    0.000 missing.py:74(clean_fill_method)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:1329(_get_names)\n",
       "        1    0.000    0.000    0.000    0.000 frame.py:844(__len__)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:306(<dictcomp>)\n",
       "        3    0.000    0.000    0.000    0.000 generic.py:677(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:2633(_check_setitem_copy)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:222(to_dense)\n",
       "        4    0.000    0.000    0.000    0.000 internals.py:352(dtype)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:5026(_shape_compat)\n",
       "        1    0.000    0.000    0.000    0.000 series.py:465(_values)\n",
       "        1    0.000    0.000    0.000    0.000 webdriver.py:1188(file_detector)\n",
       "        1    0.000    0.000    0.000    0.000 switch_to.py:30(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 response.py:231(data)\n",
       "        1    0.000    0.000    0.000    0.000 options.py:82(arguments)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'argsort' of 'numpy.ndarray' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'tolist' of 'numpy.ndarray' objects}\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method pandas._libs.algos.ensure_platform_int}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method pandas._libs.algos.ensure_int64}\n",
       "        3    0.000    0.000    0.000    0.000 request.py:362(data)\n",
       "        1    0.000    0.000    0.000    0.000 request.py:366(data)\n",
       "        1    0.000    0.000    0.000    0.000 request.py:404(has_proxy)\n",
       "        1    0.000    0.000    0.000    0.000 request.py:1289(<dictcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 client.py:891(set_debuglevel)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:803(empty)\n",
       "        1    0.000    0.000    0.000    0.000 frame.py:320(_constructor)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:229(fill_value)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:3473(__len__)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:4085(consolidate)\n",
       "        1    0.000    0.000    0.000    0.000 series.py:349(_constructor)\n",
       "        1    0.000    0.000    0.000    0.000 mobile.py:45(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 options.py:38(binary_location)\n",
       "        1    0.000    0.000    0.000    0.000 options.py:63(debugger_address)\n",
       "        1    0.000    0.000    0.000    0.000 options.py:148(experimental_options)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method pandas._libs.lib.is_float}\n",
       "        1    0.000    0.000    0.000    0.000 numeric.py:504(asanyarray)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:203(internal_values)\n",
       "        1    0.000    0.000    0.000    0.000 internals.py:199(external_values)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method pandas._libs.lib.is_bool}\n",
       "        1    0.000    0.000    0.000    0.000 {function FrozenList.__getitem__ at 0x00000000077D6F28}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%prun\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "scrape_specs = {}\n",
    "scraypahs = {}\n",
    "temp_start_time = time.time()\n",
    "\n",
    "site = 'Autoblog'\n",
    "scraypahs[site] = scraypah(scraper_dict[site])\n",
    "if scraper_dict[site]['css_bool'] == True:\n",
    "    scraypahs[site].css_scrape_em()\n",
    "else:\n",
    "    scraypahs[site].get_urls()\n",
    "    scraypahs[site].scrape_em()\n",
    "scrape_specs = print_results(scraypahs[site].source, scraypahs[site].scraped_count, scraypahs[site].skip_count,\n",
    "                             scraypahs[site].too_old, scraypahs[site].relevant_df, round(\n",
    "                                 time.time()-temp_start_time, 2),\n",
    "                             scrape_specs)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For scraper development (no need to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " 'GreenCarCongress': {'url': ['http://www.greencarcongress.com/', 'http://www.greencarcongress.com/page/2/'],\n",
    "                                     'source': 'GreenCarCongress',\n",
    "                                     'css_bool': False,\n",
    "                                     'strain_tag': 'article',\n",
    "                                     'strain_attr_name': 'class',\n",
    "                                     'strain_attr_value': 'post entry',\n",
    "                                     'url_list_query': \"[item.a['href'] for item in self.base_soup.find_all('article', attrs={'class': 'post entry'})]\",\n",
    "                                     'date_loc': \"article.find('span', attrs={'class':'entry-date'}).a.text\",\n",
    "                                     'date_format': None,\n",
    "                                     'sum_loc': \"article.find_all('p')\",\n",
    "                                     'title_loc': \"article.h2.a.text\",\n",
    "                                     'strain_bool': True,\n",
    "                                     'journal_bool': False,\n",
    "                                     'rating': 1},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'GreenCarCongress': {'url': [f'http://www.greencarcongress.com/page/{page_no}/' for page_no in range(1,8)],\n",
    "                                     'source': 'GreenCarCongress',\n",
    "                                     'css_bool': False,\n",
    "                                     'strain_tag': 'article',\n",
    "                                     'strain_attr_name': 'class',\n",
    "                                     'strain_attr_value': 'post entry',\n",
    "                                     'url_list_query': \"[item.a['href'] for item in self.base_soup.find_all('article', attrs={'class': 'post entry'})]\",\n",
    "                                     'date_loc': \"article.find('span', attrs={'class':'entry-date'}).a.text\",\n",
    "                                     'date_format': None,\n",
    "                                     'sum_loc': \"article.find_all('p')\",\n",
    "                                     'title_loc': \"article.h2.a.text\",\n",
    "                                     'strain_bool': True,\n",
    "                                     'journal_bool': False,\n",
    "                                     'rating': 1},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Quartz': {'url': 'https://qz.com/search/self-driving',\n",
    "                           'source': 'Quartz',\n",
    "                           'css_bool': False,\n",
    "                           'url_list_query': \"['https://qz.com' + a['href'] for a in self.base_soup.find_all('a', class_='_5ff1a')]\",\n",
    "                           'date_loc': \"article.time.text\",\n",
    "                           'date_format': None,\n",
    "                           'sum_loc': \"article.find_all('p')\",\n",
    "                           'title_loc': \"article.h1.text\",\n",
    "                           'strain_bool': False,\n",
    "                           'journal_bool': False,\n",
    "                           'rating': 2},"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set url, date, summary, and title locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_urls = ['https://qz.com/search/self-driving']\n",
    "url_loc = \"[header.a['href'] for header in soup.find_all('h2') if header.a!=None]\"\n",
    "date_loc = \"article.time.text\"\n",
    "sum_loc = \"article.find_all('p')\"\n",
    "title_loc = \"article.h1.text\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test a site using selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "for base_url in base_urls:\n",
    "    driver.get(base_url)\n",
    "    urls = eval(url_loc)\n",
    "    for url in urls:\n",
    "        driver.get(url)\n",
    "        article = BeautifulSoup(driver.page_source)\n",
    "        title = eval(title_loc)\n",
    "        date = eval(date_loc)\n",
    "        summary = eval(sum_loc)\n",
    "        print(title, '\\n', date, '\\n', summary, '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test a site using only Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for base_url in base_urls:\n",
    "    soup = grab_homepage(base_url)\n",
    "    urls = eval(url_loc)\n",
    "    for url in urls: \n",
    "        article = grab_homepage(url)\n",
    "        title = eval(title_loc)\n",
    "        date = eval(date_loc)\n",
    "        summary = eval(sum_loc)\n",
    "        print(title, '\\n', date, '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change Log\n",
    "* 8/29/2018: Added Citylab, Electrek, cleaned code\n",
    "* 8/7/2018: Added Transport Reviews to academic paper scraper\n",
    "* 7/30/2018: Fixed GovTech scraper\n",
    "* 6/29/2018: Changed the whole scraper over to utilize a new class called *scraypah*. \n",
    "* 5/12/2018: Added Semiconductor Engineering scraper and academic articles scraper (~3 hours)\n",
    "* 4/13/2018: Integrated word document production through python\n",
    "* 3/19/2018: Added OEM/Gov section that quickly checks 17 sites for updates - only prints a notification that it needs to be checked if there are new updates from the past week\n",
    "* 2/27/2018: Wrote a function *page_scan* to more efficiently create the relevant web page dictionary \"profiles\"\n",
    "* 2/27/2018: Added 21CTP trucking news keywords to search for. Integrated functionality into existing web scraper.\n",
    "* 2/14/2018: Added NGV Global scraper for AFV stuff\n",
    "* 2/14/2018: Added fuel cells, hybrid, hybrid-electric, 'electric buses', 'electric truck', 'electric trucks', 'electric drive' to the search terms for AFVs...\n",
    "* 1/31/2018: Added *print_results* function to streamline printed results for each scraper. Added counter to track #articles that were too old. Added meta-data tracking capability (dumps into SQL database every week)\n",
    "* 1/31/2018: Split EV market analysis and web scraper into two different Notebooks\n",
    "* 1/26/2018: Added Lexology scraper\n",
    "* 1/19/2018: Fixed GreenCarCongress scraper (site redesign)\n",
    "* 1/4/2018: Added Engadget scraper\n",
    "* 1/4/2018: Added \"replace_em\" function to streamline removal of meaningless substrings from body text summaries\n",
    "* 12/29/2017: Added Reuters, MITNews, and ARSTechnica scrapers. Did some streamlining in the EV Sales analysis\n",
    "* 12/20/2017: Wrote up quick-guide to all the post-Python processing needed for the final News Update doc.\n",
    "* 12/20/2017: Changed to .xls format. Had to import a different package to do so, but makes mail merge work better\n",
    "* 12/13/2017: Fixed Trucks.com scraper - was pulling out the wrong date for each article (pulled a date from the sidebar...)\n",
    "* 12/8/2017: Edited Trucks.com search so that it doesn't pick up paragraph tags that are actually image captions (added condition that \"class = None\")\n",
    "* 12/8/2017: Added a bunch of comments, specifically in the first code segment (\"IEEE Spectrum\") for explanatory purposes\n",
    "* 1/8/2019: Added Green Car Reports, DOE, Business Wire, and The Fuse.\n",
    "* 1/10/2019: Uploaded ipynb to Energetics' GitHub (EICode).  Log entries can now be found via GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "498px",
    "left": "1480.3px",
    "right": "20px",
    "top": "119.976px",
    "width": "658px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
